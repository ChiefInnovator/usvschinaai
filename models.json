{
  "metadata": {
    "title": "US vs CHINA AI",
    "footerText": "Data Audited Dec 30, 2025 | Source: llm-stats.com | IQ = participation-weighted average of normalized benchmarks (single-participant tests excluded)"
  },
  "teams": {
    "usa": {
      "id": "usa",
      "name": "TEAM USA",
      "flag": "\ud83c\uddfa\ud83c\uddf8",
      "description": "The Frontier Artisans",
      "badge": "OVERALL WINNER",
      "color": "blue"
    },
    "china": {
      "id": "china",
      "name": "TEAM CHINA",
      "flag": "\ud83c\udde8\ud83c\uddf3",
      "description": "The Scaling Giants",
      "badge": "RUNNER UP",
      "color": "red"
    }
  },
  "columns": [
    {
      "key": "rank",
      "label": "Rank"
    },
    {
      "key": "name",
      "label": "Model Name"
    },
    {
      "key": "iq",
      "label": "IQ Index"
    },
    {
      "key": "value",
      "label": "Value Index"
    },
    {
      "key": "unified",
      "label": "Unified Power Score"
    }
  ],
  "history": [
    {
      "timestamp": "2026-01-23T15:01:18+07:59",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "",
            "avgIq": 59.32,
            "value": 4.24,
            "unified": 849.08,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "57c/s",
            "CodeArena": "1,536",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "",
            "avgIq": 53.51,
            "value": 1.78,
            "unified": 751.3,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "168c/s",
            "CodeArena": "1,534",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "GPT-5.2",
            "organization": "",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "",
            "avgIq": 62.82,
            "value": 3.99,
            "unified": 905.51,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "14c/s",
            "CodeArena": "1,469",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "OpenAI"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "",
            "avgIq": 53.36,
            "value": 15.25,
            "unified": 768.99,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "39c/s",
            "CodeArena": "1,179",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Opus 4",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "",
            "avgIq": 26.95,
            "value": 0.3,
            "unified": 318.63,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "128c/s",
            "CodeArena": "1,126",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "",
            "avgIq": 37.11,
            "value": 2.06,
            "unified": 485.95,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,020",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Sep. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "",
            "avgIq": 30.93,
            "value": 0.34,
            "unified": 383.14,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "122c/s",
            "CodeArena": "901",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Aug. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "",
            "avgIq": 28.53,
            "value": 2.54,
            "unified": 347.62,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "144c/s",
            "CodeArena": "832",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "OpenAI"
          },
          {
            "model": "Gemini 2.5 Pro",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-2.5-pro",
            "origin": "US",
            "description": "A highly capable AI model from Google, designed for the agentic era. Gemini 2.5 Pro performs well on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input",
            "created": "",
            "avgIq": 16.61,
            "value": 1.48,
            "unified": 152.78,
            "Model": "Gemini 2.5 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "169c/s",
            "CodeArena": "727",
            "GPQA": "83.0%",
            "AIME2025": "83.0%",
            "SWE-benchVerified": "63.2%",
            "ARC-AGIv2": "4.9%",
            "MMMLU": "-",
            "MMMU": "79.6%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "50.8%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Sonnet 4",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-sonnet-4-20250514",
            "origin": "US",
            "description": "Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4",
            "created": "",
            "avgIq": 11.59,
            "value": 0.64,
            "unified": 70.2,
            "Model": "Claude Sonnet 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "192c/s",
            "CodeArena": "686",
            "GPQA": "75.4%",
            "AIME2025": "70.5%",
            "SWE-benchVerified": "72.7%",
            "ARC-AGIv2": "-",
            "MMMLU": "86.5%",
            "MMMU": "74.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "35.5%",
            "TAU2Retail": "80.5%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Anthropic"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "",
            "avgIq": 40.36,
            "value": 14.41,
            "unified": 557.06,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "57c/s",
            "CodeArena": "1,050",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "GLM-4.6",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "",
            "avgIq": 28.19,
            "value": 10.29,
            "unified": 353.63,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "260c/s",
            "CodeArena": "908",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Sep. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "",
            "avgIq": 29.66,
            "value": 19.77,
            "unified": 391.69,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "222c/s",
            "CodeArena": "811",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "MiniMax"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "",
            "avgIq": 29.56,
            "value": 42.22,
            "unified": 423.53,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "75c/s",
            "CodeArena": "611",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "",
            "avgIq": 9.37,
            "value": 13.38,
            "unified": 53.22,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "102c/s",
            "CodeArena": "571",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "",
            "avgIq": 46.46,
            "value": 18.81,
            "unified": 662.46,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "127c/s",
            "CodeArena": "571",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Sep. 2025",
            "Organization": "MoonshotAI"
          },
          {
            "model": "GLM-4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "",
            "avgIq": 7.29,
            "value": 3.64,
            "unified": 4.98,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "274c/s",
            "CodeArena": "540",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Jul. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "",
            "avgIq": 11.37,
            "value": 3.67,
            "unified": 71.23,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "101c/s",
            "CodeArena": "509",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Sep. 2025",
            "Organization": "MoonshotAI"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "",
            "avgIq": 26.89,
            "value": 67.24,
            "unified": 417.73,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "303c/s",
            "CodeArena": "505",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "Xiaomi"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "",
            "avgIq": 29.15,
            "value": 41.64,
            "unified": 416.07,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "63c/s",
            "CodeArena": "489",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-23T14:06:44+07:59",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "",
            "avgIq": 59.32,
            "value": 4.24,
            "unified": 849.08,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "57c/s",
            "CodeArena": "1,536",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "",
            "avgIq": 53.51,
            "value": 1.78,
            "unified": 751.3,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "168c/s",
            "CodeArena": "1,534",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "GPT-5.2",
            "organization": "",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "",
            "avgIq": 62.82,
            "value": 3.99,
            "unified": 905.51,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "14c/s",
            "CodeArena": "1,469",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "OpenAI"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "",
            "avgIq": 53.36,
            "value": 15.25,
            "unified": 768.99,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "39c/s",
            "CodeArena": "1,179",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Opus 4",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "",
            "avgIq": 26.95,
            "value": 0.3,
            "unified": 318.63,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "128c/s",
            "CodeArena": "1,126",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "",
            "avgIq": 37.11,
            "value": 2.06,
            "unified": 485.95,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,020",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Sep. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "",
            "avgIq": 30.93,
            "value": 0.34,
            "unified": 383.14,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "122c/s",
            "CodeArena": "901",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Aug. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "",
            "avgIq": 28.53,
            "value": 2.54,
            "unified": 347.62,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "144c/s",
            "CodeArena": "832",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "OpenAI"
          },
          {
            "model": "Gemini 2.5 Pro",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-2.5-pro",
            "origin": "US",
            "description": "A highly capable AI model from Google, designed for the agentic era. Gemini 2.5 Pro performs well on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input",
            "created": "",
            "avgIq": 16.61,
            "value": 1.48,
            "unified": 152.78,
            "Model": "Gemini 2.5 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "169c/s",
            "CodeArena": "727",
            "GPQA": "83.0%",
            "AIME2025": "83.0%",
            "SWE-benchVerified": "63.2%",
            "ARC-AGIv2": "4.9%",
            "MMMLU": "-",
            "MMMU": "79.6%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "50.8%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Sonnet 4",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-sonnet-4-20250514",
            "origin": "US",
            "description": "Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4",
            "created": "",
            "avgIq": 11.59,
            "value": 0.64,
            "unified": 70.2,
            "Model": "Claude Sonnet 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "192c/s",
            "CodeArena": "686",
            "GPQA": "75.4%",
            "AIME2025": "70.5%",
            "SWE-benchVerified": "72.7%",
            "ARC-AGIv2": "-",
            "MMMLU": "86.5%",
            "MMMU": "74.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "35.5%",
            "TAU2Retail": "80.5%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Anthropic"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "",
            "avgIq": 40.36,
            "value": 14.41,
            "unified": 557.06,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "57c/s",
            "CodeArena": "1,050",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "GLM-4.6",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "",
            "avgIq": 28.19,
            "value": 10.29,
            "unified": 353.63,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "260c/s",
            "CodeArena": "908",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Sep. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "",
            "avgIq": 29.66,
            "value": 19.77,
            "unified": 391.69,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "222c/s",
            "CodeArena": "811",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "MiniMax"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "",
            "avgIq": 29.56,
            "value": 42.22,
            "unified": 423.53,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "74c/s",
            "CodeArena": "611",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "",
            "avgIq": 9.37,
            "value": 13.38,
            "unified": 53.22,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "102c/s",
            "CodeArena": "571",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "",
            "avgIq": 46.46,
            "value": 18.81,
            "unified": 662.46,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "127c/s",
            "CodeArena": "571",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Sep. 2025",
            "Organization": "MoonshotAI"
          },
          {
            "model": "GLM-4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "",
            "avgIq": 7.29,
            "value": 3.64,
            "unified": 4.98,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "274c/s",
            "CodeArena": "540",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Jul. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "",
            "avgIq": 11.37,
            "value": 3.67,
            "unified": 71.23,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "101c/s",
            "CodeArena": "509",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Sep. 2025",
            "Organization": "MoonshotAI"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "",
            "avgIq": 26.89,
            "value": 67.24,
            "unified": 417.73,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "303c/s",
            "CodeArena": "505",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "Xiaomi"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "",
            "avgIq": 29.15,
            "value": 41.64,
            "unified": 416.07,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "63c/s",
            "CodeArena": "489",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          }
        ]
      }
    }
  ]
}