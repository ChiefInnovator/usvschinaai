{
    "metadata": {
        "title": "US vs CHINA AI",
        "subtitle": "Performance Audit: Dec 30, 2025",
        "auditDate": "Dec 30, 2025",
        "footerText": "Data Audited Dec 30, 2025 | Normalized across 11 Frontier Benchmarks including AIME, GPQA, ARC-AGI, HLE, SWE-Bench, & CodeForces."
    },
    "teams": {
        "usa": {
            "id": "usa",
            "name": "TEAM USA",
            "flag": "\ud83c\uddfa\ud83c\uddf8",
            "description": "The Frontier Artisans",
            "badge": "IQ LEADERS",
            "color": "blue"
        },
        "china": {
            "id": "china",
            "name": "TEAM CHINA",
            "flag": "\ud83c\udde8\ud83c\uddf3",
            "description": "The Scaling Giants",
            "badge": "OVERALL WINNER",
            "color": "red"
        }
    },
    "benchmarks": [
        "AIME 2025",
        "HMMT 2025",
        "GPQA Diamond",
        "ARC-AGI",
        "BrowseComp",
        "ARC-AGI v2",
        "HLE",
        "MMLU-Pro",
        "LiveCodeBench",
        "SWE-Bench Verified",
        "CodeForces"
    ],
    "columns": [
        {
            "key": "rank",
            "label": "Rank"
        },
        {
            "key": "name",
            "label": "Model Name"
        },
        {
            "key": "iq",
            "label": "IQ Index"
        },
        {
            "key": "value",
            "label": "Value Index"
        },
        {
            "key": "unified",
            "label": "Unified Power Score"
        }
    ],
    "models": [
        {
            "rank": 1,
            "name": "DeepSeek-V3.2",
            "company": "DeepSeek",
            "companyLink": "https://www.deepseek.com/",
            "origin": "CN",
            "description": "A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). Thinking mode enables extended chain-of-thought reasoning for complex problem-solving tasks. Supports JSON output and tool calls.",
            "unified": 184.8,
            "iq": 92.4,
            "value": 100.0,
            "createdDate": "2025-12-15",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "costPer1M": 0.25,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 99.1,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#aime-2025"
                },
                "HMMT 2025": {
                    "score": 89.7,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 98.8,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 92.1,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#arc-agi"
                },
                "BrowseComp": {
                    "score": 87.7,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 96.1,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#arc-agi-v2"
                },
                "HLE": {
                    "score": 83.3,
                    "source": "https://llm-stats.com/models/deepseek-reasoner#hle"
                },
                "MMLU-Pro": {
                    "score": 89.5,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 92.5,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 83.1,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 85.3,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 0.17,
            "outputCostPer1M": 0.51,
            "pricingSource": "https://llm-stats.com/models/deepseek-reasoner#pricing"
        },
        {
            "rank": 2,
            "name": "DeepSeek-V3.2-Speciale",
            "company": "DeepSeek",
            "companyLink": "https://www.deepseek.com/",
            "origin": "CN",
            "description": "A specialized variant of DeepSeek-V3.2 with 685B parameters, optimized for enhanced performance on specific tasks including multi-step reasoning.",
            "unified": 176.1,
            "iq": 90.1,
            "value": 95.5,
            "createdDate": "2025-12-20",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "costPer1M": 0.32,
            "inputCostPer1M": 0.21,
            "outputCostPer1M": 0.63,
            "pricingSource": "https://llm-stats.com/models/deepseek-v3.2-speciale#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 85.0,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#aime-2025"
                },
                "HMMT 2025": {
                    "score": 88.8,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 90.8,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 97.7,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#arc-agi"
                },
                "BrowseComp": {
                    "score": 98.3,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 80.0,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#arc-agi-v2"
                },
                "HLE": {
                    "score": 90.2,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#hle"
                },
                "MMLU-Pro": {
                    "score": 90.4,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 89.6,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 73.5,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 88.9,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            }
        },
        {
            "rank": 3,
            "name": "Gemini 3 Pro",
            "company": "Google",
            "companyLink": "https://deepmind.google/",
            "origin": "US",
            "description": "First model in the new Gemini 3 series. Best for complex tasks requiring broad world knowledge and advanced reasoning across modalities. Uses dynamic thinking with a 1M-token context window.",
            "unified": 171.7,
            "iq": 96.2,
            "value": 78.5,
            "createdDate": "2025-11-20",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "costPer1M": 0.81,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 81.2,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#aime-2025"
                },
                "HMMT 2025": {
                    "score": 100,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 93.4,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 100,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#arc-agi"
                },
                "BrowseComp": {
                    "score": 100,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 98.9,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#arc-agi-v2"
                },
                "HLE": {
                    "score": 100,
                    "source": "https://llm-stats.com/models/gemini-3-pro-preview#hle"
                },
                "MMLU-Pro": {
                    "score": 94.9,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 98.0,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 83.0,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 88.2,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 0.54,
            "outputCostPer1M": 1.62,
            "pricingSource": "https://llm-stats.com/models/gemini-3-pro-preview#pricing"
        },
        {
            "rank": 4,
            "name": "Gemini 3 Flash",
            "company": "Google",
            "companyLink": "https://deepmind.google/",
            "origin": "US",
            "description": "Frontier intelligence built for speed at a fraction of the cost. Combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1M-token input context window optimized for agentic workflows, coding, and complex analysis.",
            "unified": 170.0,
            "iq": 88.5,
            "value": 92.0,
            "createdDate": "2025-12-05",
            "link": "https://llm-stats.com/models/gemini-3-flash",
            "costPer1M": 0.39,
            "inputCostPer1M": 0.26,
            "outputCostPer1M": 0.78,
            "pricingSource": "https://llm-stats.com/models/gemini-3-flash#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 83.8,
                    "source": "https://llm-stats.com/models/gemini-3-flash#aime-2025"
                },
                "HMMT 2025": {
                    "score": 88.6,
                    "source": "https://llm-stats.com/models/gemini-3-flash#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 87.1,
                    "source": "https://llm-stats.com/models/gemini-3-flash#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 81.2,
                    "source": "https://llm-stats.com/models/gemini-3-flash#arc-agi"
                },
                "BrowseComp": {
                    "score": 85.8,
                    "source": "https://llm-stats.com/models/gemini-3-flash#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 100,
                    "source": "https://llm-stats.com/models/gemini-3-flash#arc-agi-v2"
                },
                "HLE": {
                    "score": 93.0,
                    "source": "https://llm-stats.com/models/gemini-3-flash#hle"
                },
                "MMLU-Pro": {
                    "score": 83.1,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 87.6,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 72.1,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 82.3,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            }
        },
        {
            "rank": 5,
            "name": "DeepSeek-V3.2-Exp",
            "company": "DeepSeek",
            "companyLink": "https://www.deepseek.com/",
            "origin": "CN",
            "description": "An experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. Explores fine-grained sparse attention for extended sequence processing.",
            "unified": 168.1,
            "iq": 88.0,
            "value": 91.0,
            "createdDate": "2025-11-05",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "costPer1M": 0.41,
            "inputCostPer1M": 0.27,
            "outputCostPer1M": 0.81,
            "pricingSource": "https://llm-stats.com/models/deepseek-v3.2-exp#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 89.7,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#aime-2025"
                },
                "HMMT 2025": {
                    "score": 88.8,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 83.2,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 85.0,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#arc-agi"
                },
                "BrowseComp": {
                    "score": 84.7,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 82.9,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#arc-agi-v2"
                },
                "HLE": {
                    "score": 89.0,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#hle"
                },
                "MMLU-Pro": {
                    "score": 90.5,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 81.5,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 91.4,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 80.1,
                    "source": "https://llm-stats.com/models/deepseek-v3.2-exp#codeforces"
                }
            }
        },
        {
            "rank": 6,
            "name": "Qwen 3 Max",
            "company": "Alibaba Cloud",
            "companyLink": "https://www.alibabacloud.com/",
            "origin": "CN",
            "description": "Alibaba Cloud's most capable model. Exceptional at mathematical reasoning with 93.1% on AIME 2025. Strong performance across all frontier benchmarks with excellent cost efficiency.",
            "unified": 165.2,
            "iq": 89.1,
            "value": 85.3,
            "createdDate": "2025-12-01",
            "link": "https://dev.to/czmilo/qwen3-max-2025",
            "costPer1M": 0.56,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 93.1,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#aime-2025"
                },
                "HMMT 2025": {
                    "score": 86.5,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 76.8,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 93.0,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#arc-agi"
                },
                "BrowseComp": {
                    "score": 89.9,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 93.6,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#arc-agi-v2"
                },
                "HLE": {
                    "score": 90.7,
                    "source": "https://dev.to/czmilo/qwen3-max-2025#hle"
                },
                "MMLU-Pro": {
                    "score": 85.7,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 86.0,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 78.0,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 86.0,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 0.37,
            "outputCostPer1M": 1.11,
            "pricingSource": "https://dev.to/czmilo/qwen3-max-2025#pricing"
        },
        {
            "rank": 7,
            "name": "Grok Code Fast 1",
            "company": "xAI",
            "companyLink": "https://x.ai/",
            "origin": "US",
            "description": "Speedy and economical reasoning model that excels at agentic coding. Built from scratch with new model architecture and pre-training corpus rich with programming content.",
            "unified": 161.7,
            "iq": 86.0,
            "value": 88.0,
            "createdDate": "2025-10-01",
            "link": "https://llm-stats.com/models/grok-code-fast-1",
            "costPer1M": 0.48,
            "inputCostPer1M": 0.32,
            "outputCostPer1M": 0.96,
            "pricingSource": "https://llm-stats.com/models/grok-code-fast-1#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 77.4,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#aime-2025"
                },
                "HMMT 2025": {
                    "score": 94.9,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 87.4,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 86.8,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#arc-agi"
                },
                "BrowseComp": {
                    "score": 85.6,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 79.8,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#arc-agi-v2"
                },
                "HLE": {
                    "score": 90.5,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#hle"
                },
                "MMLU-Pro": {
                    "score": 85.6,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 84.3,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 83.6,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 78.8,
                    "source": "https://llm-stats.com/models/grok-code-fast-1#codeforces"
                }
            }
        },
        {
            "rank": 8,
            "name": "Qwen3-235B-Thinking",
            "company": "Alibaba Cloud",
            "companyLink": "https://www.alibabacloud.com/",
            "origin": "CN",
            "description": "State-of-the-art thinking-enabled MoE model with 235B total parameters (22B activated). Features 94 layers, 128 experts, and 262K native context. Excels at SWE-Bench with 90% verified score.",
            "unified": 161.0,
            "iq": 87.5,
            "value": 84.0,
            "createdDate": "2025-07-20",
            "link": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507",
            "costPer1M": 0.6,
            "inputCostPer1M": 0.4,
            "outputCostPer1M": 1.2,
            "pricingSource": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 90.4,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#aime-2025"
                },
                "HMMT 2025": {
                    "score": 88.1,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 84.6,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 84.0,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#arc-agi"
                },
                "BrowseComp": {
                    "score": 87.0,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 88.2,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#arc-agi-v2"
                },
                "HLE": {
                    "score": 87.2,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#hle"
                },
                "MMLU-Pro": {
                    "score": 86.5,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 82.3,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 90.0,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 88.0,
                    "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#codeforces"
                }
            }
        },
        {
            "rank": 9,
            "name": "GPT-5 mini",
            "company": "OpenAI",
            "companyLink": "https://openai.com/",
            "origin": "US",
            "description": "A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for precise prompts with high reasoning capabilities at reduced cost.",
            "unified": 159.1,
            "iq": 82.0,
            "value": 94.0,
            "createdDate": "2025-08-07",
            "link": "https://llm-stats.com/models/gpt-5-mini-2025-08-07",
            "costPer1M": 0.35,
            "inputCostPer1M": 0.23,
            "outputCostPer1M": 0.69,
            "pricingSource": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 81.8,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#aime-2025"
                },
                "HMMT 2025": {
                    "score": 82.4,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 82.5,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 78.5,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#arc-agi"
                },
                "BrowseComp": {
                    "score": 81.3,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 82.3,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#arc-agi-v2"
                },
                "HLE": {
                    "score": 80.8,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#hle"
                },
                "MMLU-Pro": {
                    "score": 73.6,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 83.9,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 85.5,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 76.5,
                    "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#codeforces"
                }
            }
        },
        {
            "rank": 10,
            "name": "Kimi K2 Thinking",
            "company": "Moonshot AI",
            "companyLink": "https://www.moonshot.cn/",
            "origin": "CN",
            "description": "Latest, most capable open-source thinking model from Moonshot AI. Built as a thinking agent that reasons step-by-step while dynamically invoking tools. State-of-the-art on HLE and BrowseComp benchmarks.",
            "unified": 154.4,
            "iq": 84.8,
            "value": 82.1,
            "createdDate": "2025-09-05",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "costPer1M": 0.67,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 84.4,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#aime-2025"
                },
                "HMMT 2025": {
                    "score": 85.2,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 84.8,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 88.1,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#arc-agi"
                },
                "BrowseComp": {
                    "score": 86.4,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 76.4,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#arc-agi-v2"
                },
                "HLE": {
                    "score": 88.3,
                    "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#hle"
                },
                "MMLU-Pro": {
                    "score": 78.9,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 85.5,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 70.0,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 77.8,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 0.45,
            "outputCostPer1M": 1.35,
            "pricingSource": "https://llm-stats.com/models/kimi-k2-thinking-0905#pricing"
        },
        {
            "rank": 11,
            "name": "GPT-5.1",
            "company": "OpenAI",
            "companyLink": "https://openai.com/",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. OpenAI's flagship model for coding and agentic tasks with strong performance across all benchmarks.",
            "unified": 153.5,
            "iq": 93.0,
            "value": 65.0,
            "createdDate": "2025-11-13",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "costPer1M": 1.7,
            "inputCostPer1M": 1.13,
            "outputCostPer1M": 3.39,
            "pricingSource": "https://llm-stats.com/models/gpt-5.1-2025-11-13#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 95.6,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#aime-2025"
                },
                "HMMT 2025": {
                    "score": 90.3,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 94.1,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 92.2,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#arc-agi"
                },
                "BrowseComp": {
                    "score": 91.0,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 93.4,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#arc-agi-v2"
                },
                "HLE": {
                    "score": 94.4,
                    "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#hle"
                },
                "MMLU-Pro": {
                    "score": 95.3,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 93.5,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 80.2,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 88.0,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            }
        },
        {
            "rank": 12,
            "name": "GPT-5.2 Pro",
            "company": "OpenAI",
            "companyLink": "https://openai.com/",
            "origin": "US",
            "description": "Pro variant of GPT-5.2, designed for top-quality, end-to-end execution. Supports xhigh reasoning for the most demanding tasks.",
            "unified": 145.1,
            "iq": 100.0,
            "value": 45.1,
            "createdDate": "2025-12-11",
            "link": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11",
            "costPer1M": 5.07,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#aime-2025"
                },
                "HMMT 2025": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#arc-agi"
                },
                "BrowseComp": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#arc-agi-v2"
                },
                "HLE": {
                    "score": 100.0,
                    "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#hle"
                },
                "MMLU-Pro": {
                    "score": 99.8,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 95.1,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 81.1,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 88.3,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 3.38,
            "outputCostPer1M": 10.14,
            "pricingSource": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#pricing"
        },
        {
            "rank": 13,
            "name": "Llama 4-405B",
            "company": "Meta",
            "companyLink": "https://ai.meta.com/",
            "origin": "US",
            "description": "Natively multimodal model capable of processing both text and images. Features a 17B activated parameter (109B total) mixture-of-experts architecture with 16 experts, supporting a 10M token context window.",
            "unified": 144.2,
            "iq": 76.5,
            "value": 88.4,
            "createdDate": "2025-10-10",
            "link": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout",
            "costPer1M": 0.47,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 69.9,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#aime-2025"
                },
                "HMMT 2025": {
                    "score": 72.7,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 88.4,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 73.9,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#arc-agi"
                },
                "BrowseComp": {
                    "score": 83.3,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 71.1,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#arc-agi-v2"
                },
                "HLE": {
                    "score": 76.3,
                    "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#hle"
                },
                "MMLU-Pro": {
                    "score": 69.0,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 72.0,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 62.5,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 75.2,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 0.31,
            "outputCostPer1M": 0.93,
            "pricingSource": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#pricing"
        },
        {
            "rank": 14,
            "name": "GPT-5 Codex",
            "company": "OpenAI",
            "companyLink": "https://openai.com/",
            "origin": "US",
            "description": "Trained specifically for conducting code reviews and finding critical flaws. Navigates codebase to identify security vulnerabilities, performance issues, and bugs.",
            "unified": 141.8,
            "iq": 91.5,
            "value": 55.0,
            "createdDate": "2025-09-15",
            "link": "https://llm-stats.com/models/gpt-5-codex-2025-09-15",
            "costPer1M": 2.94,
            "inputCostPer1M": 1.96,
            "outputCostPer1M": 5.88,
            "pricingSource": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 92.4,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#aime-2025"
                },
                "HMMT 2025": {
                    "score": 90.6,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 85.9,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 92.4,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#arc-agi"
                },
                "BrowseComp": {
                    "score": 82.0,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 93.5,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#arc-agi-v2"
                },
                "HLE": {
                    "score": 88.1,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#hle"
                },
                "MMLU-Pro": {
                    "score": 84.0,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 99.6,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 92.5,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 89.6,
                    "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#codeforces"
                }
            }
        },
        {
            "rank": 15,
            "name": "DeepSeek-R1-0528",
            "company": "DeepSeek",
            "companyLink": "https://www.deepseek.com/",
            "origin": "CN",
            "description": "May 28, 2025 version of DeepSeek's reasoning model. Features advanced thinking capabilities. Excels in complex reasoning, mathematical problem-solving, and code generation.",
            "unified": 126.0,
            "iq": 72.0,
            "value": 75.0,
            "createdDate": "2025-05-28",
            "link": "https://llm-stats.com/models/deepseek-r1-0528",
            "costPer1M": 0.98,
            "inputCostPer1M": 0.65,
            "outputCostPer1M": 1.95,
            "pricingSource": "https://llm-stats.com/models/deepseek-r1-0528#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 75.1,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#aime-2025"
                },
                "HMMT 2025": {
                    "score": 72.6,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 73.2,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 75.1,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#arc-agi"
                },
                "BrowseComp": {
                    "score": 72.7,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 78.2,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#arc-agi-v2"
                },
                "HLE": {
                    "score": 74.7,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#hle"
                },
                "MMLU-Pro": {
                    "score": 68.6,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 74.4,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 70.4,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 74.3,
                    "source": "https://llm-stats.com/models/deepseek-r1-0528#codeforces"
                }
            }
        },
        {
            "rank": 16,
            "name": "Claude 4.5 Opus",
            "company": "Anthropic",
            "companyLink": "https://www.anthropic.com/",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance.",
            "unified": 123.8,
            "iq": 94.8,
            "value": 30.6,
            "createdDate": "2025-11-01",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "costPer1M": 11.22,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 88.4,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#aime-2025"
                },
                "HMMT 2025": {
                    "score": 92.0,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 99.5,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 91.2,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#arc-agi"
                },
                "BrowseComp": {
                    "score": 100,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 96.9,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#arc-agi-v2"
                },
                "HLE": {
                    "score": 95.5,
                    "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#hle"
                },
                "MMLU-Pro": {
                    "score": 87.6,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 90.5,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 85.9,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 89.3,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 7.48,
            "outputCostPer1M": 22.44,
            "pricingSource": "https://llm-stats.com/models/claude-opus-4-5-20251101#pricing"
        },
        {
            "rank": 17,
            "name": "Grok-4 Heavy",
            "company": "xAI",
            "companyLink": "https://x.ai/",
            "origin": "US",
            "description": "Multi-agent version of Grok 4 that spawns multiple agents in parallel to work independently then collaborate. Uses ~10x more test-time compute than regular Grok 4.",
            "unified": 120.9,
            "iq": 87.5,
            "value": 38.2,
            "createdDate": "2025-11-15",
            "link": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/",
            "costPer1M": 7.39,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 89.1,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#aime-2025"
                },
                "HMMT 2025": {
                    "score": 86.8,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 89.1,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 74.6,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#arc-agi"
                },
                "BrowseComp": {
                    "score": 88.1,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 95.4,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#arc-agi-v2"
                },
                "HLE": {
                    "score": 89.4,
                    "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#hle"
                },
                "MMLU-Pro": {
                    "score": 91.2,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 87.5,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 73.7,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 83.7,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 4.93,
            "outputCostPer1M": 14.79,
            "pricingSource": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#pricing"
        },
        {
            "rank": 18,
            "name": "GLM-4.6",
            "company": "Zhipu AI",
            "companyLink": "https://www.zhipuai.cn/",
            "origin": "CN",
            "description": "Latest version of Zhipu AI's flagship model. Features 200K token context window, superior coding performance, advanced reasoning with tool use, and stronger agent capabilities.",
            "unified": 120.0,
            "iq": 75.0,
            "value": 60.0,
            "createdDate": "2025-08-15",
            "link": "https://llm-stats.com/models/glm-4.6",
            "costPer1M": 2.24,
            "inputCostPer1M": 1.49,
            "outputCostPer1M": 4.47,
            "pricingSource": "https://llm-stats.com/models/glm-4.6#pricing",
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 71.7,
                    "source": "https://llm-stats.com/models/glm-4.6#aime-2025"
                },
                "HMMT 2025": {
                    "score": 71.3,
                    "source": "https://llm-stats.com/models/glm-4.6#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 73.2,
                    "source": "https://llm-stats.com/models/glm-4.6#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 75.3,
                    "source": "https://llm-stats.com/models/glm-4.6#arc-agi"
                },
                "BrowseComp": {
                    "score": 82.0,
                    "source": "https://llm-stats.com/models/glm-4.6#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 71.6,
                    "source": "https://llm-stats.com/models/glm-4.6#arc-agi-v2"
                },
                "HLE": {
                    "score": 74.4,
                    "source": "https://llm-stats.com/models/glm-4.6#hle"
                },
                "MMLU-Pro": {
                    "score": 79.1,
                    "source": "https://llm-stats.com/models/glm-4.6#mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 69.5,
                    "source": "https://llm-stats.com/models/glm-4.6#livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 67.6,
                    "source": "https://llm-stats.com/models/glm-4.6#swe-bench-verified"
                },
                "CodeForces": {
                    "score": 72.4,
                    "source": "https://llm-stats.com/models/glm-4.6#codeforces"
                }
            }
        },
        {
            "rank": 19,
            "name": "GLM-4.7",
            "company": "Zhipu AI",
            "companyLink": "https://www.zhipuai.cn/",
            "origin": "CN",
            "description": "Coding-centric model that thinks before acting and preserves reasoning across turns. Features stronger multi-step tool use, better terminal integration, and multilingual coding capabilities.",
            "unified": 105.9,
            "iq": 70.2,
            "value": 50.8,
            "createdDate": "2025-10-25",
            "link": "https://llm-stats.com/models/glm-4.7",
            "costPer1M": 3.71,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 67.3,
                    "source": "https://llm-stats.com/models/glm-4.7#aime-2025"
                },
                "HMMT 2025": {
                    "score": 70.2,
                    "source": "https://llm-stats.com/models/glm-4.7#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 75.9,
                    "source": "https://llm-stats.com/models/glm-4.7#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 64.3,
                    "source": "https://llm-stats.com/models/glm-4.7#arc-agi"
                },
                "BrowseComp": {
                    "score": 68.0,
                    "source": "https://llm-stats.com/models/glm-4.7#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 75.5,
                    "source": "https://llm-stats.com/models/glm-4.7#arc-agi-v2"
                },
                "HLE": {
                    "score": 70.3,
                    "source": "https://llm-stats.com/models/glm-4.7#hle"
                },
                "MMLU-Pro": {
                    "score": 66.9,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 68.6,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 58.9,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 69.2,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 2.47,
            "outputCostPer1M": 7.41,
            "pricingSource": "https://llm-stats.com/models/glm-4.7#pricing"
        },
        {
            "rank": 20,
            "name": "MiniMax-M2.1",
            "company": "MiniMax",
            "companyLink": "https://www.minimaxi.com/",
            "origin": "CN",
            "description": "Enhanced LLM focused on multi-language programming and real-world complex tasks. Exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Objective-C, and TypeScript.",
            "unified": 101.6,
            "iq": 68.4,
            "value": 48.6,
            "createdDate": "2025-09-20",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "costPer1M": 4.18,
            "benchmarkScores": {
                "AIME 2025": {
                    "score": 70.0,
                    "source": "https://llm-stats.com/models/minimax-m2.1#aime-2025"
                },
                "HMMT 2025": {
                    "score": 85.1,
                    "source": "https://llm-stats.com/models/minimax-m2.1#hmmt-2025"
                },
                "GPQA Diamond": {
                    "score": 62.6,
                    "source": "https://llm-stats.com/models/minimax-m2.1#gpqa-diamond"
                },
                "ARC-AGI": {
                    "score": 73.0,
                    "source": "https://llm-stats.com/models/minimax-m2.1#arc-agi"
                },
                "BrowseComp": {
                    "score": 58.0,
                    "source": "https://llm-stats.com/models/minimax-m2.1#browsecomp"
                },
                "ARC-AGI v2": {
                    "score": 60.7,
                    "source": "https://llm-stats.com/models/minimax-m2.1#arc-agi-v2"
                },
                "HLE": {
                    "score": 69.5,
                    "source": "https://llm-stats.com/models/minimax-m2.1#hle"
                },
                "MMLU-Pro": {
                    "score": 62.2,
                    "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                },
                "LiveCodeBench": {
                    "score": 64.5,
                    "source": "https://llm-stats.com/benchmarks/livecodebench"
                },
                "SWE-Bench Verified": {
                    "score": 58.6,
                    "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                },
                "CodeForces": {
                    "score": 57.7,
                    "source": "https://llm-stats.com/benchmarks/codeforces"
                }
            },
            "inputCostPer1M": 2.79,
            "outputCostPer1M": 8.37,
            "pricingSource": "https://llm-stats.com/models/minimax-m2.1#pricing"
        }
    ]
}