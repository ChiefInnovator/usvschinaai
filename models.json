{
  "metadata": {
    "title": "US vs CHINA AI",
    "footerText": "Data Audited Mar 1, 2026 | Source: llm-stats.com | IQ = participation-weighted average of normalized benchmarks (single-participant tests excluded)"
  },
  "teams": {
    "usa": {
      "id": "usa",
      "name": "TEAM USA",
      "flag": "\ud83c\uddfa\ud83c\uddf8",
      "description": "The Frontier Artisans",
      "badge": "OVERALL WINNER",
      "color": "blue"
    },
    "china": {
      "id": "china",
      "name": "TEAM CHINA",
      "flag": "\ud83c\udde8\ud83c\uddf3",
      "description": "The Scaling Giants",
      "badge": "RUNNER UP",
      "color": "red"
    }
  },
  "columns": [
    {
      "key": "rank",
      "label": "Rank"
    },
    {
      "key": "name",
      "label": "Model Name"
    },
    {
      "key": "iq",
      "label": "IQ Index"
    },
    {
      "key": "value",
      "label": "Value Index"
    },
    {
      "key": "unified",
      "label": "Unified Power Score"
    }
  ],
  "history": [
    {
      "timestamp": "2026-03-01T15:25:56+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 814.01,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "102c/s",
            "CodeArena": "1,999",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.25,
            "value": 5.23,
            "unified": 732.74,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "49c/s",
            "CodeArena": "1,559",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 60.24,
            "value": 2.01,
            "unified": 592.69,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "77c/s",
            "CodeArena": "1,548",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.94,
            "value": 19.13,
            "unified": 698.15,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "66c/s",
            "CodeArena": "1,532",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.83,
            "value": 4.37,
            "unified": 685.69,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "53c/s",
            "CodeArena": "1,520",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.54,
            "value": 5.17,
            "unified": 909.39,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "76c/s",
            "CodeArena": "1,513",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.75,
            "value": 5.13,
            "unified": 573.99,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "60c/s",
            "CodeArena": "1,354",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.44,
            "value": 3.41,
            "unified": 608.04,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "141c/s",
            "CodeArena": "1,353",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 60.95,
            "value": 5.42,
            "unified": 607.3,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "95c/s",
            "CodeArena": "1,204",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 45.73,
            "value": 4.06,
            "unified": 448.73,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "92c/s",
            "CodeArena": "1,187",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.61,
            "value": 17.29,
            "unified": 752.17,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "63c/s",
            "CodeArena": "1,546",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.79,
            "value": 21.22,
            "unified": 690.88,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "142c/s",
            "CodeArena": "1,463",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 44.07,
            "value": 10.49,
            "unified": 445.67,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "100c/s",
            "CodeArena": "1,144",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.54,
            "value": 40.36,
            "unified": 678.48,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "183c/s",
            "CodeArena": "1,101",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.66,
            "value": 7.91,
            "unified": 210.96,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "116c/s",
            "CodeArena": "1,052",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.96,
            "value": 15.7,
            "unified": 455.73,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "216c/s",
            "CodeArena": "1,030",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.43,
            "value": 20.42,
            "unified": 532.06,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "126c/s",
            "CodeArena": "914",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.53,
            "value": 13.02,
            "unified": 200.21,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "153c/s",
            "CodeArena": "892",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.53,
            "value": 0.82,
            "unified": 0.04,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "117c/s",
            "CodeArena": "881",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 33.04,
            "value": 47.2,
            "unified": 412.0,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "8c/s",
            "CodeArena": "826",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-03-01T04:08:40+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 814.56,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "102c/s",
            "CodeArena": "2,003",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.25,
            "value": 5.23,
            "unified": 733.21,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "47c/s",
            "CodeArena": "1,561",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 60.25,
            "value": 2.01,
            "unified": 593.37,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "75c/s",
            "CodeArena": "1,551",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.91,
            "value": 19.12,
            "unified": 698.33,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "65c/s",
            "CodeArena": "1,532",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.8,
            "value": 4.37,
            "unified": 685.93,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "55c/s",
            "CodeArena": "1,520",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.49,
            "value": 5.17,
            "unified": 909.39,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "77c/s",
            "CodeArena": "1,512",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.69,
            "value": 5.13,
            "unified": 573.91,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "60c/s",
            "CodeArena": "1,354",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.14,
            "value": 3.4,
            "unified": 605.43,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "139c/s",
            "CodeArena": "1,335",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 60.77,
            "value": 5.4,
            "unified": 605.98,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "89c/s",
            "CodeArena": "1,200",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 45.68,
            "value": 4.06,
            "unified": 448.84,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "96c/s",
            "CodeArena": "1,187",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.54,
            "value": 17.27,
            "unified": 751.89,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "63c/s",
            "CodeArena": "1,546",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.8,
            "value": 21.23,
            "unified": 691.56,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "141c/s",
            "CodeArena": "1,466",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 44.13,
            "value": 10.51,
            "unified": 446.8,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "101c/s",
            "CodeArena": "1,148",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.37,
            "value": 40.24,
            "unified": 676.96,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "182c/s",
            "CodeArena": "1,097",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.65,
            "value": 7.9,
            "unified": 211.35,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "116c/s",
            "CodeArena": "1,052",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.95,
            "value": 15.69,
            "unified": 456.14,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "204c/s",
            "CodeArena": "1,030",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.42,
            "value": 20.41,
            "unified": 532.53,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "122c/s",
            "CodeArena": "914",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.53,
            "value": 13.02,
            "unified": 200.7,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "153c/s",
            "CodeArena": "892",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.48,
            "value": 0.8,
            "unified": -0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "112c/s",
            "CodeArena": "880",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 33.04,
            "value": 47.2,
            "unified": 412.52,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "10c/s",
            "CodeArena": "826",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-28T15:25:55+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 819.68,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "102c/s",
            "CodeArena": "2,032",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 60.1,
            "value": 2.0,
            "unified": 596.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "74c/s",
            "CodeArena": "1,562",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 72.98,
            "value": 5.21,
            "unified": 735.24,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "41c/s",
            "CodeArena": "1,561",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.59,
            "value": 4.35,
            "unified": 688.42,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "55c/s",
            "CodeArena": "1,520",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.47,
            "value": 18.99,
            "unified": 698.18,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "64c/s",
            "CodeArena": "1,517",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 89.98,
            "value": 5.14,
            "unified": 909.48,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "78c/s",
            "CodeArena": "1,496",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 60.95,
            "value": 3.39,
            "unified": 607.94,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "138c/s",
            "CodeArena": "1,335",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 53.88,
            "value": 4.79,
            "unified": 538.4,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "53c/s",
            "CodeArena": "1,252",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 45.82,
            "value": 4.07,
            "unified": 454.24,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "94c/s",
            "CodeArena": "1,200",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 57.91,
            "value": 5.15,
            "unified": 580.55,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "87c/s",
            "CodeArena": "1,123",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 71.97,
            "value": 17.14,
            "unified": 750.58,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "62c/s",
            "CodeArena": "1,545",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.66,
            "value": 21.18,
            "unified": 694.57,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "140c/s",
            "CodeArena": "1,473",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 59.96,
            "value": 39.97,
            "unified": 676.55,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "184c/s",
            "CodeArena": "1,092",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.55,
            "value": 7.87,
            "unified": 213.49,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "116c/s",
            "CodeArena": "1,052",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.83,
            "value": 9.96,
            "unified": 426.0,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "101c/s",
            "CodeArena": "1,030",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.81,
            "value": 15.64,
            "unified": 458.47,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "207c/s",
            "CodeArena": "1,027",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.37,
            "value": 20.39,
            "unified": 535.99,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "120c/s",
            "CodeArena": "913",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.24,
            "value": 0.72,
            "unified": 0.02,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "101c/s",
            "CodeArena": "876",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.14,
            "value": 12.76,
            "unified": 199.25,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "152c/s",
            "CodeArena": "870",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 33.04,
            "value": 47.2,
            "unified": 415.94,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "24c/s",
            "CodeArena": "826",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-28T03:57:19+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 817.14,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "101c/s",
            "CodeArena": "2,044",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.21,
            "value": 5.23,
            "unified": 732.08,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "41c/s",
            "CodeArena": "1,566",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.45,
            "value": 2.05,
            "unified": 607.64,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "89c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.44,
            "value": 18.98,
            "unified": 681.18,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "64c/s",
            "CodeArena": "1,520",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.0,
            "value": 4.32,
            "unified": 677.63,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "55c/s",
            "CodeArena": "1,517",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.21,
            "value": 5.15,
            "unified": 905.89,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "81c/s",
            "CodeArena": "1,494",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.1,
            "value": 3.39,
            "unified": 605.85,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "148c/s",
            "CodeArena": "1,327",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.3,
            "value": 4.83,
            "unified": 538.15,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "61c/s",
            "CodeArena": "1,256",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.39,
            "value": 4.12,
            "unified": 456.36,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "111c/s",
            "CodeArena": "1,200",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.12,
            "value": 1.9,
            "unified": 327.88,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "141c/s",
            "CodeArena": "1,125",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.06,
            "value": 17.16,
            "unified": 736.31,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "54c/s",
            "CodeArena": "1,548",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.61,
            "value": 21.17,
            "unified": 675.68,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "169c/s",
            "CodeArena": "1,467",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.58,
            "value": 40.39,
            "unified": 649.81,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "190c/s",
            "CodeArena": "1,093",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.22,
            "value": 8.11,
            "unified": 214.41,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "99c/s",
            "CodeArena": "1,052",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.76,
            "value": 15.63,
            "unified": 444.79,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "205c/s",
            "CodeArena": "1,027",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.05,
            "value": 9.77,
            "unified": 409.2,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "128c/s",
            "CodeArena": "1,017",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.49,
            "value": 20.44,
            "unified": 520.03,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "123c/s",
            "CodeArena": "900",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.74,
            "value": 13.16,
            "unified": 195.83,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "145c/s",
            "CodeArena": "870",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.22,
            "value": 0.72,
            "unified": 0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "104c/s",
            "CodeArena": "865",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 387.21,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "323c/s",
            "CodeArena": "813",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-27T15:28:45+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 817.69,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "102c/s",
            "CodeArena": "2,042",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.22,
            "value": 5.23,
            "unified": 732.74,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "43c/s",
            "CodeArena": "1,566",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.48,
            "value": 2.05,
            "unified": 608.28,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "99c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.45,
            "value": 18.99,
            "unified": 681.76,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "75c/s",
            "CodeArena": "1,520",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.02,
            "value": 4.32,
            "unified": 678.21,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "55c/s",
            "CodeArena": "1,517",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.15,
            "value": 5.15,
            "unified": 905.91,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "87c/s",
            "CodeArena": "1,489",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.13,
            "value": 3.4,
            "unified": 606.5,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "141c/s",
            "CodeArena": "1,328",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.54,
            "value": 4.85,
            "unified": 541.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "62c/s",
            "CodeArena": "1,263",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.41,
            "value": 4.13,
            "unified": 456.82,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "121c/s",
            "CodeArena": "1,200",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.13,
            "value": 1.9,
            "unified": 328.16,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "142c/s",
            "CodeArena": "1,125",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.04,
            "value": 17.15,
            "unified": 736.53,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "56c/s",
            "CodeArena": "1,546",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.54,
            "value": 21.14,
            "unified": 675.27,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "167c/s",
            "CodeArena": "1,461",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.63,
            "value": 40.42,
            "unified": 650.72,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "190c/s",
            "CodeArena": "1,094",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.23,
            "value": 8.11,
            "unified": 214.54,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "110c/s",
            "CodeArena": "1,052",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.78,
            "value": 15.64,
            "unified": 445.26,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "214c/s",
            "CodeArena": "1,028",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.11,
            "value": 9.79,
            "unified": 410.03,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "132c/s",
            "CodeArena": "1,020",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.49,
            "value": 20.44,
            "unified": 520.34,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "126c/s",
            "CodeArena": "900",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.75,
            "value": 13.16,
            "unified": 195.88,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "156c/s",
            "CodeArena": "870",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.23,
            "value": 0.72,
            "unified": -0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "87c/s",
            "CodeArena": "865",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 387.34,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "443c/s",
            "CodeArena": "813",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-27T04:07:17+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 817.69,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "107c/s",
            "CodeArena": "2,042",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "",
            "created": "Nov. 2025",
            "avgIq": 73.22,
            "value": 5.23,
            "unified": 732.74,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "45c/s",
            "CodeArena": "1,566",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.48,
            "value": 2.05,
            "unified": 608.28,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "114c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.45,
            "value": 18.99,
            "unified": 681.76,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "76c/s",
            "CodeArena": "1,520",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.02,
            "value": 4.32,
            "unified": 678.21,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "62c/s",
            "CodeArena": "1,517",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.15,
            "value": 5.15,
            "unified": 905.91,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "87c/s",
            "CodeArena": "1,489",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.13,
            "value": 3.4,
            "unified": 606.5,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "151c/s",
            "CodeArena": "1,328",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.54,
            "value": 4.85,
            "unified": 541.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "64c/s",
            "CodeArena": "1,263",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.41,
            "value": 4.13,
            "unified": 456.82,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "119c/s",
            "CodeArena": "1,200",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.13,
            "value": 1.9,
            "unified": 328.16,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "145c/s",
            "CodeArena": "1,125",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.04,
            "value": 17.15,
            "unified": 736.53,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "48c/s",
            "CodeArena": "1,546",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.54,
            "value": 21.14,
            "unified": 675.27,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "170c/s",
            "CodeArena": "1,461",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.63,
            "value": 40.42,
            "unified": 650.72,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "191c/s",
            "CodeArena": "1,094",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.23,
            "value": 8.11,
            "unified": 214.54,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "113c/s",
            "CodeArena": "1,052",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.78,
            "value": 15.64,
            "unified": 445.26,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "212c/s",
            "CodeArena": "1,028",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.11,
            "value": 9.79,
            "unified": 410.03,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "130c/s",
            "CodeArena": "1,020",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.49,
            "value": 20.44,
            "unified": 520.34,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "122c/s",
            "CodeArena": "900",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.75,
            "value": 13.16,
            "unified": 195.88,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "157c/s",
            "CodeArena": "870",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.23,
            "value": 0.72,
            "unified": -0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "84c/s",
            "CodeArena": "865",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 387.34,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "449c/s",
            "CodeArena": "813",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-26T04:03:57+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 813.58,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "125c/s",
            "CodeArena": "2,016",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.36,
            "value": 5.24,
            "unified": 733.52,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "47c/s",
            "CodeArena": "1,567",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.77,
            "value": 2.06,
            "unified": 608.25,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "138c/s",
            "CodeArena": "1,561",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.62,
            "value": 4.36,
            "unified": 683.16,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "115c/s",
            "CodeArena": "1,532",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.59,
            "value": 5.18,
            "unified": 909.36,
            "Model": "Gemini 3.1 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "98c/s",
            "CodeArena": "1,514",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.7,
            "value": 19.06,
            "unified": 695.22,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "75c/s",
            "CodeArena": "1,510",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.09,
            "value": 3.39,
            "unified": 604.1,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "158c/s",
            "CodeArena": "1,329",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.68,
            "value": 4.86,
            "unified": 541.84,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,261",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.66,
            "value": 4.15,
            "unified": 458.33,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "120c/s",
            "CodeArena": "1,210",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 33.09,
            "value": 1.84,
            "unified": 314.69,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,118",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 71.41,
            "value": 17.0,
            "unified": 738.98,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "32c/s",
            "CodeArena": "1,511",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.86,
            "value": 21.25,
            "unified": 691.4,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "212c/s",
            "CodeArena": "1,470",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.18,
            "value": 40.79,
            "unified": 685.67,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "187c/s",
            "CodeArena": "1,112",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.7,
            "value": 15.61,
            "unified": 452.76,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "209c/s",
            "CodeArena": "1,032",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.37,
            "value": 7.8,
            "unified": 207.75,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "127c/s",
            "CodeArena": "1,028",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.54,
            "value": 9.89,
            "unified": 418.41,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "45c/s",
            "CodeArena": "1,024",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.24,
            "value": 20.34,
            "unified": 529.84,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "160c/s",
            "CodeArena": "900",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.51,
            "value": 0.81,
            "unified": 0.02,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "89c/s",
            "CodeArena": "867",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.1,
            "value": 12.74,
            "unified": 195.27,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "167c/s",
            "CodeArena": "842",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 33.04,
            "value": 47.2,
            "unified": 411.96,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "117c/s",
            "CodeArena": "811",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-25T15:37:53+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 808.6,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "126c/s",
            "CodeArena": "2,005",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.4,
            "value": 5.24,
            "unified": 729.48,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "53c/s",
            "CodeArena": "1,563",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.91,
            "value": 2.06,
            "unified": 605.97,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "151c/s",
            "CodeArena": "1,561",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 91.14,
            "value": 5.21,
            "unified": 909.54,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "98c/s",
            "CodeArena": "1,544",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.65,
            "value": 4.36,
            "unified": 679.32,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "140c/s",
            "CodeArena": "1,528",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.75,
            "value": 19.07,
            "unified": 691.8,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "78c/s",
            "CodeArena": "1,508",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.13,
            "value": 3.4,
            "unified": 600.97,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "160c/s",
            "CodeArena": "1,328",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.9,
            "value": 4.88,
            "unified": 540.88,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,264",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 47.06,
            "value": 4.18,
            "unified": 459.8,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "117c/s",
            "CodeArena": "1,219",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 33.07,
            "value": 1.84,
            "unified": 312.66,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "155c/s",
            "CodeArena": "1,115",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 71.63,
            "value": 17.05,
            "unified": 736.93,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "32c/s",
            "CodeArena": "1,512",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.95,
            "value": 21.28,
            "unified": 688.43,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "222c/s",
            "CodeArena": "1,470",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.31,
            "value": 40.2,
            "unified": 671.9,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "204c/s",
            "CodeArena": "1,085",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.49,
            "value": 7.84,
            "unified": 208.1,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "125c/s",
            "CodeArena": "1,034",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.72,
            "value": 15.61,
            "unified": 450.5,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "207c/s",
            "CodeArena": "1,032",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.27,
            "value": 9.83,
            "unified": 413.17,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "40c/s",
            "CodeArena": "1,008",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.24,
            "value": 20.34,
            "unified": 526.9,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "167c/s",
            "CodeArena": "900",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.49,
            "value": 0.8,
            "unified": 0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "867",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 18.76,
            "value": 12.51,
            "unified": 190.42,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "180c/s",
            "CodeArena": "822",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 33.04,
            "value": 47.2,
            "unified": 410.16,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "126c/s",
            "CodeArena": "812",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-25T04:06:06+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 833.13,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "129c/s",
            "CodeArena": "2,010",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.96,
            "value": 2.07,
            "unified": 628.1,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "187c/s",
            "CodeArena": "1,566",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.15,
            "value": 5.23,
            "unified": 749.9,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "58c/s",
            "CodeArena": "1,549",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.53,
            "value": 4.35,
            "unified": 700.49,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "133c/s",
            "CodeArena": "1,521",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.76,
            "value": 19.07,
            "unified": 712.51,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "81c/s",
            "CodeArena": "1,511",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 88.68,
            "value": 5.07,
            "unified": 909.52,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "100c/s",
            "CodeArena": "1,387",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.45,
            "value": 3.41,
            "unified": 625.68,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "164c/s",
            "CodeArena": "1,351",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.86,
            "value": 4.88,
            "unified": 560.75,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "68c/s",
            "CodeArena": "1,264",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 45.93,
            "value": 4.08,
            "unified": 467.11,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "127c/s",
            "CodeArena": "1,186",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 32.98,
            "value": 1.83,
            "unified": 329.14,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "156c/s",
            "CodeArena": "1,112",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 71.46,
            "value": 17.01,
            "unified": 756.69,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "22c/s",
            "CodeArena": "1,509",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.93,
            "value": 21.27,
            "unified": 708.49,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "225c/s",
            "CodeArena": "1,471",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.58,
            "value": 41.05,
            "unified": 704.28,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "196c/s",
            "CodeArena": "1,122",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.49,
            "value": 7.84,
            "unified": 223.13,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "132c/s",
            "CodeArena": "1,034",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.72,
            "value": 15.61,
            "unified": 468.04,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "248c/s",
            "CodeArena": "1,032",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.25,
            "value": 9.82,
            "unified": 430.77,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "45c/s",
            "CodeArena": "1,007",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.25,
            "value": 20.34,
            "unified": 545.06,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "169c/s",
            "CodeArena": "900",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 34.38,
            "value": 49.12,
            "unified": 440.71,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "132c/s",
            "CodeArena": "865",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 1.31,
            "value": 0.42,
            "unified": -0.02,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 18.59,
            "value": 12.4,
            "unified": 202.65,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "183c/s",
            "CodeArena": "811",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-24T15:37:55+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.73,
            "value": 2.72,
            "unified": 833.0,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "134c/s",
            "CodeArena": "2,000",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.09,
            "value": 2.07,
            "unified": 629.34,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "225c/s",
            "CodeArena": "1,566",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.24,
            "value": 5.23,
            "unified": 750.74,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "63c/s",
            "CodeArena": "1,549",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.93,
            "value": 19.12,
            "unified": 714.27,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "80c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.55,
            "value": 4.35,
            "unified": 700.62,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "134c/s",
            "CodeArena": "1,517",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 88.69,
            "value": 5.07,
            "unified": 909.53,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "101c/s",
            "CodeArena": "1,383",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.55,
            "value": 3.42,
            "unified": 626.57,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "167c/s",
            "CodeArena": "1,353",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.99,
            "value": 4.89,
            "unified": 561.96,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "67c/s",
            "CodeArena": "1,264",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.0,
            "value": 4.09,
            "unified": 467.74,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "129c/s",
            "CodeArena": "1,185",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 32.97,
            "value": 1.83,
            "unified": 328.89,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "154c/s",
            "CodeArena": "1,109",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 71.85,
            "value": 17.11,
            "unified": 760.79,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "16c/s",
            "CodeArena": "1,516",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.96,
            "value": 21.28,
            "unified": 708.63,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "224c/s",
            "CodeArena": "1,467",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.32,
            "value": 40.88,
            "unified": 701.11,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "186c/s",
            "CodeArena": "1,112",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 21.65,
            "value": 7.9,
            "unified": 224.72,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "117c/s",
            "CodeArena": "1,041",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 43.75,
            "value": 15.62,
            "unified": 468.26,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "225c/s",
            "CodeArena": "1,032",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 41.39,
            "value": 9.86,
            "unified": 432.14,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "45c/s",
            "CodeArena": "1,013",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.23,
            "value": 20.34,
            "unified": 544.68,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "161c/s",
            "CodeArena": "898",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 34.4,
            "value": 49.14,
            "unified": 440.71,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "139c/s",
            "CodeArena": "865",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 1.32,
            "value": 0.43,
            "unified": -0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "81c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 18.59,
            "value": 12.4,
            "unified": 202.52,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "188c/s",
            "CodeArena": "811",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-24T04:05:16+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 815.79,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "148c/s",
            "CodeArena": "1,999",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.33,
            "value": 2.08,
            "unified": 614.1,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "243c/s",
            "CodeArena": "1,565",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.61,
            "value": 5.26,
            "unified": 734.47,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "63c/s",
            "CodeArena": "1,552",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.96,
            "value": 19.13,
            "unified": 684.59,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "96c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.38,
            "value": 4.34,
            "unified": 679.36,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "136c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 90.27,
            "value": 5.16,
            "unified": 905.64,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "101c/s",
            "CodeArena": "1,456",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.55,
            "value": 3.48,
            "unified": 618.26,
            "Model": "Claude Sonnet 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "178c/s",
            "CodeArena": "1,387",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 55.56,
            "value": 4.94,
            "unified": 548.34,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "69c/s",
            "CodeArena": "1,255",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.75,
            "value": 4.16,
            "unified": 456.63,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "124c/s",
            "CodeArena": "1,171",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.84,
            "value": 1.94,
            "unified": 331.13,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "155c/s",
            "CodeArena": "1,116",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 71.98,
            "value": 17.14,
            "unified": 733.55,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "24c/s",
            "CodeArena": "1,503",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.45,
            "value": 21.44,
            "unified": 682.38,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "227c/s",
            "CodeArena": "1,479",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.06,
            "value": 10.97,
            "unified": 458.58,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "53c/s",
            "CodeArena": "1,263",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.7,
            "value": 41.13,
            "unified": 659.83,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "184c/s",
            "CodeArena": "1,087",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.69,
            "value": 8.28,
            "unified": 214.56,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "116c/s",
            "CodeArena": "1,041",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.51,
            "value": 15.9,
            "unified": 449.24,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "264c/s",
            "CodeArena": "1,033",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.76,
            "value": 20.55,
            "unified": 519.81,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "152c/s",
            "CodeArena": "878",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.79,
            "value": 0.9,
            "unified": 0.05,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "77c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.98,
            "unified": 187.67,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "205c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 383.02,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "528c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-23T15:33:20+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 824.64,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "152c/s",
            "CodeArena": "2,005",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.23,
            "value": 2.07,
            "unified": 619.76,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "261c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.53,
            "value": 5.25,
            "unified": 741.53,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "126c/s",
            "CodeArena": "1,550",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.92,
            "value": 19.12,
            "unified": 691.27,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "133c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.33,
            "value": 4.34,
            "unified": 686.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "138c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 89.33,
            "value": 5.1,
            "unified": 905.6,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "102c/s",
            "CodeArena": "1,396",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.59,
            "value": 3.48,
            "unified": 625.42,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "183c/s",
            "CodeArena": "1,393",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.67,
            "value": 5.04,
            "unified": 565.89,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "74c/s",
            "CodeArena": "1,292",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.79,
            "value": 4.16,
            "unified": 461.95,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "113c/s",
            "CodeArena": "1,174",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.73,
            "value": 1.93,
            "unified": 333.64,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "162c/s",
            "CodeArena": "1,113",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.49,
            "value": 17.26,
            "unified": 746.73,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "26c/s",
            "CodeArena": "1,524",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.43,
            "value": 21.43,
            "unified": 689.28,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "229c/s",
            "CodeArena": "1,481",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.24,
            "value": 11.01,
            "unified": 465.46,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "54c/s",
            "CodeArena": "1,276",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.05,
            "value": 40.7,
            "unified": 659.13,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "179c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.75,
            "value": 8.3,
            "unified": 217.53,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "113c/s",
            "CodeArena": "1,046",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.54,
            "value": 15.91,
            "unified": 454.27,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "288c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.67,
            "value": 20.52,
            "unified": 524.23,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "126c/s",
            "CodeArena": "873",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.78,
            "value": 0.9,
            "unified": 0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.97,
            "unified": 189.58,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "212c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 386.17,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "532c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-23T04:08:27+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 824.64,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "157c/s",
            "CodeArena": "2,005",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.23,
            "value": 2.07,
            "unified": 619.76,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "262c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.53,
            "value": 5.25,
            "unified": 741.53,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "130c/s",
            "CodeArena": "1,550",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.92,
            "value": 19.12,
            "unified": 691.27,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "152c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.33,
            "value": 4.34,
            "unified": 686.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "141c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 89.33,
            "value": 5.1,
            "unified": 905.6,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "104c/s",
            "CodeArena": "1,396",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.59,
            "value": 3.48,
            "unified": 625.42,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "189c/s",
            "CodeArena": "1,393",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.67,
            "value": 5.04,
            "unified": 565.89,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "76c/s",
            "CodeArena": "1,292",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.79,
            "value": 4.16,
            "unified": 461.95,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "113c/s",
            "CodeArena": "1,174",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.73,
            "value": 1.93,
            "unified": 333.64,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "165c/s",
            "CodeArena": "1,113",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.49,
            "value": 17.26,
            "unified": 746.73,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "32c/s",
            "CodeArena": "1,524",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.43,
            "value": 21.43,
            "unified": 689.28,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "233c/s",
            "CodeArena": "1,481",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.24,
            "value": 11.01,
            "unified": 465.46,
            "Model": "Qwen3.5-397B-A17B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "54c/s",
            "CodeArena": "1,276",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.05,
            "value": 40.7,
            "unified": 659.13,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "177c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.75,
            "value": 8.3,
            "unified": 217.53,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "113c/s",
            "CodeArena": "1,046",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.54,
            "value": 15.91,
            "unified": 454.27,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "296c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.67,
            "value": 20.52,
            "unified": 524.23,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "124c/s",
            "CodeArena": "873",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.78,
            "value": 0.9,
            "unified": 0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.97,
            "unified": 189.58,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "211c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 386.17,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "530c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-22T15:26:19+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 824.64,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "159c/s",
            "CodeArena": "2,005",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.23,
            "value": 2.07,
            "unified": 619.76,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "264c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.53,
            "value": 5.25,
            "unified": 741.53,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "131c/s",
            "CodeArena": "1,550",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.92,
            "value": 19.12,
            "unified": 691.27,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "154c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.33,
            "value": 4.34,
            "unified": 686.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "144c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 89.33,
            "value": 5.1,
            "unified": 905.6,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "103c/s",
            "CodeArena": "1,396",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.59,
            "value": 3.48,
            "unified": 625.42,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "193c/s",
            "CodeArena": "1,393",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.67,
            "value": 5.04,
            "unified": 565.89,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "87c/s",
            "CodeArena": "1,292",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.79,
            "value": 4.16,
            "unified": 461.95,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "113c/s",
            "CodeArena": "1,174",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.73,
            "value": 1.93,
            "unified": 333.64,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "166c/s",
            "CodeArena": "1,113",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.49,
            "value": 17.26,
            "unified": 746.73,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "36c/s",
            "CodeArena": "1,524",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.43,
            "value": 21.43,
            "unified": 689.28,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "240c/s",
            "CodeArena": "1,481",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.24,
            "value": 11.01,
            "unified": 465.46,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "54c/s",
            "CodeArena": "1,276",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.05,
            "value": 40.7,
            "unified": 659.13,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.75,
            "value": 8.3,
            "unified": 217.53,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "116c/s",
            "CodeArena": "1,046",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.54,
            "value": 15.91,
            "unified": 454.27,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "294c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.67,
            "value": 20.52,
            "unified": 524.23,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "119c/s",
            "CodeArena": "873",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.78,
            "value": 0.9,
            "unified": 0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.97,
            "unified": 189.58,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "211c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 386.17,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "533c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-22T04:04:01+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 824.64,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "161c/s",
            "CodeArena": "2,005",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.23,
            "value": 2.07,
            "unified": 619.76,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "268c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.53,
            "value": 5.25,
            "unified": 741.53,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "134c/s",
            "CodeArena": "1,550",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.92,
            "value": 19.12,
            "unified": 691.27,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "155c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.33,
            "value": 4.34,
            "unified": 686.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "157c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 89.33,
            "value": 5.1,
            "unified": 905.6,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "102c/s",
            "CodeArena": "1,396",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.59,
            "value": 3.48,
            "unified": 625.42,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "195c/s",
            "CodeArena": "1,393",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.67,
            "value": 5.04,
            "unified": 565.89,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "87c/s",
            "CodeArena": "1,292",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.79,
            "value": 4.16,
            "unified": 461.95,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "114c/s",
            "CodeArena": "1,174",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.73,
            "value": 1.93,
            "unified": 333.64,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "169c/s",
            "CodeArena": "1,113",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.49,
            "value": 17.26,
            "unified": 746.73,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "37c/s",
            "CodeArena": "1,524",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.43,
            "value": 21.43,
            "unified": 689.28,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "244c/s",
            "CodeArena": "1,481",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.24,
            "value": 11.01,
            "unified": 465.46,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "54c/s",
            "CodeArena": "1,276",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.05,
            "value": 40.7,
            "unified": 659.13,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.75,
            "value": 8.3,
            "unified": 217.53,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "116c/s",
            "CodeArena": "1,046",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.54,
            "value": 15.91,
            "unified": 454.27,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "289c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.67,
            "value": 20.52,
            "unified": 524.23,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "119c/s",
            "CodeArena": "873",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.78,
            "value": 0.9,
            "unified": 0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "77c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.97,
            "unified": 189.58,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "211c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 386.17,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "537c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-21T15:26:17+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 824.64,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "164c/s",
            "CodeArena": "2,005",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.23,
            "value": 2.07,
            "unified": 619.76,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "261c/s",
            "CodeArena": "1,564",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.53,
            "value": 5.25,
            "unified": 741.53,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "144c/s",
            "CodeArena": "1,550",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.92,
            "value": 19.12,
            "unified": 691.27,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "159c/s",
            "CodeArena": "1,518",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.33,
            "value": 4.34,
            "unified": 686.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "158c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 89.33,
            "value": 5.1,
            "unified": 905.6,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "96c/s",
            "CodeArena": "1,396",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.59,
            "value": 3.48,
            "unified": 625.42,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "196c/s",
            "CodeArena": "1,393",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.67,
            "value": 5.04,
            "unified": 565.89,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "88c/s",
            "CodeArena": "1,292",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.79,
            "value": 4.16,
            "unified": 461.95,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "110c/s",
            "CodeArena": "1,174",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.73,
            "value": 1.93,
            "unified": 333.64,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "169c/s",
            "CodeArena": "1,113",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.49,
            "value": 17.26,
            "unified": 746.73,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "45c/s",
            "CodeArena": "1,524",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.43,
            "value": 21.43,
            "unified": 689.28,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "246c/s",
            "CodeArena": "1,481",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.24,
            "value": 11.01,
            "unified": 465.46,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "54c/s",
            "CodeArena": "1,276",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.05,
            "value": 40.7,
            "unified": 659.13,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "170c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.75,
            "value": 8.3,
            "unified": 217.53,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "115c/s",
            "CodeArena": "1,046",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.54,
            "value": 15.91,
            "unified": 454.27,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "267c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.67,
            "value": 20.52,
            "unified": 524.23,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "116c/s",
            "CodeArena": "873",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.78,
            "value": 0.9,
            "unified": 0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "77c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.97,
            "unified": 189.58,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "209c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 386.17,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "544c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-21T03:59:19+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 831.23,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "138c/s",
            "CodeArena": "1,998",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.1,
            "value": 2.07,
            "unified": 623.28,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "259c/s",
            "CodeArena": "1,553",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.62,
            "value": 5.26,
            "unified": 748.38,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "144c/s",
            "CodeArena": "1,552",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.88,
            "value": 19.11,
            "unified": 696.18,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "158c/s",
            "CodeArena": "1,511",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.38,
            "value": 4.34,
            "unified": 692.22,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "65c/s",
            "CodeArena": "1,506",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 88.64,
            "value": 5.07,
            "unified": 905.54,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "95c/s",
            "CodeArena": "1,346",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 61.88,
            "value": 3.44,
            "unified": 622.86,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "192c/s",
            "CodeArena": "1,340",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.01,
            "value": 4.8,
            "unified": 542.08,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "85c/s",
            "CodeArena": "1,206",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.86,
            "value": 4.17,
            "unified": 466.3,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "107c/s",
            "CodeArena": "1,174",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.52,
            "value": 1.92,
            "unified": 333.94,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "158c/s",
            "CodeArena": "1,102",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.61,
            "value": 17.29,
            "unified": 753.86,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "47c/s",
            "CodeArena": "1,524",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.35,
            "value": 21.4,
            "unified": 693.69,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "202c/s",
            "CodeArena": "1,472",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.49,
            "value": 11.07,
            "unified": 471.62,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "53c/s",
            "CodeArena": "1,287",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 61.18,
            "value": 40.78,
            "unified": 665.33,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "170c/s",
            "CodeArena": "1,071",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.66,
            "value": 8.27,
            "unified": 218.04,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "118c/s",
            "CodeArena": "1,039",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.59,
            "value": 15.93,
            "unified": 458.28,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "331c/s",
            "CodeArena": "1,038",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.78,
            "value": 20.56,
            "unified": 529.33,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "112c/s",
            "CodeArena": "879",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.8,
            "value": 0.9,
            "unified": -0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.46,
            "value": 12.98,
            "unified": 190.85,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "213c/s",
            "CodeArena": "814",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 388.32,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "553c/s",
            "CodeArena": "775",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-20T15:29:15+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 836.14,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "134c/s",
            "CodeArena": "2,001",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.66,
            "value": 5.26,
            "unified": 753.37,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "75c/s",
            "CodeArena": "1,558",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.94,
            "value": 2.06,
            "unified": 625.89,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "258c/s",
            "CodeArena": "1,549",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.79,
            "value": 19.08,
            "unified": 699.6,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "34c/s",
            "CodeArena": "1,508",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.29,
            "value": 4.34,
            "unified": 695.68,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "55c/s",
            "CodeArena": "1,502",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.1,
            "value": 3.45,
            "unified": 629.38,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "190c/s",
            "CodeArena": "1,359",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 88.16,
            "value": 5.04,
            "unified": 905.62,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "83c/s",
            "CodeArena": "1,318",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.16,
            "value": 4.81,
            "unified": 547.68,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "84c/s",
            "CodeArena": "1,215",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.46,
            "value": 4.13,
            "unified": 465.84,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "102c/s",
            "CodeArena": "1,166",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.42,
            "value": 1.91,
            "unified": 336.18,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,102",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.56,
            "value": 17.28,
            "unified": 757.89,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "51c/s",
            "CodeArena": "1,526",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.38,
            "value": 21.41,
            "unified": 698.44,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "205c/s",
            "CodeArena": "1,478",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 45.94,
            "value": 10.94,
            "unified": 469.4,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "52c/s",
            "CodeArena": "1,260",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.96,
            "value": 40.64,
            "unified": 667.03,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "170c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.57,
            "value": 8.24,
            "unified": 219.98,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "114c/s",
            "CodeArena": "1,038",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.45,
            "value": 15.88,
            "unified": 460.39,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "332c/s",
            "CodeArena": "1,034",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.7,
            "value": 20.53,
            "unified": 532.31,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "110c/s",
            "CodeArena": "879",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.59,
            "value": 0.83,
            "unified": -0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "78c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.5,
            "value": 13.0,
            "unified": 194.06,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "216c/s",
            "CodeArena": "821",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 391.44,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "544c/s",
            "CodeArena": "780",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-20T13:26:39+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.85,
            "value": 2.73,
            "unified": 845.61,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "134c/s",
            "CodeArena": "2,002",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 73.65,
            "value": 5.26,
            "unified": 761.77,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "75c/s",
            "CodeArena": "1,558",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 61.87,
            "value": 2.06,
            "unified": 632.22,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "260c/s",
            "CodeArena": "1,546",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 66.78,
            "value": 19.08,
            "unified": 707.2,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "31c/s",
            "CodeArena": "1,508",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 68.31,
            "value": 4.34,
            "unified": 703.73,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "54c/s",
            "CodeArena": "1,504",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-6",
            "origin": "US",
            "description": "Claude Sonnet 4.6 is a full upgrade of the model's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. Users preferred Sonnet 4.6 over Sonnet 4.5 ap",
            "created": "Feb. 2026",
            "avgIq": 62.39,
            "value": 3.47,
            "unified": 639.67,
            "Model": "Claude Sonnet 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "194c/s",
            "CodeArena": "1,380",
            "GPQA": "89.9%",
            "AIME2025": "-",
            "SWE-benchVerified": "79.6%",
            "ARC-AGIv2": "58.3%",
            "MMMLU": "89.3%",
            "MMMU": "-",
            "BrowseComp": "74.7%",
            "CharXiv-R": "-",
            "MMMU-Pro": "75.6%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "61.3%",
            "HLE": "49.0%",
            "SimpleQA": "-",
            "OSWorld": "72.5%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 87.2,
            "value": 4.98,
            "unified": 905.56,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "114c/s",
            "CodeArena": "1,254",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.14,
            "value": 4.81,
            "unified": 553.75,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "84c/s",
            "CodeArena": "1,215",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.45,
            "value": 4.13,
            "unified": 471.01,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "100c/s",
            "CodeArena": "1,166",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 34.41,
            "value": 1.91,
            "unified": 339.98,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,102",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.54,
            "value": 17.27,
            "unified": 766.06,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "51c/s",
            "CodeArena": "1,526",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.37,
            "value": 21.41,
            "unified": 705.98,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "205c/s",
            "CodeArena": "1,478",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 45.93,
            "value": 10.94,
            "unified": 474.55,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "52c/s",
            "CodeArena": "1,260",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.95,
            "value": 40.63,
            "unified": 673.94,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "170c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 22.56,
            "value": 8.23,
            "unified": 222.41,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "117c/s",
            "CodeArena": "1,038",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.35,
            "value": 15.84,
            "unified": 464.29,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "332c/s",
            "CodeArena": "1,028",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.7,
            "value": 20.53,
            "unified": 538.09,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "111c/s",
            "CodeArena": "879",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.58,
            "value": 0.83,
            "unified": 0.05,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "78c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.5,
            "value": 13.0,
            "unified": 196.16,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "216c/s",
            "CodeArena": "821",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.3,
            "value": 75.75,
            "unified": 394.82,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "544c/s",
            "CodeArena": "780",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-20T04:02:44+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 81.2,
            "value": 2.71,
            "unified": 884.56,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "134c/s",
            "CodeArena": "2,000",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 74.18,
            "value": 5.3,
            "unified": 809.15,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "76c/s",
            "CodeArena": "1,559",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 62.82,
            "value": 2.09,
            "unified": 677.44,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "261c/s",
            "CodeArena": "1,533",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 69.57,
            "value": 4.42,
            "unified": 756.3,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "53c/s",
            "CodeArena": "1,510",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 67.67,
            "value": 19.33,
            "unified": 754.51,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "32c/s",
            "CodeArena": "1,507",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 54.57,
            "value": 4.85,
            "unified": 588.47,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "86c/s",
            "CodeArena": "1,217",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 54.04,
            "value": 4.8,
            "unified": 582.46,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "275c/s",
            "CodeArena": "1,120",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 35.17,
            "value": 1.95,
            "unified": 367.0,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "156c/s",
            "CodeArena": "1,102",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 42.94,
            "value": 3.82,
            "unified": 456.68,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "100c/s",
            "CodeArena": "1,057",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3.1 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3.1-pro-preview",
            "origin": "US",
            "description": "Gemini 3.1 Pro is the latest model in the Gemini 3 series. It excels at complex tasks requiring broad world knowledge and advanced reasoning across modalities. Gemini 3.1 Pro uses dynamic thinking by ",
            "created": "Feb. 2026",
            "avgIq": 82.8,
            "value": 4.73,
            "unified": 905.07,
            "Model": "Gemini 3.1 Pro\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.50",
            "Output$/M": "$15.00",
            "Speed": "133c/s",
            "CodeArena": "1,030",
            "GPQA": "94.3%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.6%",
            "ARC-AGIv2": "77.1%",
            "MMMLU": "92.6%",
            "MMMU": "-",
            "BrowseComp": "85.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "80.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "69.2%",
            "HLE": "51.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 72.81,
            "value": 17.34,
            "unified": 809.59,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "57c/s",
            "CodeArena": "1,545",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.12,
            "value": 21.01,
            "unified": 728.13,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "208c/s",
            "CodeArena": "1,478",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 46.25,
            "value": 11.01,
            "unified": 503.24,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "52c/s",
            "CodeArena": "1,248",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.57,
            "value": 40.38,
            "unified": 702.47,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "169c/s",
            "CodeArena": "1,067",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 44.7,
            "value": 15.96,
            "unified": 492.33,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "328c/s",
            "CodeArena": "1,028",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 23.24,
            "value": 8.48,
            "unified": 241.75,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "113c/s",
            "CodeArena": "1,023",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 50.9,
            "value": 20.61,
            "unified": 568.02,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "109c/s",
            "CodeArena": "879",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.59,
            "value": 0.83,
            "unified": -0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "79c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.25,
            "value": 12.83,
            "unified": 202.64,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "215c/s",
            "CodeArena": "827",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 30.84,
            "value": 77.11,
            "unified": 417.01,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "542c/s",
            "CodeArena": "780",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-19T15:37:39+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.89,
            "value": 2.86,
            "unified": 902.52,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "130c/s",
            "CodeArena": "2,000",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 77.94,
            "value": 5.57,
            "unified": 819.98,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "77c/s",
            "CodeArena": "1,563",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 66.97,
            "value": 2.23,
            "unified": 697.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "260c/s",
            "CodeArena": "1,533",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 71.24,
            "value": 20.35,
            "unified": 766.09,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "28c/s",
            "CodeArena": "1,511",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 73.44,
            "value": 4.66,
            "unified": 770.28,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "38c/s",
            "CodeArena": "1,505",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.99,
            "value": 5.07,
            "unified": 593.09,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "86c/s",
            "CodeArena": "1,207",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 37.0,
            "value": 2.06,
            "unified": 373.3,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "153c/s",
            "CodeArena": "1,102",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.18,
            "value": 4.1,
            "unified": 475.06,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "103c/s",
            "CodeArena": "1,079",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 59.55,
            "value": 5.29,
            "unified": 621.01,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "93c/s",
            "CodeArena": "996",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 52.81,
            "value": 4.69,
            "unified": 547.38,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "273c/s",
            "CodeArena": "983",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 75.35,
            "value": 17.94,
            "unified": 807.53,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "58c/s",
            "CodeArena": "1,577",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.92,
            "value": 21.59,
            "unified": 720.95,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "201c/s",
            "CodeArena": "1,478",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 48.52,
            "value": 11.55,
            "unified": 509.65,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "50c/s",
            "CodeArena": "1,246",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.37,
            "value": 40.25,
            "unified": 673.58,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "1,067",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 24.96,
            "value": 9.11,
            "unified": 252.05,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "108c/s",
            "CodeArena": "1,023",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 46.1,
            "value": 16.46,
            "unified": 489.6,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "263c/s",
            "CodeArena": "1,019",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 52.53,
            "value": 21.27,
            "unified": 565.13,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "109c/s",
            "CodeArena": "879",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.59,
            "value": 0.83,
            "unified": -0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "79c/s",
            "CodeArena": "840",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.82,
            "value": 13.21,
            "unified": 201.64,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "211c/s",
            "CodeArena": "823",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 32.34,
            "value": 80.86,
            "unified": 421.45,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "546c/s",
            "CodeArena": "780",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-19T04:06:11+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.89,
            "value": 2.86,
            "unified": 902.75,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "132c/s",
            "CodeArena": "1,992",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 78.05,
            "value": 5.57,
            "unified": 822.01,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "84c/s",
            "CodeArena": "1,565",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.08,
            "value": 2.24,
            "unified": 700.18,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "258c/s",
            "CodeArena": "1,533",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 71.32,
            "value": 20.38,
            "unified": 768.31,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "46c/s",
            "CodeArena": "1,512",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 73.5,
            "value": 4.67,
            "unified": 772.12,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "42c/s",
            "CodeArena": "1,505",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.36,
            "value": 5.1,
            "unified": 599.49,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "84c/s",
            "CodeArena": "1,216",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 37.03,
            "value": 2.06,
            "unified": 377.53,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "153c/s",
            "CodeArena": "1,101",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 46.3,
            "value": 4.12,
            "unified": 479.64,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "103c/s",
            "CodeArena": "1,081",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 59.53,
            "value": 5.29,
            "unified": 623.04,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "93c/s",
            "CodeArena": "994",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 52.86,
            "value": 4.7,
            "unified": 550.72,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "274c/s",
            "CodeArena": "984",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 75.62,
            "value": 18.0,
            "unified": 811.45,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "58c/s",
            "CodeArena": "1,580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.17,
            "value": 21.67,
            "unified": 725.43,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "208c/s",
            "CodeArena": "1,489",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 48.58,
            "value": 11.57,
            "unified": 513.34,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "84c/s",
            "CodeArena": "1,246",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.51,
            "value": 40.34,
            "unified": 677.24,
            "Model": "MiniMax M2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "1,069",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 24.94,
            "value": 9.1,
            "unified": 256.62,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "102c/s",
            "CodeArena": "1,020",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 46.12,
            "value": 16.47,
            "unified": 493.11,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "262c/s",
            "CodeArena": "1,019",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 52.58,
            "value": 21.29,
            "unified": 568.35,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "93c/s",
            "CodeArena": "881",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.0,
            "value": 0.64,
            "unified": -0.02,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "88c/s",
            "CodeArena": "826",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 19.83,
            "value": 13.22,
            "unified": 206.92,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "213c/s",
            "CodeArena": "823",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 32.34,
            "value": 80.86,
            "unified": 425.52,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "544c/s",
            "CodeArena": "780",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-18T15:35:34+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.85,
            "value": 2.86,
            "unified": 903.11,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "132c/s",
            "CodeArena": "1,982",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 78.35,
            "value": 5.6,
            "unified": 825.45,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "84c/s",
            "CodeArena": "1,560",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.41,
            "value": 2.25,
            "unified": 703.21,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "255c/s",
            "CodeArena": "1,535",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 71.85,
            "value": 20.53,
            "unified": 773.71,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "46c/s",
            "CodeArena": "1,519",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 72.84,
            "value": 4.62,
            "unified": 764.8,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "43c/s",
            "CodeArena": "1,500",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 59.08,
            "value": 5.25,
            "unified": 616.95,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "83c/s",
            "CodeArena": "1,224",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 41.34,
            "value": 2.3,
            "unified": 421.82,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "151c/s",
            "CodeArena": "1,092",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 48.88,
            "value": 4.34,
            "unified": 505.69,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "100c/s",
            "CodeArena": "1,079",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 60.47,
            "value": 5.37,
            "unified": 632.12,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "105c/s",
            "CodeArena": "1,009",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 28.31,
            "value": 0.31,
            "unified": 278.66,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "166c/s",
            "CodeArena": "975",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 75.78,
            "value": 18.04,
            "unified": 813.06,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "65c/s",
            "CodeArena": "1,574",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.45,
            "value": 21.76,
            "unified": 727.66,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "216c/s",
            "CodeArena": "1,469",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 49.49,
            "value": 11.78,
            "unified": 521.49,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "146c/s",
            "CodeArena": "1,312",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 60.77,
            "value": 40.52,
            "unified": 678.63,
            "Model": "MiniMax M2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "170c/s",
            "CodeArena": "1,064",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 26.1,
            "value": 9.52,
            "unified": 266.03,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "105c/s",
            "CodeArena": "1,020",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 46.4,
            "value": 16.57,
            "unified": 493.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "261c/s",
            "CodeArena": "1,014",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 22.85,
            "value": 15.23,
            "unified": 237.95,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "211c/s",
            "CodeArena": "829",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.51,
            "value": 0.81,
            "unified": 0.64,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "102c/s",
            "CodeArena": "825",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 51.45,
            "value": 20.83,
            "unified": 553.73,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "85c/s",
            "CodeArena": "782",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 32.66,
            "value": 81.66,
            "unified": 425.64,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "538c/s",
            "CodeArena": "767",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-18T04:07:11+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.85,
            "value": 2.86,
            "unified": 903.11,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "131c/s",
            "CodeArena": "2,033",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 77.84,
            "value": 5.56,
            "unified": 819.84,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "82c/s",
            "CodeArena": "1,557",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 66.87,
            "value": 2.23,
            "unified": 697.1,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "253c/s",
            "CodeArena": "1,540",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 72.05,
            "value": 4.57,
            "unified": 756.01,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "42c/s",
            "CodeArena": "1,466",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 69.75,
            "value": 19.93,
            "unified": 749.95,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "45c/s",
            "CodeArena": "1,393",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 58.59,
            "value": 5.21,
            "unified": 611.13,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "84c/s",
            "CodeArena": "1,223",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 40.97,
            "value": 2.28,
            "unified": 416.97,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "150c/s",
            "CodeArena": "1,084",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 48.62,
            "value": 4.32,
            "unified": 502.21,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "102c/s",
            "CodeArena": "1,079",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 60.31,
            "value": 5.36,
            "unified": 629.93,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "103c/s",
            "CodeArena": "1,009",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 28.24,
            "value": 0.31,
            "unified": 276.86,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "166c/s",
            "CodeArena": "975",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 76.03,
            "value": 18.1,
            "unified": 815.66,
            "Model": "GLM-5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "75c/s",
            "CodeArena": "1,614",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.36,
            "value": 21.08,
            "unified": 703.89,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "213c/s",
            "CodeArena": "1,363",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 26.0,
            "value": 9.49,
            "unified": 263.88,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "105c/s",
            "CodeArena": "1,020",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 46.31,
            "value": 16.54,
            "unified": 492.22,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "260c/s",
            "CodeArena": "1,014",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 40.54,
            "value": 9.65,
            "unified": 421.33,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "137c/s",
            "CodeArena": "836",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 22.89,
            "value": 15.26,
            "unified": 237.37,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "211c/s",
            "CodeArena": "829",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.65,
            "value": 0.85,
            "unified": 0.65,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "102c/s",
            "CodeArena": "825",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 51.75,
            "value": 34.5,
            "unified": 573.19,
            "Model": "MiniMax M2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "808",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 51.68,
            "value": 20.92,
            "unified": 555.73,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "81c/s",
            "CodeArena": "791",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 32.66,
            "value": 81.66,
            "unified": 424.67,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "542c/s",
            "CodeArena": "761",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-17T15:35:57+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.85,
            "value": 2.86,
            "unified": 903.11,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "129c/s",
            "CodeArena": "2,029",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 77.79,
            "value": 5.56,
            "unified": 819.14,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "81c/s",
            "CodeArena": "1,550",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 66.92,
            "value": 2.23,
            "unified": 697.39,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "249c/s",
            "CodeArena": "1,539",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 72.21,
            "value": 4.58,
            "unified": 757.51,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "41c/s",
            "CodeArena": "1,475",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 69.79,
            "value": 19.94,
            "unified": 750.22,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "45c/s",
            "CodeArena": "1,393",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.84,
            "value": 5.14,
            "unified": 602.6,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "87c/s",
            "CodeArena": "1,195",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 41.07,
            "value": 2.28,
            "unified": 417.51,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "155c/s",
            "CodeArena": "1,086",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 48.43,
            "value": 4.3,
            "unified": 499.67,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "100c/s",
            "CodeArena": "1,070",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 59.91,
            "value": 5.32,
            "unified": 625.21,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "993",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 28.29,
            "value": 0.31,
            "unified": 276.66,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "163c/s",
            "CodeArena": "975",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 76.72,
            "value": 18.27,
            "unified": 823.22,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "78c/s",
            "CodeArena": "1,633",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.5,
            "value": 21.13,
            "unified": 705.23,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "214c/s",
            "CodeArena": "1,369",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 25.94,
            "value": 9.47,
            "unified": 262.52,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "107c/s",
            "CodeArena": "1,014",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 46.32,
            "value": 16.54,
            "unified": 491.83,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "262c/s",
            "CodeArena": "1,012",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 23.32,
            "value": 15.54,
            "unified": 241.55,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "212c/s",
            "CodeArena": "854",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Qwen3.5-397B-A17B",
            "organization": "Qwen",
            "link": "https://llm-stats.com/models/qwen3.5-397b-a17b",
            "origin": "CN",
            "description": "Qwen3.5-397B-A17B is Qwen's flagship Mixture-of-Experts model with 397 billion total parameters and 17 billion activated parameters. It delivers state-of-the-art performance across knowledge, reasonin",
            "created": "Feb. 2026",
            "avgIq": 40.59,
            "value": 9.67,
            "unified": 421.42,
            "Model": "Qwen3.5-397B-A17B\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$3.60",
            "Speed": "136c/s",
            "CodeArena": "837",
            "GPQA": "88.4%",
            "AIME2025": "-",
            "SWE-benchVerified": "76.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "88.5%",
            "MMMU": "-",
            "BrowseComp": "69.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "28.7%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "38.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "397",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.74,
            "value": 0.88,
            "unified": 0.65,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "103c/s",
            "CodeArena": "825",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 51.82,
            "value": 34.55,
            "unified": 573.62,
            "Model": "MiniMax M2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "808",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 51.71,
            "value": 20.94,
            "unified": 555.71,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "77c/s",
            "CodeArena": "791",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 32.66,
            "value": 81.66,
            "unified": 424.05,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "503c/s",
            "CodeArena": "759",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-17T04:05:39+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.56,
            "value": 2.85,
            "unified": 903.04,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "127c/s",
            "CodeArena": "2,019",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 78.0,
            "value": 5.57,
            "unified": 822.39,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "85c/s",
            "CodeArena": "1,555",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "45.8%",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 66.71,
            "value": 2.22,
            "unified": 692.92,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "251c/s",
            "CodeArena": "1,539",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 72.42,
            "value": 4.6,
            "unified": 759.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "41c/s",
            "CodeArena": "1,482",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 69.91,
            "value": 19.97,
            "unified": 749.71,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "37c/s",
            "CodeArena": "1,388",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "43.5%",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.86,
            "value": 5.14,
            "unified": 598.15,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "84c/s",
            "CodeArena": "1,160",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 40.29,
            "value": 2.24,
            "unified": 399.55,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "158c/s",
            "CodeArena": "1,086",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 48.8,
            "value": 4.34,
            "unified": 496.61,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "105c/s",
            "CodeArena": "1,064",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 61.43,
            "value": 5.46,
            "unified": 638.18,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "95c/s",
            "CodeArena": "993",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 27.22,
            "value": 0.3,
            "unified": 252.07,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "172c/s",
            "CodeArena": "971",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 73.83,
            "value": 17.58,
            "unified": 790.36,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "88c/s",
            "CodeArena": "1,520",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.6,
            "value": 21.16,
            "unified": 703.26,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "211c/s",
            "CodeArena": "1,348",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 27.22,
            "value": 9.93,
            "unified": 263.63,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "96c/s",
            "CodeArena": "1,014",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 47.38,
            "value": 16.92,
            "unified": 495.87,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "261c/s",
            "CodeArena": "1,010",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 24.02,
            "value": 16.02,
            "unified": 235.41,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "193c/s",
            "CodeArena": "854",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.52,
            "value": 1.46,
            "unified": 1.41,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "106c/s",
            "CodeArena": "825",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 53.27,
            "value": 21.57,
            "unified": 566.9,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "79c/s",
            "CodeArena": "807",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 51.66,
            "value": 34.44,
            "unified": 564.38,
            "Model": "MiniMax M2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "163c/s",
            "CodeArena": "780",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 33.56,
            "value": 83.89,
            "unified": 422.48,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "493c/s",
            "CodeArena": "756",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 36.03,
            "value": 51.47,
            "unified": 411.15,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "107c/s",
            "CodeArena": "716",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-16T15:29:33+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.6,
            "value": 2.85,
            "unified": 900.99,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "125c/s",
            "CodeArena": "1,983",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.25,
            "value": 5.66,
            "unified": 832.4,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "88c/s",
            "CodeArena": "1,554",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.66,
            "value": 2.26,
            "unified": 697.9,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "245c/s",
            "CodeArena": "1,544",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 75.86,
            "value": 4.82,
            "unified": 793.23,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "41c/s",
            "CodeArena": "1,484",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 70.47,
            "value": 20.13,
            "unified": 748.85,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "36c/s",
            "CodeArena": "1,389",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 58.6,
            "value": 5.21,
            "unified": 598.92,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "89c/s",
            "CodeArena": "1,182",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 48.4,
            "value": 4.3,
            "unified": 482.77,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "111c/s",
            "CodeArena": "1,079",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.35,
            "value": 2.19,
            "unified": 378.37,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,072",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 63.41,
            "value": 5.64,
            "unified": 653.62,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "1,020",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 54.97,
            "value": 4.89,
            "unified": 557.58,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "260c/s",
            "CodeArena": "931",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 74.67,
            "value": 17.78,
            "unified": 793.71,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "86c/s",
            "CodeArena": "1,430",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.65,
            "value": 21.5,
            "unified": 707.22,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "208c/s",
            "CodeArena": "1,320",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 51.89,
            "value": 18.53,
            "unified": 537.44,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "258c/s",
            "CodeArena": "1,023",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.97,
            "value": 12.4,
            "unified": 328.61,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "88c/s",
            "CodeArena": "1,013",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 27.52,
            "value": 18.35,
            "unified": 262.22,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "189c/s",
            "CodeArena": "850",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 5.85,
            "value": 1.89,
            "unified": -0.03,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "106c/s",
            "CodeArena": "823",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.1,
            "value": 23.12,
            "unified": 601.14,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "86c/s",
            "CodeArena": "790",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.97,
            "value": 94.91,
            "unified": 462.43,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "465c/s",
            "CodeArena": "747",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 38.74,
            "value": 55.34,
            "unified": 428.61,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "104c/s",
            "CodeArena": "702",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.43,
            "value": 3.71,
            "unified": 19.77,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "128c/s",
            "CodeArena": "678",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-16T04:09:27+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.53,
            "value": 2.85,
            "unified": 901.54,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "122c/s",
            "CodeArena": "1,979",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 66.39,
            "value": 2.21,
            "unified": 691.97,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "244c/s",
            "CodeArena": "1,550",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 78.82,
            "value": 5.63,
            "unified": 831.39,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "88c/s",
            "CodeArena": "1,549",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 75.85,
            "value": 4.82,
            "unified": 798.11,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "40c/s",
            "CodeArena": "1,489",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 69.84,
            "value": 19.96,
            "unified": 749.63,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "35c/s",
            "CodeArena": "1,381",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 55.74,
            "value": 4.95,
            "unified": 578.92,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "87c/s",
            "CodeArena": "1,173",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 36.04,
            "value": 2.0,
            "unified": 360.69,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,071",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 44.82,
            "value": 3.98,
            "unified": 458.62,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "111c/s",
            "CodeArena": "1,042",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 61.53,
            "value": 5.47,
            "unified": 642.6,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "106c/s",
            "CodeArena": "1,032",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 53.34,
            "value": 4.74,
            "unified": 552.48,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "259c/s",
            "CodeArena": "927",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 79.19,
            "value": 18.86,
            "unified": 850.39,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "85c/s",
            "CodeArena": "1,569",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.05,
            "value": 20.66,
            "unified": 687.23,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "206c/s",
            "CodeArena": "1,214",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 50.15,
            "value": 17.91,
            "unified": 532.45,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "257c/s",
            "CodeArena": "1,029",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 30.89,
            "value": 11.28,
            "unified": 314.96,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "88c/s",
            "CodeArena": "1,013",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 24.81,
            "value": 16.54,
            "unified": 254.49,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "186c/s",
            "CodeArena": "847",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 55.01,
            "value": 22.27,
            "unified": 590.42,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "88c/s",
            "CodeArena": "794",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.5",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.5",
            "origin": "CN",
            "description": "MiniMax M2.5 is the world's first production-level model designed natively for Agent scenarios. Building on the M2.1 foundation, M2.5 delivers significant improvements in programming, tool calling, se",
            "created": "Feb. 2026",
            "avgIq": 56.46,
            "value": 37.64,
            "unified": 623.5,
            "Model": "MiniMax M2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "160c/s",
            "CodeArena": "787",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "80.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "76.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 36.14,
            "value": 90.34,
            "unified": 461.16,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "460c/s",
            "CodeArena": "759",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 39.19,
            "value": 55.99,
            "unified": 455.83,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "98c/s",
            "CodeArena": "712",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 3.03,
            "value": 1.51,
            "unified": -0.02,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "128c/s",
            "CodeArena": "678",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-15T15:26:17+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.29,
            "value": 2.84,
            "unified": 903.05,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "119c/s",
            "CodeArena": "1,959",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.23,
            "value": 5.66,
            "unified": 842.07,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "88c/s",
            "CodeArena": "1,555",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.81,
            "value": 2.26,
            "unified": 717.96,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "239c/s",
            "CodeArena": "1,554",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 75.19,
            "value": 4.77,
            "unified": 798.48,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "39c/s",
            "CodeArena": "1,490",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 70.01,
            "value": 20.0,
            "unified": 760.06,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "35c/s",
            "CodeArena": "1,381",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 58.22,
            "value": 5.18,
            "unified": 619.86,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "83c/s",
            "CodeArena": "1,172",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.56,
            "value": 2.2,
            "unified": 419.82,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "155c/s",
            "CodeArena": "1,070",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 47.25,
            "value": 4.2,
            "unified": 503.1,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "111c/s",
            "CodeArena": "1,042",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 63.2,
            "value": 5.62,
            "unified": 672.87,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "107c/s",
            "CodeArena": "1,032",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 19.83,
            "value": 1.26,
            "unified": 210.58,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "203c/s",
            "CodeArena": "928",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 75.58,
            "value": 17.99,
            "unified": 816.64,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "79c/s",
            "CodeArena": "1,447",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.83,
            "value": 20.91,
            "unified": 706.34,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "201c/s",
            "CodeArena": "1,210",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 51.74,
            "value": 18.48,
            "unified": 565.61,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "257c/s",
            "CodeArena": "1,029",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.92,
            "value": 12.38,
            "unified": 371.07,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "80c/s",
            "CodeArena": "1,013",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 27.95,
            "value": 18.63,
            "unified": 314.71,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "186c/s",
            "CodeArena": "847",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.05,
            "value": 23.1,
            "unified": 626.61,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "90c/s",
            "CodeArena": "796",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.6,
            "value": 93.99,
            "unified": 496.71,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "452c/s",
            "CodeArena": "756",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 38.37,
            "value": 54.82,
            "unified": 463.22,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "95c/s",
            "CodeArena": "717",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.46,
            "value": 3.73,
            "unified": 82.72,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "130c/s",
            "CodeArena": "678",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "110c/s",
            "CodeArena": "673",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-15T04:08:03+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.29,
            "value": 2.84,
            "unified": 903.04,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "118c/s",
            "CodeArena": "1,998",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.14,
            "value": 2.24,
            "unified": 710.8,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "235c/s",
            "CodeArena": "1,547",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 78.63,
            "value": 5.62,
            "unified": 835.73,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "87c/s",
            "CodeArena": "1,543",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 74.78,
            "value": 4.75,
            "unified": 794.16,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "38c/s",
            "CodeArena": "1,480",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 69.75,
            "value": 19.93,
            "unified": 757.23,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "35c/s",
            "CodeArena": "1,382",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.52,
            "value": 5.02,
            "unified": 601.76,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "83c/s",
            "CodeArena": "1,124",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.3,
            "value": 2.18,
            "unified": 417.04,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "151c/s",
            "CodeArena": "1,065",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 47.13,
            "value": 4.19,
            "unified": 501.8,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "109c/s",
            "CodeArena": "1,044",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 63.09,
            "value": 5.61,
            "unified": 671.69,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "106c/s",
            "CodeArena": "1,034",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 20.27,
            "value": 1.29,
            "unified": 215.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "198c/s",
            "CodeArena": "936",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 74.76,
            "value": 17.8,
            "unified": 807.78,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "77c/s",
            "CodeArena": "1,441",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.72,
            "value": 20.88,
            "unified": 705.08,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "197c/s",
            "CodeArena": "1,215",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 51.74,
            "value": 18.48,
            "unified": 565.63,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "257c/s",
            "CodeArena": "1,035",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.83,
            "value": 12.35,
            "unified": 370.14,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "80c/s",
            "CodeArena": "1,013",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 27.96,
            "value": 18.64,
            "unified": 314.82,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "185c/s",
            "CodeArena": "847",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.16,
            "value": 23.14,
            "unified": 627.75,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "85c/s",
            "CodeArena": "800",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.64,
            "value": 94.11,
            "unified": 497.23,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "443c/s",
            "CodeArena": "755",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 38.5,
            "value": 55.0,
            "unified": 464.7,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "92c/s",
            "CodeArena": "717",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.59,
            "value": 3.8,
            "unified": 84.13,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "127c/s",
            "CodeArena": "678",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "115c/s",
            "CodeArena": "666",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-14T15:26:35+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.6,
            "value": 2.85,
            "unified": 902.81,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "115c/s",
            "CodeArena": "1,979",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.97,
            "value": 2.27,
            "unified": 716.04,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "240c/s",
            "CodeArena": "1,549",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.19,
            "value": 5.66,
            "unified": 838.03,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "82c/s",
            "CodeArena": "1,540",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 75.44,
            "value": 4.79,
            "unified": 797.6,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "34c/s",
            "CodeArena": "1,439",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 70.47,
            "value": 20.13,
            "unified": 761.14,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "33c/s",
            "CodeArena": "1,377",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.79,
            "value": 5.05,
            "unified": 600.78,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "81c/s",
            "CodeArena": "1,102",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 49.25,
            "value": 4.38,
            "unified": 520.49,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "114c/s",
            "CodeArena": "1,093",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.49,
            "value": 2.19,
            "unified": 415.06,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "145c/s",
            "CodeArena": "1,062",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.23,
            "value": 5.71,
            "unified": 680.11,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "131c/s",
            "CodeArena": "1,032",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 55.81,
            "value": 4.96,
            "unified": 590.41,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "271c/s",
            "CodeArena": "952",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.28,
            "value": 21.06,
            "unified": 707.24,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "194c/s",
            "CodeArena": "1,217",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 52.43,
            "value": 18.72,
            "unified": 569.05,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "260c/s",
            "CodeArena": "1,040",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.82,
            "value": 12.34,
            "unified": 365.77,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "77c/s",
            "CodeArena": "986",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 58.91,
            "value": 14.03,
            "unified": 632.56,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "56c/s",
            "CodeArena": "897",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 27.69,
            "value": 18.46,
            "unified": 307.51,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "182c/s",
            "CodeArena": "842",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.58,
            "value": 23.31,
            "unified": 628.24,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "88c/s",
            "CodeArena": "800",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.25,
            "value": 95.62,
            "unified": 499.82,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "343c/s",
            "CodeArena": "744",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 39.45,
            "value": 56.36,
            "unified": 471.39,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "78c/s",
            "CodeArena": "711",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.4,
            "value": 0.13,
            "unified": -0.02,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "119c/s",
            "CodeArena": "666",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.43,
            "value": 3.71,
            "unified": 78.0,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "123c/s",
            "CodeArena": "656",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-14T03:59:53+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.6,
            "value": 2.85,
            "unified": 902.8,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "136c/s",
            "CodeArena": "1,968",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 68.11,
            "value": 2.27,
            "unified": 717.48,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "238c/s",
            "CodeArena": "1,548",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.4,
            "value": 5.67,
            "unified": 840.3,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "81c/s",
            "CodeArena": "1,546",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 75.49,
            "value": 4.79,
            "unified": 798.05,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "38c/s",
            "CodeArena": "1,436",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 70.56,
            "value": 20.16,
            "unified": 762.05,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "33c/s",
            "CodeArena": "1,377",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.89,
            "value": 5.06,
            "unified": 601.89,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "82c/s",
            "CodeArena": "1,102",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 49.35,
            "value": 4.39,
            "unified": 521.58,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "115c/s",
            "CodeArena": "1,093",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.64,
            "value": 2.2,
            "unified": 416.71,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "151c/s",
            "CodeArena": "1,066",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.4,
            "value": 5.72,
            "unified": 681.94,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "140c/s",
            "CodeArena": "1,035",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 55.72,
            "value": 4.95,
            "unified": 589.37,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "275c/s",
            "CodeArena": "945",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.35,
            "value": 21.08,
            "unified": 707.98,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "254c/s",
            "CodeArena": "1,217",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 52.41,
            "value": 18.72,
            "unified": 568.87,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "170c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.97,
            "value": 12.4,
            "unified": 367.44,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "63c/s",
            "CodeArena": "993",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 58.97,
            "value": 14.04,
            "unified": 633.19,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "57c/s",
            "CodeArena": "897",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 27.72,
            "value": 18.48,
            "unified": 307.74,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "176c/s",
            "CodeArena": "842",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.29,
            "value": 23.2,
            "unified": 625.08,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "92c/s",
            "CodeArena": "781",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.32,
            "value": 95.81,
            "unified": 500.61,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "336c/s",
            "CodeArena": "748",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 39.46,
            "value": 56.37,
            "unified": 471.41,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "77c/s",
            "CodeArena": "711",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.4,
            "value": 0.13,
            "unified": 0.01,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "120c/s",
            "CodeArena": "666",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.43,
            "value": 3.71,
            "unified": 77.99,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "123c/s",
            "CodeArena": "656",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-13T15:30:02+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.29,
            "value": 2.84,
            "unified": 902.9,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "168c/s",
            "CodeArena": "1,949",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.97,
            "value": 2.27,
            "unified": 718.6,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "245c/s",
            "CodeArena": "1,549",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.28,
            "value": 5.66,
            "unified": 842.18,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "163c/s",
            "CodeArena": "1,546",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 74.71,
            "value": 4.74,
            "unified": 792.74,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "57c/s",
            "CodeArena": "1,441",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 70.14,
            "value": 20.04,
            "unified": 760.42,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "105c/s",
            "CodeArena": "1,376",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.56,
            "value": 5.03,
            "unified": 600.56,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "89c/s",
            "CodeArena": "1,102",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 49.22,
            "value": 4.37,
            "unified": 521.97,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "118c/s",
            "CodeArena": "1,093",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.8,
            "value": 2.21,
            "unified": 419.84,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "141c/s",
            "CodeArena": "1,066",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 63.5,
            "value": 5.64,
            "unified": 674.76,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "117c/s",
            "CodeArena": "1,027",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 22.12,
            "value": 1.4,
            "unified": 231.46,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "194c/s",
            "CodeArena": "942",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.65,
            "value": 20.86,
            "unified": 703.14,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "250c/s",
            "CodeArena": "1,185",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 52.1,
            "value": 18.61,
            "unified": 567.69,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "169c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.84,
            "value": 12.35,
            "unified": 367.32,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "71c/s",
            "CodeArena": "993",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.07,
            "value": 18.72,
            "unified": 312.97,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "160c/s",
            "CodeArena": "839",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.99,
            "value": 23.07,
            "unified": 624.25,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "110c/s",
            "CodeArena": "776",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.75,
            "value": 94.39,
            "unified": 495.9,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "344c/s",
            "CodeArena": "749",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 52.94,
            "value": 12.6,
            "unified": 570.13,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "52c/s",
            "CodeArena": "717",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 38.63,
            "value": 55.19,
            "unified": 463.66,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "81c/s",
            "CodeArena": "711",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.42,
            "value": 0.13,
            "unified": -0.02,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "115c/s",
            "CodeArena": "666",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.37,
            "value": 3.68,
            "unified": 77.42,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "133c/s",
            "CodeArena": "656",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-13T04:07:20+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.29,
            "value": 2.84,
            "unified": 902.96,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "166c/s",
            "CodeArena": "1,957",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 67.82,
            "value": 2.26,
            "unified": 717.44,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "245c/s",
            "CodeArena": "1,549",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.05,
            "value": 5.65,
            "unified": 839.91,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "160c/s",
            "CodeArena": "1,539",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 74.63,
            "value": 4.74,
            "unified": 792.14,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "57c/s",
            "CodeArena": "1,441",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 70.04,
            "value": 20.01,
            "unified": 759.79,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "108c/s",
            "CodeArena": "1,376",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 56.43,
            "value": 5.02,
            "unified": 599.84,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "86c/s",
            "CodeArena": "1,103",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 48.85,
            "value": 4.34,
            "unified": 518.88,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "121c/s",
            "CodeArena": "1,086",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 39.69,
            "value": 2.2,
            "unified": 419.62,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "139c/s",
            "CodeArena": "1,066",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 63.35,
            "value": 5.63,
            "unified": 673.66,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "115c/s",
            "CodeArena": "1,027",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 21.59,
            "value": 1.37,
            "unified": 227.2,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "196c/s",
            "CodeArena": "940",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.32,
            "value": 20.75,
            "unified": 700.04,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "247c/s",
            "CodeArena": "1,170",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 52.02,
            "value": 18.58,
            "unified": 567.54,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "169c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 33.69,
            "value": 12.29,
            "unified": 366.84,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "71c/s",
            "CodeArena": "989",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.0,
            "value": 18.67,
            "unified": 313.45,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "159c/s",
            "CodeArena": "839",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.92,
            "value": 23.04,
            "unified": 624.14,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "115c/s",
            "CodeArena": "776",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.68,
            "value": 94.21,
            "unified": 496.16,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "337c/s",
            "CodeArena": "749",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 38.43,
            "value": 54.91,
            "unified": 462.37,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "81c/s",
            "CodeArena": "707",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-5",
            "origin": "CN",
            "description": "GLM-5 is Zhipu AI's flagship foundation model designed for complex system engineering and long-range Agent tasks, shifting focus from coding to engineering. It features 744B total parameters (40B acti",
            "created": "Feb. 2026",
            "avgIq": 52.43,
            "value": 12.48,
            "unified": 565.37,
            "Model": "GLM-5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "200k",
            "Input$/M": "$1.00",
            "Output$/M": "$3.20",
            "Speed": "48c/s",
            "CodeArena": "705",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "77.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "75.9%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "67.8%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "744",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.25,
            "value": 0.08,
            "unified": 0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "115c/s",
            "CodeArena": "666",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.37,
            "value": 3.68,
            "unified": 79.13,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "140c/s",
            "CodeArena": "660",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-12T15:35:18+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 86.43,
            "value": 2.88,
            "unified": 902.95,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "178c/s",
            "CodeArena": "1,957",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 71.29,
            "value": 2.38,
            "unified": 744.87,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "245c/s",
            "CodeArena": "1,549",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.49,
            "value": 5.68,
            "unified": 833.65,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "159c/s",
            "CodeArena": "1,536",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 76.31,
            "value": 4.85,
            "unified": 799.68,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "150c/s",
            "CodeArena": "1,428",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 71.14,
            "value": 20.33,
            "unified": 761.93,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "185c/s",
            "CodeArena": "1,376",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 58.03,
            "value": 5.16,
            "unified": 609.6,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "83c/s",
            "CodeArena": "1,108",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 50.33,
            "value": 4.47,
            "unified": 528.78,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "116c/s",
            "CodeArena": "1,086",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 40.81,
            "value": 2.27,
            "unified": 427.35,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "146c/s",
            "CodeArena": "1,066",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.93,
            "value": 5.77,
            "unified": 682.11,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "119c/s",
            "CodeArena": "1,027",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 26.31,
            "value": 1.67,
            "unified": 275.69,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "205c/s",
            "CodeArena": "943",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.76,
            "value": 20.89,
            "unified": 696.11,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "271c/s",
            "CodeArena": "1,172",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 52.94,
            "value": 18.91,
            "unified": 570.92,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "201c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 34.79,
            "value": 12.7,
            "unified": 375.51,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "95c/s",
            "CodeArena": "991",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.74,
            "value": 19.16,
            "unified": 319.25,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "159c/s",
            "CodeArena": "835",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 58.32,
            "value": 23.61,
            "unified": 631.84,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "99c/s",
            "CodeArena": "783",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.46,
            "value": 96.14,
            "unified": 500.45,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "316c/s",
            "CodeArena": "748",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 40.24,
            "value": 57.48,
            "unified": 478.79,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "90c/s",
            "CodeArena": "718",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 3.34,
            "value": 1.08,
            "unified": 35.89,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "103c/s",
            "CodeArena": "666",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.03,
            "value": 4.51,
            "unified": 98.69,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "147c/s",
            "CodeArena": "660",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "132c/s",
            "CodeArena": "581",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-12T04:12:09+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 86.43,
            "value": 2.88,
            "unified": 902.94,
            "Model": "Claude Opus 4.6",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "176c/s",
            "CodeArena": "1,925",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 71.96,
            "value": 2.4,
            "unified": 751.86,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "236c/s",
            "CodeArena": "1,556",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 79.86,
            "value": 5.7,
            "unified": 837.47,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "154c/s",
            "CodeArena": "1,535",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 76.32,
            "value": 4.85,
            "unified": 799.7,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "150c/s",
            "CodeArena": "1,407",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 71.59,
            "value": 20.46,
            "unified": 766.71,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "290c/s",
            "CodeArena": "1,386",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 58.73,
            "value": 5.22,
            "unified": 616.99,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "85c/s",
            "CodeArena": "1,118",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 50.74,
            "value": 4.51,
            "unified": 533.04,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "110c/s",
            "CodeArena": "1,086",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "",
            "created": "Sep. 2025",
            "avgIq": 41.02,
            "value": 2.28,
            "unified": 429.53,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "144c/s",
            "CodeArena": "1,062",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.05,
            "value": 5.69,
            "unified": 672.83,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "119c/s",
            "CodeArena": "983",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 25.45,
            "value": 1.62,
            "unified": 266.64,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "206c/s",
            "CodeArena": "920",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 64.84,
            "value": 20.92,
            "unified": 696.9,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "275c/s",
            "CodeArena": "1,161",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 53.15,
            "value": 18.98,
            "unified": 573.18,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "193c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 34.99,
            "value": 12.77,
            "unified": 377.64,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "117c/s",
            "CodeArena": "991",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.86,
            "value": 19.24,
            "unified": 320.43,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "157c/s",
            "CodeArena": "833",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.58,
            "value": 96.44,
            "unified": 501.71,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "309c/s",
            "CodeArena": "748",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.47,
            "value": 23.27,
            "unified": 622.6,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "100c/s",
            "CodeArena": "723",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 40.4,
            "value": 57.71,
            "unified": 480.54,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "89c/s",
            "CodeArena": "718",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 3.45,
            "value": 1.11,
            "unified": 37.06,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "96c/s",
            "CodeArena": "663",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.1,
            "value": 4.55,
            "unified": 99.42,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "155c/s",
            "CodeArena": "658",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "136c/s",
            "CodeArena": "577",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-11T15:38:01+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.73,
            "value": 0.0,
            "unified": 900.03,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "178c/s",
            "CodeArena": "1,906",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 76.97,
            "value": 0.0,
            "unified": 808.01,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "231c/s",
            "CodeArena": "1,550",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 80.62,
            "value": 0.0,
            "unified": 846.39,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "154c/s",
            "CodeArena": "1,543",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 78.13,
            "value": 0.0,
            "unified": 820.22,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "159c/s",
            "CodeArena": "1,416",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 75.58,
            "value": 0.0,
            "unified": 793.4,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "297c/s",
            "CodeArena": "1,382",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 70.49,
            "value": 0.0,
            "unified": 739.96,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "88c/s",
            "CodeArena": "1,113",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 63.77,
            "value": 0.0,
            "unified": 669.49,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "116c/s",
            "CodeArena": "1,094",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.12,
            "value": 0.0,
            "unified": 526.19,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "143c/s",
            "CodeArena": "1,062",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 75.06,
            "value": 0.0,
            "unified": 788.0,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "116c/s",
            "CodeArena": "983",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 48.32,
            "value": 0.0,
            "unified": 507.28,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "201c/s",
            "CodeArena": "921",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.67,
            "value": 0.0,
            "unified": 720.89,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "271c/s",
            "CodeArena": "1,165",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 37.22,
            "value": 0.0,
            "unified": 390.69,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "144c/s",
            "CodeArena": "828",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 47.64,
            "value": 0.0,
            "unified": 500.17,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "92c/s",
            "CodeArena": "739",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 30.27,
            "value": 0.0,
            "unified": 317.81,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "133c/s",
            "CodeArena": "577",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 34.5,
            "value": 0.0,
            "unified": 362.2,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "59c/s",
            "CodeArena": "548",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 40.92,
            "value": 0.0,
            "unified": 429.58,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "104c/s",
            "CodeArena": "401",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2",
            "origin": "CN",
            "description": "MiniMax M2 is an open-source large language model by MiniMax, built for agents and coding tasks. It delivers state-of-the-art tool use, reasoning, and search performance while maintaining exceptional ",
            "created": "Oct. 2025",
            "avgIq": 20.92,
            "value": 0.0,
            "unified": 219.67,
            "Model": "MiniMax M2",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "367c/s",
            "CodeArena": "351",
            "GPQA": "78.0%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "69.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "44.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "12.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "46.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "ERNIE 4.5",
            "organization": "Baidu",
            "link": "https://llm-stats.com/models/ernie-4.5",
            "origin": "CN",
            "description": "ERNIE 4.5 is an enhanced version of ERNIE 4.0 with improved reasoning capabilities, better instruction following, and stronger performance across multiple domains including mathematics, coding, and cr",
            "created": "Jun. 2025",
            "avgIq": 15.12,
            "value": 0.0,
            "unified": 158.72,
            "Model": "ERNIE 4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "134c/s",
            "CodeArena": "83",
            "GPQA": "74.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "1.8%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "21",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-R1",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-r1",
            "origin": "CN",
            "description": "DeepSeek-R1 is a reasoning-focused language model from DeepSeek that features advanced thinking capabilities. It serves as the foundation for DeepSeek's reasoning model family and pioneered their thin",
            "created": "Jan. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-R1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "\u2013",
            "CodeArena": "\u2013",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "671",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek R1 Distill Llama 70B",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-r1-distill-llama-70b",
            "origin": "CN",
            "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chai",
            "created": "Jan. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek R1 Distill Llama 70B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "\u2013",
            "CodeArena": "\u2013",
            "GPQA": "65.2%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "70.6",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-11T04:13:02+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.73,
            "value": 0.0,
            "unified": 900.03,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "178c/s",
            "CodeArena": "1,906",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 76.97,
            "value": 0.0,
            "unified": 808.01,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "232c/s",
            "CodeArena": "1,550",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 80.62,
            "value": 0.0,
            "unified": 846.39,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "153c/s",
            "CodeArena": "1,543",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 78.13,
            "value": 0.0,
            "unified": 820.22,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "159c/s",
            "CodeArena": "1,416",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 75.58,
            "value": 0.0,
            "unified": 793.4,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "301c/s",
            "CodeArena": "1,382",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 70.49,
            "value": 0.0,
            "unified": 739.96,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "88c/s",
            "CodeArena": "1,113",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 63.77,
            "value": 0.0,
            "unified": 669.49,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "118c/s",
            "CodeArena": "1,094",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.12,
            "value": 0.0,
            "unified": 526.19,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "146c/s",
            "CodeArena": "1,062",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 75.06,
            "value": 0.0,
            "unified": 788.0,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "116c/s",
            "CodeArena": "983",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 48.32,
            "value": 0.0,
            "unified": 507.28,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "202c/s",
            "CodeArena": "921",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.67,
            "value": 0.0,
            "unified": 720.89,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "259c/s",
            "CodeArena": "1,165",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 37.22,
            "value": 0.0,
            "unified": 390.69,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "153c/s",
            "CodeArena": "828",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 47.64,
            "value": 0.0,
            "unified": 500.17,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "92c/s",
            "CodeArena": "739",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 30.27,
            "value": 0.0,
            "unified": 317.81,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "125c/s",
            "CodeArena": "577",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 34.5,
            "value": 0.0,
            "unified": 362.2,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "58c/s",
            "CodeArena": "548",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 40.92,
            "value": 0.0,
            "unified": 429.58,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "104c/s",
            "CodeArena": "401",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiniMax M2",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2",
            "origin": "CN",
            "description": "MiniMax M2 is an open-source large language model by MiniMax, built for agents and coding tasks. It delivers state-of-the-art tool use, reasoning, and search performance while maintaining exceptional ",
            "created": "Oct. 2025",
            "avgIq": 20.92,
            "value": 0.0,
            "unified": 219.67,
            "Model": "MiniMax M2",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "354c/s",
            "CodeArena": "351",
            "GPQA": "78.0%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "69.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "44.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "12.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "46.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "ERNIE 4.5",
            "organization": "Baidu",
            "link": "https://llm-stats.com/models/ernie-4.5",
            "origin": "CN",
            "description": "ERNIE 4.5 is an enhanced version of ERNIE 4.0 with improved reasoning capabilities, better instruction following, and stronger performance across multiple domains including mathematics, coding, and cr",
            "created": "Jun. 2025",
            "avgIq": 15.12,
            "value": 0.0,
            "unified": 158.72,
            "Model": "ERNIE 4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "132c/s",
            "CodeArena": "83",
            "GPQA": "74.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "1.8%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "21",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-R1",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-r1",
            "origin": "CN",
            "description": "DeepSeek-R1 is a reasoning-focused language model from DeepSeek that features advanced thinking capabilities. It serves as the foundation for DeepSeek's reasoning model family and pioneered their thin",
            "created": "Jan. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-R1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "\u2013",
            "CodeArena": "\u2013",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "671",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek R1 Distill Llama 70B",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-r1-distill-llama-70b",
            "origin": "CN",
            "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chai",
            "created": "Jan. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek R1 Distill Llama 70B",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "-",
            "Input$/M": "-",
            "Output$/M": "-",
            "Speed": "\u2013",
            "CodeArena": "\u2013",
            "GPQA": "65.2%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "70.6",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-10T15:42:47+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 86.43,
            "value": 2.88,
            "unified": 902.93,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "181c/s",
            "CodeArena": "1,901",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 72.04,
            "value": 2.4,
            "unified": 752.64,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "223c/s",
            "CodeArena": "1,542",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 80.17,
            "value": 5.73,
            "unified": 840.76,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "155c/s",
            "CodeArena": "1,537",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 76.72,
            "value": 4.87,
            "unified": 803.88,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "158c/s",
            "CodeArena": "1,423",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 71.77,
            "value": 20.5,
            "unified": 768.49,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "289c/s",
            "CodeArena": "1,383",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 58.81,
            "value": 5.23,
            "unified": 617.77,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "82c/s",
            "CodeArena": "1,111",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 51.24,
            "value": 4.55,
            "unified": 538.23,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "110c/s",
            "CodeArena": "1,094",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 41.25,
            "value": 2.29,
            "unified": 431.93,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "138c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.26,
            "value": 5.71,
            "unified": 675.01,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "110c/s",
            "CodeArena": "983",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 25.83,
            "value": 1.64,
            "unified": 270.67,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "204c/s",
            "CodeArena": "919",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 65.07,
            "value": 20.99,
            "unified": 699.3,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "255c/s",
            "CodeArena": "1,165",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 53.24,
            "value": 19.01,
            "unified": 574.03,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "184c/s",
            "CodeArena": "1,033",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 34.93,
            "value": 12.75,
            "unified": 376.94,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "104c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.85,
            "value": 19.23,
            "unified": 320.28,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "153c/s",
            "CodeArena": "828",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.73,
            "value": 96.82,
            "unified": 503.27,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "306c/s",
            "CodeArena": "754",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 40.97,
            "value": 58.53,
            "unified": 487.11,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "103c/s",
            "CodeArena": "739",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.94,
            "value": 23.05,
            "unified": 616.68,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "107c/s",
            "CodeArena": "688",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 3.31,
            "value": 1.07,
            "unified": 35.54,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "92c/s",
            "CodeArena": "658",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.08,
            "value": 4.54,
            "unified": 99.29,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "141c/s",
            "CodeArena": "656",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "120c/s",
            "CodeArena": "577",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-10T04:14:01+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 86.43,
            "value": 2.88,
            "unified": 902.88,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "182c/s",
            "CodeArena": "1,641",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 84.43,
            "value": 6.03,
            "unified": 885.34,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "154c/s",
            "CodeArena": "1,555",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 76.97,
            "value": 2.57,
            "unified": 804.06,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "223c/s",
            "CodeArena": "1,531",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 79.41,
            "value": 5.04,
            "unified": 832.07,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "163c/s",
            "CodeArena": "1,424",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 74.85,
            "value": 21.39,
            "unified": 801.17,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "281c/s",
            "CodeArena": "1,388",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 62.22,
            "value": 5.53,
            "unified": 653.5,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "86c/s",
            "CodeArena": "1,102",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 55.14,
            "value": 4.9,
            "unified": 579.16,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "107c/s",
            "CodeArena": "1,102",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 43.75,
            "value": 2.43,
            "unified": 458.03,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "136c/s",
            "CodeArena": "1,063",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 67.07,
            "value": 5.96,
            "unified": 704.47,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "116c/s",
            "CodeArena": "983",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.95,
            "value": 2.03,
            "unified": 334.77,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "198c/s",
            "CodeArena": "919",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.54,
            "value": 21.79,
            "unified": 725.38,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "227c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.05,
            "value": 19.66,
            "unified": 593.25,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "186c/s",
            "CodeArena": "1,033",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 36.53,
            "value": 13.33,
            "unified": 393.93,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "149c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.78,
            "value": 19.85,
            "unified": 330.25,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "166c/s",
            "CodeArena": "828",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.39,
            "value": 98.48,
            "unified": 510.21,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "309c/s",
            "CodeArena": "754",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 41.8,
            "value": 59.71,
            "unified": 495.9,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "100c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.33,
            "value": 23.21,
            "unified": 620.55,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "107c/s",
            "CodeArena": "688",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.28,
            "value": 1.38,
            "unified": 45.96,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "87c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.29,
            "value": 4.65,
            "unified": 101.5,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "151c/s",
            "CodeArena": "652",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "118c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-09T15:36:42+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.22,
            "value": 6.16,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "147c/s",
            "CodeArena": "1,551",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.29,
            "value": 2.64,
            "unified": 830.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "204c/s",
            "CodeArena": "1,527",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 84.53,
            "value": 2.82,
            "unified": 885.21,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "184c/s",
            "CodeArena": "1,448",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 80.86,
            "value": 5.13,
            "unified": 849.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "165c/s",
            "CodeArena": "1,434",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.07,
            "value": 21.73,
            "unified": 815.97,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "278c/s",
            "CodeArena": "1,378",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 66.11,
            "value": 5.88,
            "unified": 696.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "87c/s",
            "CodeArena": "1,156",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 44.98,
            "value": 2.5,
            "unified": 472.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "136c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.96,
            "value": 4.8,
            "unified": 568.12,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "106c/s",
            "CodeArena": "1,028",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 68.9,
            "value": 6.12,
            "unified": 725.39,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "104c/s",
            "CodeArena": "995",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 34.4,
            "value": 2.18,
            "unified": 361.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "187c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.64,
            "value": 22.14,
            "unified": 738.82,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "228c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.65,
            "value": 19.88,
            "unified": 600.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "189c/s",
            "CodeArena": "1,021",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 37.3,
            "value": 13.61,
            "unified": 403.06,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "164c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.26,
            "value": 19.51,
            "unified": 325.09,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "167c/s",
            "CodeArena": "783",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.68,
            "value": 99.21,
            "unified": 514.22,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "307c/s",
            "CodeArena": "752",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.24,
            "value": 60.35,
            "unified": 501.77,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "96c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.08,
            "value": 23.11,
            "unified": 619.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "99c/s",
            "CodeArena": "668",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.68,
            "value": 1.51,
            "unified": 50.33,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.72,
            "value": 4.86,
            "unified": 106.36,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "170c/s",
            "CodeArena": "662",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "118c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-09T04:12:24+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.22,
            "value": 6.16,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "147c/s",
            "CodeArena": "1,551",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.29,
            "value": 2.64,
            "unified": 830.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "202c/s",
            "CodeArena": "1,527",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 84.53,
            "value": 2.82,
            "unified": 885.21,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "186c/s",
            "CodeArena": "1,448",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 80.86,
            "value": 5.13,
            "unified": 849.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "162c/s",
            "CodeArena": "1,434",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.07,
            "value": 21.73,
            "unified": 815.97,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "273c/s",
            "CodeArena": "1,378",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 66.11,
            "value": 5.88,
            "unified": 696.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,156",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "",
            "created": "Sep. 2025",
            "avgIq": 44.98,
            "value": 2.5,
            "unified": 472.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "136c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.96,
            "value": 4.8,
            "unified": 568.12,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "1,028",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 68.9,
            "value": 6.12,
            "unified": 725.39,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "104c/s",
            "CodeArena": "995",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 34.4,
            "value": 2.18,
            "unified": 361.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "188c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.64,
            "value": 22.14,
            "unified": 738.82,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "216c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.65,
            "value": 19.88,
            "unified": 600.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "189c/s",
            "CodeArena": "1,021",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 37.3,
            "value": 13.61,
            "unified": 403.06,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "164c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.26,
            "value": 19.51,
            "unified": 325.09,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "169c/s",
            "CodeArena": "783",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.68,
            "value": 99.21,
            "unified": 514.22,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "311c/s",
            "CodeArena": "752",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.24,
            "value": 60.35,
            "unified": 501.77,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "95c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.08,
            "value": 23.11,
            "unified": 619.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "97c/s",
            "CodeArena": "668",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.68,
            "value": 1.51,
            "unified": 50.33,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.72,
            "value": 4.86,
            "unified": 106.36,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "172c/s",
            "CodeArena": "662",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "109c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-08T15:26:38+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.22,
            "value": 6.16,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "150c/s",
            "CodeArena": "1,551",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.29,
            "value": 2.64,
            "unified": 830.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "203c/s",
            "CodeArena": "1,527",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 84.53,
            "value": 2.82,
            "unified": 885.21,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "186c/s",
            "CodeArena": "1,448",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 80.86,
            "value": 5.13,
            "unified": 849.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "164c/s",
            "CodeArena": "1,434",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.07,
            "value": 21.73,
            "unified": 815.97,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "276c/s",
            "CodeArena": "1,378",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 66.11,
            "value": 5.88,
            "unified": 696.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,156",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 44.98,
            "value": 2.5,
            "unified": 472.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "135c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.96,
            "value": 4.8,
            "unified": 568.12,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "1,028",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 68.9,
            "value": 6.12,
            "unified": 725.39,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "104c/s",
            "CodeArena": "995",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 34.4,
            "value": 2.18,
            "unified": 361.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "188c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.64,
            "value": 22.14,
            "unified": 738.82,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "213c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.65,
            "value": 19.88,
            "unified": 600.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "191c/s",
            "CodeArena": "1,021",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 37.3,
            "value": 13.61,
            "unified": 403.06,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "164c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.26,
            "value": 19.51,
            "unified": 325.09,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "783",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.68,
            "value": 99.21,
            "unified": 514.22,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "311c/s",
            "CodeArena": "752",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.24,
            "value": 60.35,
            "unified": 501.77,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "95c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.08,
            "value": 23.11,
            "unified": 619.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "99c/s",
            "CodeArena": "668",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.68,
            "value": 1.51,
            "unified": 50.33,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.72,
            "value": 4.86,
            "unified": 106.36,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "172c/s",
            "CodeArena": "662",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "112c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-08T04:14:33+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.22,
            "value": 6.16,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "155c/s",
            "CodeArena": "1,551",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.29,
            "value": 2.64,
            "unified": 830.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "204c/s",
            "CodeArena": "1,527",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 84.53,
            "value": 2.82,
            "unified": 885.21,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "187c/s",
            "CodeArena": "1,448",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 80.86,
            "value": 5.13,
            "unified": 849.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "165c/s",
            "CodeArena": "1,434",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.07,
            "value": 21.73,
            "unified": 815.97,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "270c/s",
            "CodeArena": "1,378",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 66.11,
            "value": 5.88,
            "unified": 696.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,156",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 44.98,
            "value": 2.5,
            "unified": 472.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "135c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.96,
            "value": 4.8,
            "unified": 568.12,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "1,028",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 68.9,
            "value": 6.12,
            "unified": 725.39,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "104c/s",
            "CodeArena": "995",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 34.4,
            "value": 2.18,
            "unified": 361.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "188c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.64,
            "value": 22.14,
            "unified": 738.82,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "210c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.65,
            "value": 19.88,
            "unified": 600.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "191c/s",
            "CodeArena": "1,021",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 37.3,
            "value": 13.61,
            "unified": 403.06,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "164c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.26,
            "value": 19.51,
            "unified": 325.09,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "171c/s",
            "CodeArena": "783",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.68,
            "value": 99.21,
            "unified": 514.22,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "306c/s",
            "CodeArena": "752",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.24,
            "value": 60.35,
            "unified": 501.77,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "96c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.08,
            "value": 23.11,
            "unified": 619.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "99c/s",
            "CodeArena": "668",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.68,
            "value": 1.51,
            "unified": 50.33,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.72,
            "value": 4.86,
            "unified": 106.36,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "178c/s",
            "CodeArena": "662",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "112c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-07T15:26:20+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.22,
            "value": 6.16,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "158c/s",
            "CodeArena": "1,551",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.29,
            "value": 2.64,
            "unified": 830.31,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "203c/s",
            "CodeArena": "1,527",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 84.53,
            "value": 2.82,
            "unified": 885.21,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "188c/s",
            "CodeArena": "1,448",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 80.86,
            "value": 5.13,
            "unified": 849.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "164c/s",
            "CodeArena": "1,434",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.07,
            "value": 21.73,
            "unified": 815.97,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "270c/s",
            "CodeArena": "1,378",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 66.11,
            "value": 5.88,
            "unified": 696.02,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,156",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 44.98,
            "value": 2.5,
            "unified": 472.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "136c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.96,
            "value": 4.8,
            "unified": 568.12,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "1,028",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 68.9,
            "value": 6.12,
            "unified": 725.39,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "104c/s",
            "CodeArena": "995",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 34.4,
            "value": 2.18,
            "unified": 361.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "188c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.64,
            "value": 22.14,
            "unified": 738.82,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "211c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.65,
            "value": 19.88,
            "unified": 600.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "170c/s",
            "CodeArena": "1,021",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 37.3,
            "value": 13.61,
            "unified": 403.06,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "164c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.26,
            "value": 19.51,
            "unified": 325.09,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "157c/s",
            "CodeArena": "783",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.68,
            "value": 99.21,
            "unified": 514.22,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "338c/s",
            "CodeArena": "752",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.24,
            "value": 60.35,
            "unified": 501.77,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "92c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.08,
            "value": 23.11,
            "unified": 619.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "99c/s",
            "CodeArena": "668",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.68,
            "value": 1.51,
            "unified": 50.33,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.72,
            "value": 4.86,
            "unified": 106.36,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "178c/s",
            "CodeArena": "662",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "112c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-07T04:00:02+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.22,
            "value": 6.16,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "158c/s",
            "CodeArena": "1,558",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 78.96,
            "value": 2.63,
            "unified": 826.9,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "199c/s",
            "CodeArena": "1,523",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 84.51,
            "value": 2.82,
            "unified": 884.97,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "189c/s",
            "CodeArena": "1,453",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 80.77,
            "value": 5.13,
            "unified": 848.31,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "159c/s",
            "CodeArena": "1,435",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 75.95,
            "value": 21.7,
            "unified": 814.71,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "260c/s",
            "CodeArena": "1,378",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 65.95,
            "value": 5.86,
            "unified": 694.31,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "66c/s",
            "CodeArena": "1,156",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 44.88,
            "value": 2.49,
            "unified": 471.02,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "134c/s",
            "CodeArena": "1,064",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.84,
            "value": 4.79,
            "unified": 566.79,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "104c/s",
            "CodeArena": "1,028",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 68.78,
            "value": 6.11,
            "unified": 724.16,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "101c/s",
            "CodeArena": "995",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 34.15,
            "value": 2.17,
            "unified": 358.67,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "188c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 68.55,
            "value": 22.11,
            "unified": 737.83,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "207c/s",
            "CodeArena": "1,174",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.74,
            "value": 19.91,
            "unified": 601.9,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "171c/s",
            "CodeArena": "1,028",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 37.23,
            "value": 13.59,
            "unified": 402.37,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "172c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 29.23,
            "value": 19.49,
            "unified": 324.75,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "157c/s",
            "CodeArena": "783",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.65,
            "value": 99.14,
            "unified": 513.93,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "347c/s",
            "CodeArena": "752",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.2,
            "value": 60.29,
            "unified": 501.37,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "93c/s",
            "CodeArena": "737",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.07,
            "value": 23.11,
            "unified": 619.03,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "108c/s",
            "CodeArena": "668",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.64,
            "value": 1.5,
            "unified": 49.97,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "664",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.7,
            "value": 4.85,
            "unified": 106.19,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "185c/s",
            "CodeArena": "662",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "99c/s",
            "CodeArena": "580",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-06T15:29:02+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.06,
            "value": 6.15,
            "unified": 906.08,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "160c/s",
            "CodeArena": "1,541",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.49,
            "value": 2.65,
            "unified": 832.82,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "191c/s",
            "CodeArena": "1,515",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.27,
            "value": 2.84,
            "unified": 893.99,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "182c/s",
            "CodeArena": "1,474",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 81.16,
            "value": 5.15,
            "unified": 853.18,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "150c/s",
            "CodeArena": "1,421",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 75.94,
            "value": 21.7,
            "unified": 816.57,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "257c/s",
            "CodeArena": "1,366",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 43.55,
            "value": 2.42,
            "unified": 452.91,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "140c/s",
            "CodeArena": "1,028",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 52.8,
            "value": 4.69,
            "unified": 553.14,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "97c/s",
            "CodeArena": "1,012",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.85,
            "value": 5.14,
            "unified": 607.01,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "59c/s",
            "CodeArena": "963",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 58.43,
            "value": 5.19,
            "unified": 613.19,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "297c/s",
            "CodeArena": "924",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 32.58,
            "value": 2.07,
            "unified": 336.6,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "186c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.99,
            "value": 21.93,
            "unified": 732.88,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "199c/s",
            "CodeArena": "1,148",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.41,
            "value": 19.79,
            "unified": 597.56,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "179c/s",
            "CodeArena": "1,022",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 36.81,
            "value": 13.43,
            "unified": 394.01,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "277c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.67,
            "value": 19.11,
            "unified": 314.4,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "159c/s",
            "CodeArena": "785",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 41.69,
            "value": 59.55,
            "unified": 497.18,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "90c/s",
            "CodeArena": "740",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.41,
            "value": 22.84,
            "unified": 611.52,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "85c/s",
            "CodeArena": "673",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 8.52,
            "value": 4.26,
            "unified": 84.89,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "257c/s",
            "CodeArena": "650",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.21,
            "value": 0.71,
            "unified": 14.25,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "649",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.86,
            "value": 1.23,
            "unified": 0.58,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "96c/s",
            "CodeArena": "619",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 36.0,
            "value": 90.0,
            "unified": 471.18,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "333c/s",
            "CodeArena": "611",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-06T04:03:18+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 86.06,
            "value": 6.15,
            "unified": 906.08,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "163c/s",
            "CodeArena": "1,541",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 79.49,
            "value": 2.65,
            "unified": 832.82,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "192c/s",
            "CodeArena": "1,515",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.6",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-6",
            "origin": "US",
            "description": "Claude Opus 4.6 is Anthropic's most intelligent model, improving on its predecessor's coding skills with more careful planning, longer agentic task sustenance, more reliable operation in larger codeba",
            "created": "Feb. 2026",
            "avgIq": 85.27,
            "value": 2.84,
            "unified": 893.99,
            "Model": "Claude Opus 4.6\nNEW",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "193c/s",
            "CodeArena": "1,474",
            "GPQA": "91.3%",
            "AIME2025": "99.8%",
            "SWE-benchVerified": "80.8%",
            "ARC-AGIv2": "68.8%",
            "MMMLU": "91.1%",
            "MMMU": "-",
            "BrowseComp": "84.0%",
            "CharXiv-R": "77.4%",
            "MMMU-Pro": "77.3%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "59.5%",
            "HLE": "53.1%",
            "SimpleQA": "-",
            "OSWorld": "72.7%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 81.16,
            "value": 5.15,
            "unified": 853.18,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "144c/s",
            "CodeArena": "1,421",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 75.94,
            "value": 21.7,
            "unified": 816.57,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "258c/s",
            "CodeArena": "1,366",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 43.55,
            "value": 2.42,
            "unified": 452.91,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "142c/s",
            "CodeArena": "1,028",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 52.8,
            "value": 4.69,
            "unified": 553.14,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "1,012",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 57.85,
            "value": 5.14,
            "unified": 607.01,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "60c/s",
            "CodeArena": "963",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.03,
            "value": 5.25,
            "unified": 619.59,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "304c/s",
            "CodeArena": "943",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 32.58,
            "value": 2.07,
            "unified": 336.6,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "186c/s",
            "CodeArena": "914",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.99,
            "value": 21.93,
            "unified": 732.88,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "200c/s",
            "CodeArena": "1,148",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.41,
            "value": 19.79,
            "unified": 597.56,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "183c/s",
            "CodeArena": "1,022",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 36.81,
            "value": 13.43,
            "unified": 394.01,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "281c/s",
            "CodeArena": "980",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 28.67,
            "value": 19.11,
            "unified": 314.4,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "161c/s",
            "CodeArena": "785",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 41.69,
            "value": 59.55,
            "unified": 497.18,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "88c/s",
            "CodeArena": "740",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.36,
            "value": 22.82,
            "unified": 611.01,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "82c/s",
            "CodeArena": "671",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 8.55,
            "value": 4.27,
            "unified": 85.2,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "265c/s",
            "CodeArena": "651",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.21,
            "value": 0.71,
            "unified": 14.25,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "88c/s",
            "CodeArena": "649",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.86,
            "value": 1.23,
            "unified": 0.58,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "96c/s",
            "CodeArena": "619",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 36.0,
            "value": 90.0,
            "unified": 471.18,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "348c/s",
            "CodeArena": "611",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-05T04:02:21+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 88.68,
            "value": 6.33,
            "unified": 906.43,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "164c/s",
            "CodeArena": "1,537",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 85.35,
            "value": 2.84,
            "unified": 868.29,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "189c/s",
            "CodeArena": "1,511",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 85.05,
            "value": 5.4,
            "unified": 867.95,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "116c/s",
            "CodeArena": "1,449",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.51,
            "value": 22.72,
            "unified": 829.06,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "247c/s",
            "CodeArena": "1,376",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 49.33,
            "value": 2.74,
            "unified": 495.83,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "140c/s",
            "CodeArena": "1,008",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 60.06,
            "value": 5.34,
            "unified": 609.5,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "57c/s",
            "CodeArena": "969",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 30.71,
            "value": 1.95,
            "unified": 302.53,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "163c/s",
            "CodeArena": "894",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 51.69,
            "value": 4.59,
            "unified": 522.2,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "114c/s",
            "CodeArena": "891",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 23.64,
            "value": 0.26,
            "unified": 227.68,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "126c/s",
            "CodeArena": "869",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 57.7,
            "value": 5.13,
            "unified": 584.96,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "282c/s",
            "CodeArena": "849",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 69.53,
            "value": 22.43,
            "unified": 725.56,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "190c/s",
            "CodeArena": "1,129",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.46,
            "value": 20.16,
            "unified": 588.07,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "176c/s",
            "CodeArena": "1,002",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 38.77,
            "value": 14.15,
            "unified": 398.81,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "301c/s",
            "CodeArena": "963",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 32.92,
            "value": 21.95,
            "unified": 346.62,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "165c/s",
            "CodeArena": "749",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 43.33,
            "value": 61.89,
            "unified": 496.64,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "86c/s",
            "CodeArena": "742",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.74,
            "value": 23.38,
            "unified": 604.68,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "139c/s",
            "CodeArena": "669",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.52,
            "value": 4.76,
            "unified": 86.48,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "302c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 1.92,
            "value": 0.62,
            "unified": 3.51,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "88c/s",
            "CodeArena": "642",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 1.62,
            "value": 2.31,
            "unified": 2.14,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "96c/s",
            "CodeArena": "624",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.74,
            "value": 94.36,
            "unified": 473.42,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "433c/s",
            "CodeArena": "609",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-04T15:30:23+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 88.68,
            "value": 6.33,
            "unified": 906.43,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "164c/s",
            "CodeArena": "1,541",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 85.25,
            "value": 2.84,
            "unified": 867.4,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "188c/s",
            "CodeArena": "1,512",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 85.04,
            "value": 5.4,
            "unified": 867.9,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "115c/s",
            "CodeArena": "1,452",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.33,
            "value": 22.66,
            "unified": 827.29,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "237c/s",
            "CodeArena": "1,371",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 49.28,
            "value": 2.74,
            "unified": 496.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "138c/s",
            "CodeArena": "1,008",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 59.99,
            "value": 5.33,
            "unified": 609.41,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "55c/s",
            "CodeArena": "969",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 52.34,
            "value": 4.65,
            "unified": 529.73,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "112c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 30.9,
            "value": 1.96,
            "unified": 305.67,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "167c/s",
            "CodeArena": "897",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 23.62,
            "value": 0.26,
            "unified": 228.71,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "126c/s",
            "CodeArena": "869",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 57.67,
            "value": 5.13,
            "unified": 585.24,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "219c/s",
            "CodeArena": "849",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 69.54,
            "value": 22.43,
            "unified": 726.14,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "197c/s",
            "CodeArena": "1,132",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.42,
            "value": 20.15,
            "unified": 588.29,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "175c/s",
            "CodeArena": "1,002",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 38.73,
            "value": 14.14,
            "unified": 399.43,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "315c/s",
            "CodeArena": "963",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 32.88,
            "value": 21.92,
            "unified": 347.35,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "172c/s",
            "CodeArena": "748",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.93,
            "value": 61.32,
            "unified": 492.81,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "85c/s",
            "CodeArena": "731",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 57.73,
            "value": 23.37,
            "unified": 605.24,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "125c/s",
            "CodeArena": "669",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.52,
            "value": 4.76,
            "unified": 88.0,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "296c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 1.45,
            "value": 0.47,
            "unified": 0.22,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "85c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 1.61,
            "value": 2.3,
            "unified": 3.81,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "92c/s",
            "CodeArena": "624",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.74,
            "value": 94.36,
            "unified": 474.45,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "385c/s",
            "CodeArena": "609",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-04T04:02:11+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 88.68,
            "value": 6.33,
            "unified": 906.58,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "163c/s",
            "CodeArena": "1,540",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.03,
            "value": 2.87,
            "unified": 876.07,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "180c/s",
            "CodeArena": "1,535",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.72,
            "value": 5.38,
            "unified": 865.42,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "57c/s",
            "CodeArena": "1,431",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.45,
            "value": 22.7,
            "unified": 830.01,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "245c/s",
            "CodeArena": "1,371",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 49.58,
            "value": 2.75,
            "unified": 506.08,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "136c/s",
            "CodeArena": "1,002",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 60.86,
            "value": 5.41,
            "unified": 623.28,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "58c/s",
            "CodeArena": "973",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.12,
            "value": 4.72,
            "unified": 543.98,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "112c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.1,
            "value": 0.27,
            "unified": 244.89,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "119c/s",
            "CodeArena": "869",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 29.72,
            "value": 1.89,
            "unified": 303.57,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "141c/s",
            "CodeArena": "866",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 58.32,
            "value": 5.18,
            "unified": 597.26,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "210c/s",
            "CodeArena": "849",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 70.07,
            "value": 22.6,
            "unified": 734.67,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "231c/s",
            "CodeArena": "1,142",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.64,
            "value": 20.23,
            "unified": 595.92,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "172c/s",
            "CodeArena": "995",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.14,
            "value": 14.29,
            "unified": 412.12,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "318c/s",
            "CodeArena": "963",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 33.56,
            "value": 22.38,
            "unified": 363.96,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "179c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "230",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 43.75,
            "value": 62.5,
            "unified": 509.12,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "82c/s",
            "CodeArena": "731",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 58.33,
            "value": 23.62,
            "unified": 616.62,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "123c/s",
            "CodeArena": "669",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.21,
            "value": 5.1,
            "unified": 108.94,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "286c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.99,
            "value": 0.96,
            "unified": 31.32,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "83c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.38,
            "value": 95.96,
            "unified": 489.56,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "354c/s",
            "CodeArena": "609",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "93c/s",
            "CodeArena": "581",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-03T15:35:22+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 88.68,
            "value": 6.33,
            "unified": 906.64,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "166c/s",
            "CodeArena": "1,540",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.03,
            "value": 2.87,
            "unified": 876.08,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "189c/s",
            "CodeArena": "1,535",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.7,
            "value": 5.38,
            "unified": 865.26,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "44c/s",
            "CodeArena": "1,431",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.41,
            "value": 22.69,
            "unified": 829.82,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "257c/s",
            "CodeArena": "1,371",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 49.42,
            "value": 2.75,
            "unified": 504.41,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "139c/s",
            "CodeArena": "1,002",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 60.05,
            "value": 5.34,
            "unified": 615.06,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "59c/s",
            "CodeArena": "959",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 52.85,
            "value": 4.7,
            "unified": 541.29,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "118c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 28.98,
            "value": 1.84,
            "unified": 296.03,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "150c/s",
            "CodeArena": "866",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 58.19,
            "value": 5.17,
            "unified": 595.97,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "213c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 23.36,
            "value": 0.26,
            "unified": 237.35,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "118c/s",
            "CodeArena": "845",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 69.86,
            "value": 22.53,
            "unified": 732.66,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "235c/s",
            "CodeArena": "1,137",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.56,
            "value": 20.2,
            "unified": 595.21,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "169c/s",
            "CodeArena": "997",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.26,
            "value": 14.33,
            "unified": 413.47,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "321c/s",
            "CodeArena": "974",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 33.38,
            "value": 22.26,
            "unified": 362.2,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "192c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 43.46,
            "value": 62.09,
            "unified": 506.37,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "82c/s",
            "CodeArena": "731",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 58.12,
            "value": 23.53,
            "unified": 614.61,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "134c/s",
            "CodeArena": "669",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.97,
            "value": 4.98,
            "unified": 106.4,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "294c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.45,
            "value": 0.79,
            "unified": 25.69,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "76c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.04,
            "value": 95.11,
            "unified": 486.1,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "360c/s",
            "CodeArena": "604",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "102c/s",
            "CodeArena": "591",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-03T04:03:12+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 884.56,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "188c/s",
            "CodeArena": "1,553",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.99,
            "value": 6.28,
            "unified": 906.61,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "165c/s",
            "CodeArena": "1,523",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.61,
            "value": 5.37,
            "unified": 871.07,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "34c/s",
            "CodeArena": "1,437",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.09,
            "value": 22.6,
            "unified": 832.86,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "230c/s",
            "CodeArena": "1,366",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 49.38,
            "value": 2.74,
            "unified": 507.94,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "137c/s",
            "CodeArena": "1,004",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 59.95,
            "value": 5.33,
            "unified": 618.8,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "60c/s",
            "CodeArena": "959",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 52.78,
            "value": 4.69,
            "unified": 544.82,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "118c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 29.92,
            "value": 1.9,
            "unified": 308.01,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "139c/s",
            "CodeArena": "876",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 23.68,
            "value": 0.26,
            "unified": 242.45,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "116c/s",
            "CodeArena": "859",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 58.17,
            "value": 5.17,
            "unified": 600.41,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "201c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 70.73,
            "value": 22.81,
            "unified": 747.5,
            "Model": "Kimi K2.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "261c/s",
            "CodeArena": "1,181",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.3,
            "value": 20.11,
            "unified": 597.07,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "144c/s",
            "CodeArena": "989",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 38.76,
            "value": 14.15,
            "unified": 411.37,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "323c/s",
            "CodeArena": "955",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 33.49,
            "value": 22.33,
            "unified": 366.15,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "192c/s",
            "CodeArena": "759",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.05,
            "value": 62.92,
            "unified": 516.92,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "83c/s",
            "CodeArena": "747",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 58.18,
            "value": 23.56,
            "unified": 619.96,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "123c/s",
            "CodeArena": "669",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.04,
            "value": 5.02,
            "unified": 107.95,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "304c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 2.63,
            "value": 0.85,
            "unified": 27.8,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "81c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 37.9,
            "value": 94.76,
            "unified": 487.67,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "352c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "99c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-02T15:32:45+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 886.62,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "203c/s",
            "CodeArena": "1,560",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "",
            "created": "Nov. 2025",
            "avgIq": 87.67,
            "value": 6.26,
            "unified": 906.23,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "174c/s",
            "CodeArena": "1,514",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "",
            "created": "Dec. 2025",
            "avgIq": 84.58,
            "value": 5.37,
            "unified": 871.83,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "36c/s",
            "CodeArena": "1,436",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.1,
            "value": 22.6,
            "unified": 830.35,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "246c/s",
            "CodeArena": "1,363",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.36,
            "value": 2.8,
            "unified": 499.26,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "139c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 60.89,
            "value": 5.41,
            "unified": 615.81,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "56c/s",
            "CodeArena": "959",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.83,
            "value": 4.78,
            "unified": 538.81,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "125c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.53,
            "value": 2.0,
            "unified": 294.89,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "154c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.05,
            "value": 5.25,
            "unified": 595.78,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "202c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.15,
            "value": 0.27,
            "unified": 213.33,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "112c/s",
            "CodeArena": "850",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 70.47,
            "value": 22.73,
            "unified": 737.21,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "259c/s",
            "CodeArena": "1,156",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.76,
            "value": 20.27,
            "unified": 586.48,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "141c/s",
            "CodeArena": "987",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.28,
            "value": 14.34,
            "unified": 391.4,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "356c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.15,
            "value": 22.77,
            "unified": 344.62,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "195c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.66,
            "value": 63.8,
            "unified": 500.6,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "86c/s",
            "CodeArena": "730",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 11.05,
            "value": 5.52,
            "unified": 77.06,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "293c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.89,
            "value": 1.58,
            "unified": 6.38,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "78c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.87,
            "value": 97.16,
            "unified": 472.39,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "357c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.42,
            "value": 6.31,
            "unified": 6.24,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "99c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 584.29,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "153c/s",
            "CodeArena": "542",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-02T04:09:40+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 884.0,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "207c/s",
            "CodeArena": "1,554",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.91,
            "value": 6.28,
            "unified": 906.21,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "174c/s",
            "CodeArena": "1,519",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.61,
            "value": 5.37,
            "unified": 869.5,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "37c/s",
            "CodeArena": "1,432",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.19,
            "value": 22.62,
            "unified": 828.64,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "250c/s",
            "CodeArena": "1,362",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.48,
            "value": 2.8,
            "unified": 497.65,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "144c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.05,
            "value": 5.43,
            "unified": 614.71,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "58c/s",
            "CodeArena": "959",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.98,
            "value": 4.8,
            "unified": 537.58,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "139c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.92,
            "value": 2.03,
            "unified": 296.02,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "152c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.17,
            "value": 5.26,
            "unified": 594.15,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "208c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.17,
            "value": 0.27,
            "unified": 210.37,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "113c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.13,
            "value": 22.95,
            "unified": 741.86,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "268c/s",
            "CodeArena": "1,182",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.37,
            "value": 14.37,
            "unified": 389.36,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "397c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.05,
            "value": 20.02,
            "unified": 575.65,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "139c/s",
            "CodeArena": "950",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.22,
            "value": 22.82,
            "unified": 342.36,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "196c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.77,
            "value": 63.96,
            "unified": 498.88,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "87c/s",
            "CodeArena": "730",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 11.13,
            "value": 5.57,
            "unified": 74.71,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "303c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 5.06,
            "value": 1.63,
            "unified": 4.97,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.93,
            "value": 97.33,
            "unified": 470.09,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "353c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.73,
            "value": 6.76,
            "unified": 6.67,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "104c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 581.36,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "161c/s",
            "CodeArena": "539",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-01T15:27:00+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 884.0,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "205c/s",
            "CodeArena": "1,554",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.91,
            "value": 6.28,
            "unified": 906.21,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "172c/s",
            "CodeArena": "1,519",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.61,
            "value": 5.37,
            "unified": 869.5,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "37c/s",
            "CodeArena": "1,432",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.19,
            "value": 22.62,
            "unified": 828.64,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "253c/s",
            "CodeArena": "1,362",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.48,
            "value": 2.8,
            "unified": 497.65,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "143c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.05,
            "value": 5.43,
            "unified": 614.71,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "59c/s",
            "CodeArena": "959",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.98,
            "value": 4.8,
            "unified": 537.58,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "139c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.92,
            "value": 2.03,
            "unified": 296.02,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "156c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.17,
            "value": 5.26,
            "unified": 594.15,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "163c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.17,
            "value": 0.27,
            "unified": 210.37,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "113c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.13,
            "value": 22.95,
            "unified": 741.86,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "267c/s",
            "CodeArena": "1,182",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.37,
            "value": 14.37,
            "unified": 389.36,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "416c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.05,
            "value": 20.02,
            "unified": 575.65,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "141c/s",
            "CodeArena": "950",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.22,
            "value": 22.82,
            "unified": 342.36,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "198c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.77,
            "value": 63.96,
            "unified": 498.88,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "83c/s",
            "CodeArena": "730",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 11.13,
            "value": 5.57,
            "unified": 74.71,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "284c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 5.06,
            "value": 1.63,
            "unified": 4.97,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.93,
            "value": 97.33,
            "unified": 470.09,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "345c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.73,
            "value": 6.76,
            "unified": 6.67,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "103c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 581.36,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "165c/s",
            "CodeArena": "539",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-02-01T04:13:11+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 885.07,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "205c/s",
            "CodeArena": "1,560",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.81,
            "value": 6.27,
            "unified": 906.16,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "171c/s",
            "CodeArena": "1,520",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.52,
            "value": 5.37,
            "unified": 869.57,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "40c/s",
            "CodeArena": "1,432",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 78.99,
            "value": 22.57,
            "unified": 827.43,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "276c/s",
            "CodeArena": "1,357",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.4,
            "value": 2.8,
            "unified": 497.54,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "151c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.64,
            "value": 5.48,
            "unified": 621.93,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "60c/s",
            "CodeArena": "977",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.9,
            "value": 4.79,
            "unified": 537.46,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "133c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.73,
            "value": 2.01,
            "unified": 294.55,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "180c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.11,
            "value": 5.25,
            "unified": 594.39,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "163c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.13,
            "value": 0.27,
            "unified": 210.44,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "112c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.23,
            "value": 22.98,
            "unified": 743.8,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "271c/s",
            "CodeArena": "1,190",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.32,
            "value": 14.35,
            "unified": 389.41,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "430c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.0,
            "value": 20.0,
            "unified": 575.86,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "141c/s",
            "CodeArena": "950",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.2,
            "value": 22.8,
            "unified": 342.65,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "198c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.74,
            "value": 63.91,
            "unified": 499.14,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "72c/s",
            "CodeArena": "730",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.89,
            "value": 5.44,
            "unified": 72.35,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "284c/s",
            "CodeArena": "659",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 5.03,
            "value": 1.62,
            "unified": 4.96,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "80c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.93,
            "value": 97.32,
            "unified": 470.64,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "334c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.7,
            "value": 6.72,
            "unified": 6.66,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "103c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 582.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "165c/s",
            "CodeArena": "539",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-31T15:26:44+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 885.6,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "212c/s",
            "CodeArena": "1,559",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.76,
            "value": 6.27,
            "unified": 906.21,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "175c/s",
            "CodeArena": "1,517",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.6,
            "value": 5.37,
            "unified": 871.0,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "50c/s",
            "CodeArena": "1,436",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 78.98,
            "value": 22.57,
            "unified": 827.85,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "289c/s",
            "CodeArena": "1,356",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.42,
            "value": 2.8,
            "unified": 497.93,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "158c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.65,
            "value": 5.48,
            "unified": 622.44,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "63c/s",
            "CodeArena": "977",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.91,
            "value": 4.79,
            "unified": 537.89,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "126c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.76,
            "value": 2.02,
            "unified": 294.99,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "207c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.12,
            "value": 5.26,
            "unified": 594.81,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "167c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.14,
            "value": 0.27,
            "unified": 210.55,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "113c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.24,
            "value": 22.98,
            "unified": 744.36,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "277c/s",
            "CodeArena": "1,190",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.33,
            "value": 14.35,
            "unified": 389.67,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "417c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.01,
            "value": 20.0,
            "unified": 576.25,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "171c/s",
            "CodeArena": "950",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.2,
            "value": 22.8,
            "unified": 342.83,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "240c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.74,
            "value": 63.92,
            "unified": 499.42,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "69c/s",
            "CodeArena": "730",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.89,
            "value": 5.45,
            "unified": 72.32,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "281c/s",
            "CodeArena": "659",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 5.03,
            "value": 1.62,
            "unified": 4.91,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "76c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.93,
            "value": 97.32,
            "unified": 470.81,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "330c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.71,
            "value": 6.72,
            "unified": 6.6,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "104c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 582.46,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "176c/s",
            "CodeArena": "539",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-31T03:59:54+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 884.94,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "210c/s",
            "CodeArena": "1,560",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.83,
            "value": 6.27,
            "unified": 906.15,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "176c/s",
            "CodeArena": "1,521",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.49,
            "value": 5.36,
            "unified": 869.25,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "50c/s",
            "CodeArena": "1,431",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 78.95,
            "value": 22.56,
            "unified": 827.3,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "301c/s",
            "CodeArena": "1,356",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.35,
            "value": 2.8,
            "unified": 498.59,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "160c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.55,
            "value": 5.47,
            "unified": 622.11,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "63c/s",
            "CodeArena": "977",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.8,
            "value": 4.78,
            "unified": 537.92,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "122c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.47,
            "value": 2.0,
            "unified": 294.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "198c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.03,
            "value": 5.25,
            "unified": 594.77,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "171c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.07,
            "value": 0.27,
            "unified": 212.77,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "116c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.19,
            "value": 22.97,
            "unified": 744.12,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "282c/s",
            "CodeArena": "1,190",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.27,
            "value": 14.33,
            "unified": 391.1,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "424c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.97,
            "value": 19.99,
            "unified": 576.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "172c/s",
            "CodeArena": "951",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.13,
            "value": 22.76,
            "unified": 344.45,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "241c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.51,
            "value": 63.58,
            "unified": 498.43,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "67c/s",
            "CodeArena": "726",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.97,
            "value": 5.49,
            "unified": 77.01,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "278c/s",
            "CodeArena": "666",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.84,
            "value": 1.56,
            "unified": 6.79,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "78c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.84,
            "value": 97.11,
            "unified": 472.02,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "319c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.33,
            "value": 6.18,
            "unified": 6.07,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "115c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 583.59,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "169c/s",
            "CodeArena": "543",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-31T00:44:52+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 884.94,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "210c/s",
            "CodeArena": "1,560",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.83,
            "value": 6.27,
            "unified": 906.15,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "175c/s",
            "CodeArena": "1,521",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.49,
            "value": 5.36,
            "unified": 869.25,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "51c/s",
            "CodeArena": "1,431",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 78.95,
            "value": 22.56,
            "unified": 827.3,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "300c/s",
            "CodeArena": "1,356",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.35,
            "value": 2.8,
            "unified": 498.59,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "159c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.55,
            "value": 5.47,
            "unified": 622.11,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "63c/s",
            "CodeArena": "977",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 53.8,
            "value": 4.78,
            "unified": 537.92,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "122c/s",
            "CodeArena": "908",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 31.47,
            "value": 2.0,
            "unified": 294.26,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "198c/s",
            "CodeArena": "863",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.03,
            "value": 5.25,
            "unified": 594.77,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "171c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.07,
            "value": 0.27,
            "unified": 212.77,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "118c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.19,
            "value": 22.97,
            "unified": 744.12,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "282c/s",
            "CodeArena": "1,190",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.27,
            "value": 14.33,
            "unified": 391.1,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "424c/s",
            "CodeArena": "954",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.97,
            "value": 19.99,
            "unified": 576.96,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "171c/s",
            "CodeArena": "951",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.13,
            "value": 22.76,
            "unified": 344.45,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "242c/s",
            "CodeArena": "755",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.51,
            "value": 63.58,
            "unified": 498.43,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "67c/s",
            "CodeArena": "726",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.97,
            "value": 5.49,
            "unified": 77.01,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "278c/s",
            "CodeArena": "666",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.84,
            "value": 1.56,
            "unified": 6.79,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "78c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.84,
            "value": 97.11,
            "unified": 472.02,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "319c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.33,
            "value": 6.18,
            "unified": 6.07,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "115c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 583.59,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "169c/s",
            "CodeArena": "543",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-30T15:30:30+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 887.27,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "209c/s",
            "CodeArena": "1,560",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.61,
            "value": 6.26,
            "unified": 906.15,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "160c/s",
            "CodeArena": "1,511",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.23,
            "value": 5.35,
            "unified": 868.72,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "53c/s",
            "CodeArena": "1,416",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 78.99,
            "value": 22.57,
            "unified": 829.88,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "290c/s",
            "CodeArena": "1,358",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.21,
            "value": 2.79,
            "unified": 498.39,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "148c/s",
            "CodeArena": "1,012",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.55,
            "value": 5.47,
            "unified": 623.74,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "63c/s",
            "CodeArena": "977",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 54.33,
            "value": 4.83,
            "unified": 545.1,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "119c/s",
            "CodeArena": "922",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 32.06,
            "value": 2.04,
            "unified": 301.45,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "192c/s",
            "CodeArena": "869",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.03,
            "value": 5.25,
            "unified": 596.32,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "157c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 23.96,
            "value": 0.27,
            "unified": 212.12,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "115c/s",
            "CodeArena": "842",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.06,
            "value": 22.92,
            "unified": 744.58,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "283c/s",
            "CodeArena": "1,184",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.06,
            "value": 20.02,
            "unified": 579.39,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "186c/s",
            "CodeArena": "955",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.18,
            "value": 14.3,
            "unified": 391.13,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "421c/s",
            "CodeArena": "950",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.11,
            "value": 22.74,
            "unified": 345.06,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "188c/s",
            "CodeArena": "754",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.54,
            "value": 63.63,
            "unified": 499.96,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "68c/s",
            "CodeArena": "727",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 11.02,
            "value": 5.51,
            "unified": 77.76,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "186c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 4.84,
            "value": 1.56,
            "unified": 6.81,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "92c/s",
            "CodeArena": "634",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.84,
            "value": 97.11,
            "unified": 473.0,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "295c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 4.33,
            "value": 6.18,
            "unified": 6.07,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "117c/s",
            "CodeArena": "587",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 585.07,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "174c/s",
            "CodeArena": "543",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-30T04:00:35+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.18,
            "value": 2.87,
            "unified": 883.63,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "208c/s",
            "CodeArena": "1,543",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.97,
            "value": 6.28,
            "unified": 906.17,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "134c/s",
            "CodeArena": "1,511",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.49,
            "value": 5.36,
            "unified": 868.1,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "53c/s",
            "CodeArena": "1,416",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 79.5,
            "value": 22.71,
            "unified": 832.8,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "275c/s",
            "CodeArena": "1,369",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.57,
            "value": 2.81,
            "unified": 503.65,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "146c/s",
            "CodeArena": "1,017",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-high-2025-08-07",
            "origin": "US",
            "description": "GPT-5 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant is optimized for coding and agentic tasks across domains, offering deeper analysis and ",
            "created": "Aug. 2025",
            "avgIq": 61.83,
            "value": 5.5,
            "unified": 626.54,
            "Model": "GPT-5 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "63c/s",
            "CodeArena": "977",
            "GPQA": "87.3%",
            "AIME2025": "94.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 54.58,
            "value": 4.85,
            "unified": 548.52,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "119c/s",
            "CodeArena": "922",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 32.6,
            "value": 2.07,
            "unified": 311.21,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "184c/s",
            "CodeArena": "869",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 59.19,
            "value": 5.26,
            "unified": 598.11,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "154c/s",
            "CodeArena": "852",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 24.19,
            "value": 0.27,
            "unified": 219.59,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "113c/s",
            "CodeArena": "847",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 71.89,
            "value": 23.19,
            "unified": 752.16,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "292c/s",
            "CodeArena": "1,211",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.14,
            "value": 20.05,
            "unified": 580.89,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "186c/s",
            "CodeArena": "952",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.33,
            "value": 14.35,
            "unified": 395.7,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "422c/s",
            "CodeArena": "950",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 34.19,
            "value": 22.79,
            "unified": 349.52,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "186c/s",
            "CodeArena": "754",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 44.48,
            "value": 63.54,
            "unified": 501.35,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "66c/s",
            "CodeArena": "722",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 11.08,
            "value": 5.54,
            "unified": 85.2,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "187c/s",
            "CodeArena": "668",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 5.68,
            "value": 1.83,
            "unified": 23.75,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "89c/s",
            "CodeArena": "648",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 38.86,
            "value": 97.16,
            "unified": 476.17,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "287c/s",
            "CodeArena": "594",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 3.6,
            "value": 5.14,
            "unified": 5.03,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "114c/s",
            "CodeArena": "579",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.32,
            "value": 22.8,
            "unified": 585.59,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "178c/s",
            "CodeArena": "543",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-29T15:29:06+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 85.77,
            "value": 6.13,
            "unified": 909.71,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "103c/s",
            "CodeArena": "1,572",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 83.58,
            "value": 2.79,
            "unified": 879.81,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "205c/s",
            "CodeArena": "1,545",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.37,
            "value": 5.36,
            "unified": 892.89,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "54c/s",
            "CodeArena": "1,452",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.28,
            "value": 21.79,
            "unified": 830.21,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "260c/s",
            "CodeArena": "1,259",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 50.73,
            "value": 2.82,
            "unified": 514.38,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "142c/s",
            "CodeArena": "1,132",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 25.49,
            "value": 0.28,
            "unified": 229.38,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "98c/s",
            "CodeArena": "1,122",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 33.6,
            "value": 0.37,
            "unified": 319.81,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "104c/s",
            "CodeArena": "933",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 62.28,
            "value": 5.54,
            "unified": 647.47,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "99c/s",
            "CodeArena": "849",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 56.67,
            "value": 5.04,
            "unified": 584.22,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "156c/s",
            "CodeArena": "813",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 21.95,
            "value": 1.39,
            "unified": 191.85,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "183c/s",
            "CodeArena": "779",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 54.24,
            "value": 19.37,
            "unified": 581.0,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "182c/s",
            "CodeArena": "1,039",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 66.29,
            "value": 21.38,
            "unified": 718.4,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "295c/s",
            "CodeArena": "1,002",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 35.35,
            "value": 12.9,
            "unified": 360.15,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "422c/s",
            "CodeArena": "953",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 33.36,
            "value": 22.24,
            "unified": 353.57,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "179c/s",
            "CodeArena": "847",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.22,
            "value": 60.32,
            "unified": 515.54,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "66c/s",
            "CodeArena": "656",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 9.55,
            "value": 13.64,
            "unified": 74.28,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "110c/s",
            "CodeArena": "653",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 4.87,
            "value": 2.44,
            "unified": 3.64,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "174c/s",
            "CodeArena": "649",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 25.74,
            "value": 37.86,
            "unified": 294.8,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "163.8k",
            "Input$/M": "$0.27",
            "Output$/M": "$0.41",
            "Speed": "60c/s",
            "CodeArena": "646",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 55.42,
            "value": 22.44,
            "unified": 599.24,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "175c/s",
            "CodeArena": "596",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 34.25,
            "value": 48.93,
            "unified": 407.86,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "80c/s",
            "CodeArena": "556",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-29T04:00:23+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 85.9,
            "value": 6.14,
            "unified": 910.22,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "95c/s",
            "CodeArena": "1,564",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 85.38,
            "value": 2.85,
            "unified": 899.32,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "203c/s",
            "CodeArena": "1,539",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.61,
            "value": 5.37,
            "unified": 895.41,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "53c/s",
            "CodeArena": "1,452",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.1,
            "value": 21.74,
            "unified": 833.53,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "257c/s",
            "CodeArena": "1,245",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 29.43,
            "value": 0.33,
            "unified": 308.91,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "132c/s",
            "CodeArena": "1,135",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 53.77,
            "value": 2.99,
            "unified": 568.36,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "143c/s",
            "CodeArena": "1,123",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 37.05,
            "value": 0.41,
            "unified": 388.87,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "95c/s",
            "CodeArena": "935",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.37,
            "value": 5.72,
            "unified": 683.9,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "100c/s",
            "CodeArena": "863",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 57.4,
            "value": 5.1,
            "unified": 609.84,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "161c/s",
            "CodeArena": "797",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 22.08,
            "value": 1.4,
            "unified": 233.66,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "174c/s",
            "CodeArena": "777",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.42,
            "value": 19.79,
            "unified": 613.56,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "183c/s",
            "CodeArena": "1,030",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.17,
            "value": 21.67,
            "unified": 739.83,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "299c/s",
            "CodeArena": "1,023",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 38.22,
            "value": 13.95,
            "unified": 423.68,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "425c/s",
            "CodeArena": "937",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 36.2,
            "value": 24.13,
            "unified": 419.39,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "154c/s",
            "CodeArena": "847",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 15.74,
            "value": 22.49,
            "unified": 202.35,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "107c/s",
            "CodeArena": "713",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.07,
            "value": 60.11,
            "unified": 540.82,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "65c/s",
            "CodeArena": "655",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.29,
            "value": 4.64,
            "unified": 105.05,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "173c/s",
            "CodeArena": "649",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 29.07,
            "value": 42.76,
            "unified": 375.75,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "163.8k",
            "Input$/M": "$0.27",
            "Output$/M": "$0.41",
            "Speed": "60c/s",
            "CodeArena": "644",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.6,
            "value": 22.91,
            "unified": 631.13,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "172c/s",
            "CodeArena": "593",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "90c/s",
            "CodeArena": "554",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-28T15:31:16+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 85.9,
            "value": 6.14,
            "unified": 910.27,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "94c/s",
            "CodeArena": "1,560",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 84.86,
            "value": 2.83,
            "unified": 893.79,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "203c/s",
            "CodeArena": "1,517",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 84.56,
            "value": 5.37,
            "unified": 894.91,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "53c/s",
            "CodeArena": "1,446",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.05,
            "value": 21.73,
            "unified": 833.1,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "256c/s",
            "CodeArena": "1,241",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 29.44,
            "value": 0.33,
            "unified": 308.95,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "132c/s",
            "CodeArena": "1,135",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 53.83,
            "value": 2.99,
            "unified": 568.97,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "142c/s",
            "CodeArena": "1,125",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 37.06,
            "value": 0.41,
            "unified": 388.97,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "106c/s",
            "CodeArena": "937",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 64.28,
            "value": 5.71,
            "unified": 683.03,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "863",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
            "origin": "US",
            "description": "The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort.",
            "created": "Nov. 2025",
            "avgIq": 57.31,
            "value": 5.09,
            "unified": 609.01,
            "Model": "GPT-5.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "180c/s",
            "CodeArena": "797",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 22.88,
            "value": 1.45,
            "unified": 242.12,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "172c/s",
            "CodeArena": "788",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 67.68,
            "value": 21.83,
            "unified": 745.63,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "307c/s",
            "CodeArena": "1,047",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 55.51,
            "value": 19.83,
            "unified": 614.76,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "182c/s",
            "CodeArena": "1,035",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 38.19,
            "value": 13.94,
            "unified": 423.42,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "423c/s",
            "CodeArena": "937",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 36.15,
            "value": 24.1,
            "unified": 419.04,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "136c/s",
            "CodeArena": "847",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 15.38,
            "value": 21.98,
            "unified": 197.95,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "108c/s",
            "CodeArena": "713",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 41.85,
            "value": 59.78,
            "unified": 538.43,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "64c/s",
            "CodeArena": "652",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 9.19,
            "value": 4.59,
            "unified": 103.94,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "172c/s",
            "CodeArena": "649",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 28.98,
            "value": 42.61,
            "unified": 374.89,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "163.8k",
            "Input$/M": "$0.27",
            "Output$/M": "$0.41",
            "Speed": "62c/s",
            "CodeArena": "644",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.5,
            "value": 22.87,
            "unified": 630.21,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "185c/s",
            "CodeArena": "593",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.0,
            "value": 0.0,
            "unified": 0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "92c/s",
            "CodeArena": "559",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-28T03:52:42+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 85.93,
            "value": 6.14,
            "unified": 910.41,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "92c/s",
            "CodeArena": "1,594",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 83.75,
            "value": 2.79,
            "unified": 881.76,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "203c/s",
            "CodeArena": "1,509",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 83.28,
            "value": 5.29,
            "unified": 881.11,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "65c/s",
            "CodeArena": "1,430",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 75.3,
            "value": 21.51,
            "unified": 825.05,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "251c/s",
            "CodeArena": "1,231",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 28.93,
            "value": 0.32,
            "unified": 301.6,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "154c/s",
            "CodeArena": "1,132",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 52.21,
            "value": 2.9,
            "unified": 550.58,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "144c/s",
            "CodeArena": "1,072",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 36.66,
            "value": 0.41,
            "unified": 382.96,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "116c/s",
            "CodeArena": "933",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 24.59,
            "value": 1.56,
            "unified": 258.17,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "179c/s",
            "CodeArena": "803",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "Nov. 2025",
            "avgIq": 53.13,
            "value": 4.72,
            "unified": 563.39,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "142c/s",
            "CodeArena": "785",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 47.71,
            "value": 4.24,
            "unified": 505.56,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "98c/s",
            "CodeArena": "769",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 54.47,
            "value": 19.45,
            "unified": 602.71,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "180c/s",
            "CodeArena": "998",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 38.37,
            "value": 14.0,
            "unified": 424.21,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "448c/s",
            "CodeArena": "938",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Kimi K2.5",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2.5",
            "origin": "CN",
            "description": "Kimi K2.5 is Moonshot AI's flagship agentic model and a new SOTA open model. It unifies vision and text, thinking and non-thinking modes, and single-agent and multi-agent execution into one model. Bui",
            "created": "Jan. 2026",
            "avgIq": 63.9,
            "value": 20.61,
            "unified": 703.8,
            "Model": "Kimi K2.5\nNEW",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "251c/s",
            "CodeArena": "886",
            "GPQA": "87.6%",
            "AIME2025": "96.1%",
            "SWE-benchVerified": "76.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "74.9%",
            "CharXiv-R": "77.5%",
            "MMMU-Pro": "78.5%",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "50.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 35.88,
            "value": 23.92,
            "unified": 415.07,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "111c/s",
            "CodeArena": "821",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 16.02,
            "value": 22.88,
            "unified": 204.58,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "109c/s",
            "CodeArena": "713",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 8.97,
            "value": 4.48,
            "unified": 99.0,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "173c/s",
            "CodeArena": "631",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 40.89,
            "value": 58.42,
            "unified": 526.87,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "65c/s",
            "CodeArena": "618",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.26,
            "value": 0.09,
            "unified": 0.04,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "84c/s",
            "CodeArena": "550",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 56.03,
            "value": 22.68,
            "unified": 624.61,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "186c/s",
            "CodeArena": "549",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 27.12,
            "value": 39.88,
            "unified": 350.36,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "163.8k",
            "Input$/M": "$0.27",
            "Output$/M": "$0.41",
            "Speed": "57c/s",
            "CodeArena": "545",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-27T15:30:15+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 85.08,
            "value": 6.08,
            "unified": 908.0,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "36c/s",
            "CodeArena": "1,572",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 84.93,
            "value": 2.83,
            "unified": 900.82,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "198c/s",
            "CodeArena": "1,530",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 85.26,
            "value": 5.41,
            "unified": 908.81,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "79c/s",
            "CodeArena": "1,454",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 73.96,
            "value": 21.13,
            "unified": 815.2,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "207c/s",
            "CodeArena": "1,207",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 29.16,
            "value": 0.32,
            "unified": 300.86,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "116c/s",
            "CodeArena": "1,132",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 52.52,
            "value": 2.92,
            "unified": 554.78,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "143c/s",
            "CodeArena": "1,072",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 37.01,
            "value": 0.41,
            "unified": 384.89,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "112c/s",
            "CodeArena": "936",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "Nov. 2025",
            "avgIq": 55.1,
            "value": 4.9,
            "unified": 585.75,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "148c/s",
            "CodeArena": "785",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Thinking",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-thinking-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Thinking is the next iteration of GPT-5, designed with enhanced adaptive reasoning capabilities. Unlike GPT-5.1 Instant, GPT-5.1 Thinking adapts thinking time more precisely to each question, ",
            "created": "Nov. 2025",
            "avgIq": 56.63,
            "value": 5.03,
            "unified": 602.32,
            "Model": "GPT-5.1 Thinking",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "77c/s",
            "CodeArena": "744",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 47.24,
            "value": 4.2,
            "unified": 500.61,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "92c/s",
            "CodeArena": "723",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.1,
            "value": 20.04,
            "unified": 622.61,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "153c/s",
            "CodeArena": "990",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.56,
            "value": 14.44,
            "unified": 436.21,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "470c/s",
            "CodeArena": "934",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 37.47,
            "value": 24.98,
            "unified": 432.14,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "48c/s",
            "CodeArena": "823",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 14.12,
            "value": 20.17,
            "unified": 174.46,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "104c/s",
            "CodeArena": "678",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 40.83,
            "value": 58.33,
            "unified": 525.53,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "64c/s",
            "CodeArena": "594",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 58.3,
            "value": 23.6,
            "unified": 652.23,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "192c/s",
            "CodeArena": "564",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.54,
            "value": 3.77,
            "unified": 75.85,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "174c/s",
            "CodeArena": "555",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.99,
            "value": 0.32,
            "unified": -0.04,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "86c/s",
            "CodeArena": "550",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 28.08,
            "value": 41.3,
            "unified": 359.99,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "163.8k",
            "Input$/M": "$0.27",
            "Output$/M": "$0.41",
            "Speed": "55c/s",
            "CodeArena": "537",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 37.82,
            "value": 54.03,
            "unified": 485.96,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "77c/s",
            "CodeArena": "531",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-27T03:56:35+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "",
            "created": "Nov. 2025",
            "avgIq": 85.08,
            "value": 6.08,
            "unified": 909.07,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "36c/s",
            "CodeArena": "1,573",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "",
            "created": "Nov. 2025",
            "avgIq": 85.07,
            "value": 2.84,
            "unified": 903.39,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "198c/s",
            "CodeArena": "1,536",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 85.16,
            "value": 5.41,
            "unified": 908.82,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "98c/s",
            "CodeArena": "1,449",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 73.81,
            "value": 21.09,
            "unified": 814.48,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "220c/s",
            "CodeArena": "1,200",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 29.15,
            "value": 0.32,
            "unified": 301.09,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "107c/s",
            "CodeArena": "1,132",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 52.5,
            "value": 2.92,
            "unified": 555.29,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "142c/s",
            "CodeArena": "1,072",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 37.01,
            "value": 0.41,
            "unified": 385.26,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "114c/s",
            "CodeArena": "936",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "Nov. 2025",
            "avgIq": 55.08,
            "value": 4.9,
            "unified": 586.3,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "153c/s",
            "CodeArena": "785",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Thinking",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-thinking-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Thinking is the next iteration of GPT-5, designed with enhanced adaptive reasoning capabilities. Unlike GPT-5.1 Instant, GPT-5.1 Thinking adapts thinking time more precisely to each question, ",
            "created": "Nov. 2025",
            "avgIq": 56.68,
            "value": 5.04,
            "unified": 603.56,
            "Model": "GPT-5.1 Thinking",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "75c/s",
            "CodeArena": "746",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5-medium-2025-08-07",
            "origin": "US",
            "description": "GPT-5 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. Optimized for coding and agentic tasks, this variant offers more thorough anal",
            "created": "Aug. 2025",
            "avgIq": 47.23,
            "value": 4.2,
            "unified": 501.13,
            "Model": "GPT-5 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "92c/s",
            "CodeArena": "723",
            "GPQA": "88.1%",
            "AIME2025": "88.9%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Sep. 2024",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 56.1,
            "value": 20.03,
            "unified": 623.21,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "153c/s",
            "CodeArena": "990",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 39.65,
            "value": 14.47,
            "unified": 437.78,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "470c/s",
            "CodeArena": "939",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 37.53,
            "value": 25.02,
            "unified": 433.23,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "46c/s",
            "CodeArena": "826",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 14.11,
            "value": 20.15,
            "unified": 174.45,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "109c/s",
            "CodeArena": "678",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 40.83,
            "value": 58.33,
            "unified": 526.01,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "63c/s",
            "CodeArena": "594",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 58.3,
            "value": 23.6,
            "unified": 652.95,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "195c/s",
            "CodeArena": "564",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 7.54,
            "value": 3.77,
            "unified": 75.93,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "206c/s",
            "CodeArena": "555",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.99,
            "value": 0.32,
            "unified": -0.05,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "88c/s",
            "CodeArena": "550",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Exp",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It e",
            "created": "Sep. 2025",
            "avgIq": 28.08,
            "value": 41.3,
            "unified": 360.33,
            "Model": "DeepSeek-V3.2-Exp",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "163.8k",
            "Input$/M": "$0.27",
            "Output$/M": "$0.41",
            "Speed": "60c/s",
            "CodeArena": "537",
            "GPQA": "79.9%",
            "AIME2025": "89.3%",
            "SWE-benchVerified": "67.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "40.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "97.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.7%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 37.82,
            "value": 54.03,
            "unified": 486.43,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "77c/s",
            "CodeArena": "531",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-24T18:32:59+00:00",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.1,
            "value": 2.87,
            "unified": 887.09,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "184c/s",
            "CodeArena": "1,546",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.63,
            "value": 6.26,
            "unified": 906.27,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "35c/s",
            "CodeArena": "1,537",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 85.86,
            "value": 5.45,
            "unified": 887.25,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "86c/s",
            "CodeArena": "1,480",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 74.39,
            "value": 21.25,
            "unified": 785.18,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "178c/s",
            "CodeArena": "1,165",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 30.59,
            "value": 0.34,
            "unified": 312.67,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "121c/s",
            "CodeArena": "1,132",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 52.58,
            "value": 2.92,
            "unified": 541.85,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "123c/s",
            "CodeArena": "1,049",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 38.16,
            "value": 0.42,
            "unified": 390.74,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "115c/s",
            "CodeArena": "936",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "Nov. 2025",
            "avgIq": 56.45,
            "value": 5.02,
            "unified": 583.8,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "128c/s",
            "CodeArena": "785",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 High",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-high-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 High provides enhanced reasoning capabilities with high-effort thinking for complex problems. This variant offers deeper analysis and more thorough responses compared to the base model, making",
            "created": "Nov. 2025",
            "avgIq": 62.17,
            "value": 5.53,
            "unified": 643.29,
            "Model": "GPT-5.1 High",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "100c/s",
            "CodeArena": "746",
            "GPQA": "88.1%",
            "AIME2025": "99.6%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Thinking",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-thinking-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Thinking is the next iteration of GPT-5, designed with enhanced adaptive reasoning capabilities. Unlike GPT-5.1 Instant, GPT-5.1 Thinking adapts thinking time more precisely to each question, ",
            "created": "Nov. 2025",
            "avgIq": 57.32,
            "value": 5.1,
            "unified": 592.87,
            "Model": "GPT-5.1 Thinking",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "80c/s",
            "CodeArena": "744",
            "GPQA": "88.1%",
            "AIME2025": "94.0%",
            "SWE-benchVerified": "76.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "85.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "26.7%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 59.13,
            "value": 21.12,
            "unified": 627.83,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "181c/s",
            "CodeArena": "1,034",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 40.23,
            "value": 14.68,
            "unified": 426.61,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "486c/s",
            "CodeArena": "920",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 38.06,
            "value": 25.37,
            "unified": 415.09,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "197c/s",
            "CodeArena": "828",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 13.94,
            "value": 19.92,
            "unified": 161.14,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "111c/s",
            "CodeArena": "651",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.37,
            "value": 60.52,
            "unified": 495.3,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "74c/s",
            "CodeArena": "619",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 59.64,
            "value": 24.15,
            "unified": 636.18,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "151c/s",
            "CodeArena": "571",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 8.86,
            "value": 4.43,
            "unified": 93.0,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "327c/s",
            "CodeArena": "540",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 0.26,
            "value": 0.08,
            "unified": 0.0,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "81c/s",
            "CodeArena": "511",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.31,
            "value": 98.27,
            "unified": 502.22,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "117c/s",
            "CodeArena": "510",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 37.73,
            "value": 53.9,
            "unified": 440.75,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "68c/s",
            "CodeArena": "506",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-24T12:28:43+07:59",
      "teams": {
        "US": [
          {
            "model": "Claude Opus 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "Nov. 2025",
            "avgIq": 86.3,
            "value": 2.88,
            "unified": 892.42,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "187c/s",
            "CodeArena": "1,554",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "Nov. 2025",
            "avgIq": 87.27,
            "value": 6.23,
            "unified": 905.93,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "34c/s",
            "CodeArena": "1,527",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "Dec. 2025",
            "avgIq": 85.59,
            "value": 5.43,
            "unified": 887.56,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "86c/s",
            "CodeArena": "1,490",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "Dec. 2025",
            "avgIq": 76.06,
            "value": 21.73,
            "unified": 804.13,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "178c/s",
            "CodeArena": "1,166",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "May 2025",
            "avgIq": 31.49,
            "value": 0.35,
            "unified": 315.37,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "122c/s",
            "CodeArena": "1,120",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "Sep. 2025",
            "avgIq": 53.19,
            "value": 2.96,
            "unified": 545.48,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "122c/s",
            "CodeArena": "1,059",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "Anthropic",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "Aug. 2025",
            "avgIq": 38.42,
            "value": 0.43,
            "unified": 388.1,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "114c/s",
            "CodeArena": "901",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "Nov. 2025",
            "avgIq": 56.21,
            "value": 5.0,
            "unified": 579.15,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "125c/s",
            "CodeArena": "785",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "Gemini 2.5 Pro",
            "organization": "Google",
            "link": "https://llm-stats.com/models/gemini-2.5-pro",
            "origin": "US",
            "description": "A highly capable AI model from Google, designed for the agentic era. Gemini 2.5 Pro performs well on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input",
            "created": "May 2025",
            "avgIq": 21.47,
            "value": 1.91,
            "unified": 211.93,
            "Model": "Gemini 2.5 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "161c/s",
            "CodeArena": "727",
            "GPQA": "83.0%",
            "AIME2025": "83.0%",
            "SWE-benchVerified": "63.2%",
            "ARC-AGIv2": "4.9%",
            "MMMLU": "-",
            "MMMU": "79.6%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "50.8%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes"
          },
          {
            "model": "GPT-5.2 Codex",
            "organization": "OpenAI",
            "link": "https://llm-stats.com/models/gpt-5.2-codex",
            "origin": "US",
            "description": "GPT-5.2 Codex is a version of GPT-5.2 optimized for agentic coding tasks in Codex or similar environments. It features higher reasoning capabilities and is available in the Responses API. The underlyi",
            "created": "Jan. 2026",
            "avgIq": 19.78,
            "value": 1.26,
            "unified": 193.51,
            "Model": "GPT-5.2 Codex",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "118c/s",
            "CodeArena": "694",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "Dec. 2025",
            "avgIq": 59.46,
            "value": 21.24,
            "unified": 629.62,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "181c/s",
            "CodeArena": "1,036",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "GLM-4.6",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "Sep. 2025",
            "avgIq": 40.89,
            "value": 14.92,
            "unified": 428.61,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "503c/s",
            "CodeArena": "920",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "MiniMax",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "Dec. 2025",
            "avgIq": 38.96,
            "value": 25.97,
            "unified": 419.53,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "197c/s",
            "CodeArena": "811",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "Dec. 2025",
            "avgIq": 15.76,
            "value": 22.52,
            "unified": 172.97,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "111c/s",
            "CodeArena": "651",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "Dec. 2025",
            "avgIq": 42.94,
            "value": 61.34,
            "unified": 497.0,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "77c/s",
            "CodeArena": "619",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "Sep. 2025",
            "avgIq": 60.26,
            "value": 24.39,
            "unified": 641.16,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "149c/s",
            "CodeArena": "571",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "GLM-4.5",
            "organization": "ZAI",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "Jul. 2025",
            "avgIq": 10.3,
            "value": 5.15,
            "unified": 98.11,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "328c/s",
            "CodeArena": "540",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "Xiaomi",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "Dec. 2025",
            "avgIq": 39.72,
            "value": 99.29,
            "unified": 501.6,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "118c/s",
            "CodeArena": "510",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "MoonshotAI",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "Sep. 2025",
            "avgIq": 1.4,
            "value": 0.45,
            "unified": 0.09,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "82c/s",
            "CodeArena": "509",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "DeepSeek",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "Dec. 2025",
            "avgIq": 37.77,
            "value": 53.95,
            "unified": 435.33,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "70c/s",
            "CodeArena": "482",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No"
          }
        ]
      }
    },
    {
      "timestamp": "2026-01-23T08:08:56+00:00",
      "teams": {
        "US": [
          {
            "model": "Gemini 3 Pro",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-3-pro-preview",
            "origin": "US",
            "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking",
            "created": "",
            "avgIq": 59.32,
            "value": 4.24,
            "unified": 849.08,
            "Model": "Gemini 3 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$2.00",
            "Output$/M": "$12.00",
            "Speed": "57c/s",
            "CodeArena": "1,536",
            "GPQA": "91.9%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "76.2%",
            "ARC-AGIv2": "31.1%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "81.4%",
            "MMMU-Pro": "81.0%",
            "ScreenSpotPro": "72.7%",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "72.1%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Opus 4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
            "origin": "US",
            "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance",
            "created": "",
            "avgIq": 53.51,
            "value": 1.78,
            "unified": 751.3,
            "Model": "Claude Opus 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$5.00",
            "Output$/M": "$25.00",
            "Speed": "168c/s",
            "CodeArena": "1,534",
            "GPQA": "87.0%",
            "AIME2025": "-",
            "SWE-benchVerified": "80.9%",
            "ARC-AGIv2": "37.6%",
            "MMMLU": "90.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "62.3%",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "66.3%",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Mar. 2025",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "GPT-5.2",
            "organization": "",
            "link": "https://llm-stats.com/models/gpt-5.2-2025-12-11",
            "origin": "US",
            "description": "GPT\u20115.2 introduces substantial gains in professional knowledge work, outperforming experts on GDPval with 70.9% wins or ties, and setting new highs in coding (SWE\u2011Bench Pro 55.6%), science (GPQA Diamo",
            "created": "",
            "avgIq": 62.82,
            "value": 3.99,
            "unified": 905.51,
            "Model": "GPT-5.2",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.75",
            "Output$/M": "$14.00",
            "Speed": "14c/s",
            "CodeArena": "1,469",
            "GPQA": "92.4%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "80.0%",
            "ARC-AGIv2": "52.9%",
            "MMMLU": "89.6%",
            "MMMU": "-",
            "BrowseComp": "65.8%",
            "CharXiv-R": "82.1%",
            "MMMU-Pro": "79.5%",
            "ScreenSpotPro": "86.3%",
            "MCPAtlas": "60.6%",
            "HLE": "34.5%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "46.3%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "40.3%",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Aug. 2025",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "OpenAI"
          },
          {
            "model": "Gemini 3 Flash",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-3-flash-preview",
            "origin": "US",
            "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. It combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1 million-to",
            "created": "",
            "avgIq": 53.36,
            "value": 15.25,
            "unified": 768.99,
            "Model": "Gemini 3 Flash",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1M",
            "Input$/M": "$0.50",
            "Output$/M": "$3.00",
            "Speed": "39c/s",
            "CodeArena": "1,179",
            "GPQA": "90.4%",
            "AIME2025": "99.7%",
            "SWE-benchVerified": "78.0%",
            "ARC-AGIv2": "33.6%",
            "MMMLU": "91.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "80.3%",
            "MMMU-Pro": "81.2%",
            "ScreenSpotPro": "69.1%",
            "MCPAtlas": "57.4%",
            "HLE": "-",
            "SimpleQA": "68.7%",
            "OSWorld": "-",
            "Toolathlon": "49.4%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Opus 4",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-20250514",
            "origin": "US",
            "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. O",
            "created": "",
            "avgIq": 26.95,
            "value": 0.3,
            "unified": 318.63,
            "Model": "Claude Opus 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "128c/s",
            "CodeArena": "1,126",
            "GPQA": "79.6%",
            "AIME2025": "75.5%",
            "SWE-benchVerified": "72.5%",
            "ARC-AGIv2": "8.6%",
            "MMMLU": "88.8%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "39.2%",
            "TAU2Retail": "81.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "Claude Sonnet 4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-sonnet-4-5-20250929",
            "origin": "US",
            "description": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and m",
            "created": "",
            "avgIq": 37.11,
            "value": 2.06,
            "unified": 485.95,
            "Model": "Claude Sonnet 4.5",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "157c/s",
            "CodeArena": "1,020",
            "GPQA": "83.4%",
            "AIME2025": "87.0%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "89.1%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "61.4%",
            "Toolathlon": "-",
            "TerminalBench": "50.0%",
            "TAU2Retail": "86.2%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "Sep. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "Claude Opus 4.1",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-opus-4-1-20250805",
            "origin": "US",
            "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and",
            "created": "",
            "avgIq": 30.93,
            "value": 0.34,
            "unified": 383.14,
            "Model": "Claude Opus 4.1",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$15.00",
            "Output$/M": "$75.00",
            "Speed": "121c/s",
            "CodeArena": "901",
            "GPQA": "80.9%",
            "AIME2025": "78.0%",
            "SWE-benchVerified": "74.5%",
            "ARC-AGIv2": "-",
            "MMMLU": "89.5%",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "43.3%",
            "TAU2Retail": "82.4%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Aug. 2025",
            "Organization": "Anthropic"
          },
          {
            "model": "GPT-5.1 Medium",
            "organization": "",
            "link": "https://llm-stats.com/models/gpt-5.1-medium-2025-11-12",
            "origin": "US",
            "description": "GPT-5.1 Medium balances reasoning depth with response speed, providing medium-effort thinking for moderately complex tasks. This variant offers more thorough analysis than instant responses while main",
            "created": "",
            "avgIq": 28.53,
            "value": 2.54,
            "unified": 347.62,
            "Model": "GPT-5.1 Medium",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "400k",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "144c/s",
            "CodeArena": "832",
            "GPQA": "-",
            "AIME2025": "98.4%",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Nov. 2025",
            "Organization": "OpenAI"
          },
          {
            "model": "Gemini 2.5 Pro",
            "organization": "",
            "link": "https://llm-stats.com/models/gemini-2.5-pro",
            "origin": "US",
            "description": "A highly capable AI model from Google, designed for the agentic era. Gemini 2.5 Pro performs well on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input",
            "created": "",
            "avgIq": 16.61,
            "value": 1.48,
            "unified": 152.78,
            "Model": "Gemini 2.5 Pro",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "1.0M",
            "Input$/M": "$1.25",
            "Output$/M": "$10.00",
            "Speed": "169c/s",
            "CodeArena": "727",
            "GPQA": "83.0%",
            "AIME2025": "83.0%",
            "SWE-benchVerified": "63.2%",
            "ARC-AGIv2": "4.9%",
            "MMMLU": "-",
            "MMMU": "79.6%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "50.8%",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "Jan. 2025",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Google"
          },
          {
            "model": "Claude Sonnet 4",
            "organization": "",
            "link": "https://llm-stats.com/models/claude-sonnet-4-20250514",
            "origin": "US",
            "description": "Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4",
            "created": "",
            "avgIq": 11.59,
            "value": 0.64,
            "unified": 70.2,
            "Model": "Claude Sonnet 4",
            "Country": "\ud83c\uddfa\ud83c\uddf8",
            "License": "Closed",
            "Context": "200k",
            "Input$/M": "$3.00",
            "Output$/M": "$15.00",
            "Speed": "192c/s",
            "CodeArena": "686",
            "GPQA": "75.4%",
            "AIME2025": "70.5%",
            "SWE-benchVerified": "72.7%",
            "ARC-AGIv2": "-",
            "MMMLU": "86.5%",
            "MMMU": "74.4%",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "35.5%",
            "TAU2Retail": "80.5%",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "May 2025",
            "Organization": "Anthropic"
          }
        ],
        "CN": [
          {
            "model": "GLM-4.7",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.7",
            "origin": "CN",
            "description": "GLM 4.7 is a coding\u2011centric model that thinks before acting, preserves its reasoning across turns, and lets you control thinking per request for speed or accuracy. It upgrades agentic workflows with s",
            "created": "",
            "avgIq": 40.36,
            "value": 14.41,
            "unified": 557.06,
            "Model": "GLM-4.7",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "204.8k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.20",
            "Speed": "57c/s",
            "CodeArena": "1,050",
            "GPQA": "85.7%",
            "AIME2025": "95.7%",
            "SWE-benchVerified": "73.8%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "52.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "42.8%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "33.3%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "358",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Dec. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "GLM-4.6",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.6",
            "origin": "CN",
            "description": "GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performanc",
            "created": "",
            "avgIq": 28.19,
            "value": 10.29,
            "unified": 353.63,
            "Model": "GLM-4.6",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.55",
            "Output$/M": "$2.19",
            "Speed": "260c/s",
            "CodeArena": "908",
            "GPQA": "81.0%",
            "AIME2025": "93.9%",
            "SWE-benchVerified": "68.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "45.1%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "17.2%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "40.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "357",
            "KnowledgeCutoff": "-",
            "Multimodal": "Yes",
            "Released": "Sep. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "MiniMax M2.1",
            "organization": "",
            "link": "https://llm-stats.com/models/minimax-m2.1",
            "origin": "CN",
            "description": "MiniMax M2.1 is an enhanced large language model focused on multi-language programming and real-world complex tasks. It features exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Object",
            "created": "",
            "avgIq": 29.66,
            "value": 19.77,
            "unified": 391.69,
            "Model": "MiniMax M2.1",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "1M",
            "Input$/M": "$0.30",
            "Output$/M": "$1.20",
            "Speed": "222c/s",
            "CodeArena": "811",
            "GPQA": "81.0%",
            "AIME2025": "81.0%",
            "SWE-benchVerified": "67.0%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "62.0%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "43.5%",
            "TerminalBench": "47.9%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "-",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "MiniMax"
          },
          {
            "model": "DeepSeek-V3.2-Speciale",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
            "origin": "CN",
            "description": "DeepSeek-V3.2-Speciale is a specialized variant of DeepSeek-V3.2, optimized for enhanced performance on specific tasks.",
            "created": "",
            "avgIq": 29.56,
            "value": 42.22,
            "unified": 423.53,
            "Model": "DeepSeek-V3.2-Speciale",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "76c/s",
            "CodeArena": "611",
            "GPQA": "-",
            "AIME2025": "96.0%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "30.6%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          },
          {
            "model": "DeepSeek-V3.2 (Non-thinking)",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-chat",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in non-thinking mode. A powerful language model with 685B parameters using DeepSeek Sparse Attention (DSA) for efficient long-context processing. Supports JSON output, tool calls, and ch",
            "created": "",
            "avgIq": 9.37,
            "value": 13.38,
            "unified": 53.22,
            "Model": "DeepSeek-V3.2 (Non-thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "102c/s",
            "CodeArena": "571",
            "GPQA": "-",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          },
          {
            "model": "Kimi K2-Thinking-0905",
            "organization": "",
            "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
            "origin": "CN",
            "description": "Kimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, it is built as a thinking agent that reasons step-by-step while dynamically invoking tools. I",
            "created": "",
            "avgIq": 46.46,
            "value": 18.81,
            "unified": 662.46,
            "Model": "Kimi K2-Thinking-0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "262.1k",
            "Input$/M": "$0.47",
            "Output$/M": "$2.00",
            "Speed": "127c/s",
            "CodeArena": "571",
            "GPQA": "84.5%",
            "AIME2025": "100.0%",
            "SWE-benchVerified": "71.3%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "60.2%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "51.0%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "47.1%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Sep. 2025",
            "Organization": "MoonshotAI"
          },
          {
            "model": "GLM-4.5",
            "organization": "",
            "link": "https://llm-stats.com/models/glm-4.5",
            "origin": "CN",
            "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Tr",
            "created": "",
            "avgIq": 7.29,
            "value": 3.64,
            "unified": 4.98,
            "Model": "GLM-4.5",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.40",
            "Output$/M": "$1.60",
            "Speed": "274c/s",
            "CodeArena": "540",
            "GPQA": "79.1%",
            "AIME2025": "-",
            "SWE-benchVerified": "64.2%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "26.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "14.4%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "37.5%",
            "TAU2Retail": "79.7%",
            "FrontierMath": "-",
            "Parameters(B)": "355",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Jul. 2025",
            "Organization": "ZAI"
          },
          {
            "model": "Kimi K2 0905",
            "organization": "",
            "link": "https://llm-stats.com/models/kimi-k2-0905",
            "origin": "CN",
            "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active",
            "created": "",
            "avgIq": 11.37,
            "value": 3.67,
            "unified": 71.23,
            "Model": "Kimi K2 0905",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Closed",
            "Context": "262.1k",
            "Input$/M": "$0.60",
            "Output$/M": "$2.50",
            "Speed": "101c/s",
            "CodeArena": "509",
            "GPQA": "75.8%",
            "AIME2025": "-",
            "SWE-benchVerified": "-",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "-",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "-",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "1000",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Sep. 2025",
            "Organization": "MoonshotAI"
          },
          {
            "model": "MiMo-V2-Flash",
            "organization": "",
            "link": "https://llm-stats.com/models/mimo-v2-flash",
            "origin": "CN",
            "description": "MiMo-V2-Flash is a powerful, efficient, and ultra-fast foundation language model that excels in reasoning, coding, and agentic scenarios. It is a Mixture-of-Experts model with 309B total parameters an",
            "created": "",
            "avgIq": 26.89,
            "value": 67.24,
            "unified": 417.73,
            "Model": "MiMo-V2-Flash",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "256k",
            "Input$/M": "$0.10",
            "Output$/M": "$0.30",
            "Speed": "303c/s",
            "CodeArena": "505",
            "GPQA": "83.7%",
            "AIME2025": "94.1%",
            "SWE-benchVerified": "73.4%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "58.3%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "22.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "-",
            "TerminalBench": "30.5%",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "309",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "Xiaomi"
          },
          {
            "model": "DeepSeek-V3.2 (Thinking)",
            "organization": "",
            "link": "https://llm-stats.com/models/deepseek-reasoner",
            "origin": "CN",
            "description": "DeepSeek-V3.2 in thinking mode. A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). This mode enables extended chain-of-thought reasoning for complex problem-solving",
            "created": "",
            "avgIq": 29.15,
            "value": 41.64,
            "unified": 416.07,
            "Model": "DeepSeek-V3.2 (Thinking)",
            "Country": "\ud83c\udde8\ud83c\uddf3",
            "License": "Open",
            "Context": "131.1k",
            "Input$/M": "$0.28",
            "Output$/M": "$0.42",
            "Speed": "63c/s",
            "CodeArena": "489",
            "GPQA": "82.4%",
            "AIME2025": "93.1%",
            "SWE-benchVerified": "73.1%",
            "ARC-AGIv2": "-",
            "MMMLU": "-",
            "MMMU": "-",
            "BrowseComp": "51.4%",
            "CharXiv-R": "-",
            "MMMU-Pro": "-",
            "ScreenSpotPro": "-",
            "MCPAtlas": "-",
            "HLE": "25.1%",
            "SimpleQA": "-",
            "OSWorld": "-",
            "Toolathlon": "35.2%",
            "TerminalBench": "-",
            "TAU2Retail": "-",
            "FrontierMath": "-",
            "Parameters(B)": "685",
            "KnowledgeCutoff": "-",
            "Multimodal": "No",
            "Released": "Dec. 2025",
            "Organization": "DeepSeek"
          }
        ]
      }
    }
  ]
}