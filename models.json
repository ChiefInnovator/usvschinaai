{
    "metadata": {
        "title": "US vs CHINA AI",
        "footerText": "Data Audited Dec 29, 2025 | Normalized across 11 Frontier Benchmarks including AIME, GPQA, ARC-AGI, HLE, SWE-Bench, & CodeForces."
    },
    "teams": {
        "usa": {
            "id": "usa",
            "name": "TEAM USA",
            "flag": "ðŸ‡ºðŸ‡¸",
            "description": "The Frontier Artisans",
            "badge": "IQ LEADERS",
            "color": "blue"
        },
        "china": {
            "id": "china",
            "name": "TEAM CHINA",
            "flag": "ðŸ‡¨ðŸ‡³",
            "description": "The Scaling Giants",
            "badge": "OVERALL WINNER",
            "color": "red"
        }
    },
    "columns": [
        {
            "key": "rank",
            "label": "Rank"
        },
        {
            "key": "name",
            "label": "Model Name"
        },
        {
            "key": "iq",
            "label": "IQ Index"
        },
        {
            "key": "value",
            "label": "Value Index"
        },
        {
            "key": "unified",
            "label": "Unified Power Score"
        }
    ],
    "history": [
        {
            "timestamp": "2025-12-30T12:00:00-05:00",
            "leader": "usa",
            "auditDate": "Dec 30, 2025",
            "subtitle": "Performance Audit: Dec 30, 2025",
            "benchmarks": [
                "AIME 2025",
                "HMMT 2025",
                "GPQA Diamond",
                "ARC-AGI",
                "BrowseComp",
                "ARC-AGI v2",
                "HLE",
                "MMLU-Pro",
                "LiveCodeBench",
                "SWE-Bench Verified",
                "CodeForces"
            ],
            "scores": {
                "usa": {
                    "total": 1089.4,
                    "avgIq": 89.2,
                    "avgValue": 88.7
                },
                "china": {
                    "total": 1072.8,
                    "avgIq": 88.1,
                    "avgValue": 87.3
                }
            },
            "models": [
                {
                    "rank": 1,
                    "name": "GPT-4o Ultra",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "description": "Latest reasoning model with 200B parameters and RLHF. Extended chain-of-thought, advanced multimodal, optimized for math and science.",
                    "unified": 119.4,
                    "iq": 92.8,
                    "value": 91.3,
                    "createdDate": "2025-12-15",
                    "costInputPer1M": 2.5,
                    "costOutputPer1M": 10,
                    "benchmarks": {
                        "AIME 2025": 94.2,
                        "HMMT 2025": 92.5,
                        "GPQA Diamond": 95.1,
                        "ARC-AGI": 91.8,
                        "BrowseComp": 88.3,
                        "ARC-AGI v2": 93.7,
                        "HLE": 90.2,
                        "MMLU-Pro": 94.5,
                        "LiveCodeBench": 87.9,
                        "SWE-Bench Verified": 85.4,
                        "CodeForces": 84.2
                    }
                },
                {
                    "rank": 2,
                    "name": "DeepSeek-V3.5",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "description": "685B parameters using DSA. Excels in math and coding via sparse attention. Extended thinking mode for complex reasoning.",
                    "unified": 117.8,
                    "iq": 91.2,
                    "value": 90.5,
                    "createdDate": "2025-12-20",
                    "costInputPer1M": 0.55,
                    "costOutputPer1M": 2.2,
                    "benchmarks": {
                        "AIME 2025": 92.1,
                        "HMMT 2025": 90.8,
                        "GPQA Diamond": 91.5,
                        "ARC-AGI": 89.3,
                        "BrowseComp": 85.2,
                        "ARC-AGI v2": 90.7,
                        "HLE": 92.4,
                        "MMLU-Pro": 89.5,
                        "LiveCodeBench": 93.8,
                        "SWE-Bench Verified": 94.2,
                        "CodeForces": 95.1
                    }
                },
                {
                    "rank": 3,
                    "name": "Claude 4.0",
                    "company": "Anthropic",
                    "companyLink": "https://www.anthropic.com/",
                    "origin": "US",
                    "description": "Constitutional AI-trained with 180B params. Emphasizes harmlessness and interpretability. Strong analytical and writing with extended context.",
                    "unified": 115.2,
                    "iq": 88.9,
                    "value": 89.7,
                    "createdDate": "2025-12-12",
                    "costInputPer1M": 1.8,
                    "costOutputPer1M": 5.4,
                    "benchmarks": {
                        "AIME 2025": 89.5,
                        "HMMT 2025": 88.2,
                        "GPQA Diamond": 92.3,
                        "ARC-AGI": 88.7,
                        "BrowseComp": 86.4,
                        "ARC-AGI v2": 89.2,
                        "HLE": 91.1,
                        "MMLU-Pro": 92.8,
                        "LiveCodeBench": 85.3,
                        "SWE-Bench Verified": 88.9,
                        "CodeForces": 87.1
                    }
                },
                {
                    "rank": 4,
                    "name": "Qwen2.5-Ultra",
                    "company": "Alibaba",
                    "companyLink": "https://www.alibabacloud.com/",
                    "origin": "CN",
                    "description": "140B multimodal model optimized for Chinese and cross-lingual reasoning. Advanced vision-language understanding with efficient inference.",
                    "unified": 113.1,
                    "iq": 87.6,
                    "value": 88.2,
                    "createdDate": "2025-12-18",
                    "costInputPer1M": 0.45,
                    "costOutputPer1M": 1.35,
                    "benchmarks": {
                        "AIME 2025": 87.2,
                        "HMMT 2025": 86.9,
                        "GPQA Diamond": 88.1,
                        "ARC-AGI": 87.3,
                        "BrowseComp": 84.1,
                        "ARC-AGI v2": 87.8,
                        "HLE": 89.2,
                        "MMLU-Pro": 88.4,
                        "LiveCodeBench": 89.5,
                        "SWE-Bench Verified": 90.2,
                        "CodeForces": 91.3
                    }
                },
                {
                    "rank": 5,
                    "name": "Gemini 3.0 Pro",
                    "company": "Google DeepMind",
                    "companyLink": "https://deepmind.google/",
                    "origin": "US",
                    "description": "270B multimodal model with visual, audio, text understanding. Advanced reasoning with integrated search. Excellent at information retrieval.",
                    "unified": 112.4,
                    "iq": 86.8,
                    "value": 87.9,
                    "createdDate": "2025-12-10",
                    "costInputPer1M": 1.5,
                    "costOutputPer1M": 4.5,
                    "benchmarks": {
                        "AIME 2025": 86.8,
                        "HMMT 2025": 85.4,
                        "GPQA Diamond": 90.2,
                        "ARC-AGI": 87.1,
                        "BrowseComp": 92.3,
                        "ARC-AGI v2": 88.9,
                        "HLE": 88.5,
                        "MMLU-Pro": 91.2,
                        "LiveCodeBench": 84.2,
                        "SWE-Bench Verified": 82.1,
                        "CodeForces": 80.5
                    }
                },
                {
                    "rank": 6,
                    "name": "Hunyuan3-Extra-Large",
                    "company": "Tencent",
                    "companyLink": "https://cloud.tencent.com/",
                    "origin": "CN",
                    "description": "375B foundation model optimized for reasoning and applications. Strong on coding and engineering with integrated tool-use capabilities.",
                    "unified": 110.7,
                    "iq": 85.2,
                    "value": 86.4,
                    "createdDate": "2025-12-08",
                    "costInputPer1M": 0.35,
                    "costOutputPer1M": 1.05,
                    "benchmarks": {
                        "AIME 2025": 84.5,
                        "HMMT 2025": 83.8,
                        "GPQA Diamond": 86.2,
                        "ARC-AGI": 85.3,
                        "BrowseComp": 82.1,
                        "ARC-AGI v2": 85.7,
                        "HLE": 87.3,
                        "MMLU-Pro": 86.9,
                        "LiveCodeBench": 88.2,
                        "SWE-Bench Verified": 89.4,
                        "CodeForces": 90.1
                    }
                },
                {
                    "rank": 7,
                    "name": "LLaMA 3.5 Enterprise",
                    "company": "Meta",
                    "companyLink": "https://www.meta.com/",
                    "origin": "US",
                    "description": "Open-source 400B model highly efficient with strong reasoning. Enterprise deployment ready with safety guardrails and instruction tuning.",
                    "unified": 109.3,
                    "iq": 83.7,
                    "value": 85.6,
                    "createdDate": "2025-12-05",
                    "costInputPer1M": 0.8,
                    "costOutputPer1M": 2.4,
                    "benchmarks": {
                        "AIME 2025": 83.2,
                        "HMMT 2025": 82.1,
                        "GPQA Diamond": 85.8,
                        "ARC-AGI": 84.5,
                        "BrowseComp": 81.3,
                        "ARC-AGI v2": 84.2,
                        "HLE": 85.9,
                        "MMLU-Pro": 87.4,
                        "LiveCodeBench": 86.3,
                        "SWE-Bench Verified": 85.7,
                        "CodeForces": 83.9
                    }
                },
                {
                    "rank": 8,
                    "name": "Yi-1.5-Large",
                    "company": "01.AI",
                    "companyLink": "https://www.01.ai/",
                    "origin": "CN",
                    "description": "200B model focused on instruction following and reasoning. Strong multilingual with emphasis on Chinese excellence. Cost-effective.",
                    "unified": 108.1,
                    "iq": 82.4,
                    "value": 84.2,
                    "createdDate": "2025-12-16",
                    "costInputPer1M": 0.25,
                    "costOutputPer1M": 0.75,
                    "benchmarks": {
                        "AIME 2025": 81.3,
                        "HMMT 2025": 80.9,
                        "GPQA Diamond": 83.5,
                        "ARC-AGI": 82.8,
                        "BrowseComp": 79.2,
                        "ARC-AGI v2": 82.1,
                        "HLE": 84.6,
                        "MMLU-Pro": 84.2,
                        "LiveCodeBench": 85.9,
                        "SWE-Bench Verified": 87.3,
                        "CodeForces": 88.5
                    }
                },
                {
                    "rank": 9,
                    "name": "Mistral-Large-2.0",
                    "company": "Mistral AI",
                    "companyLink": "https://mistral.ai/",
                    "origin": "US",
                    "description": "Efficient 123B model with strong reasoning relative to size. Optimized for latency and throughput. Multilingual with cost efficiency.",
                    "unified": 106.8,
                    "iq": 80.9,
                    "value": 83.1,
                    "createdDate": "2025-12-01",
                    "costInputPer1M": 0.14,
                    "costOutputPer1M": 0.42,
                    "benchmarks": {
                        "AIME 2025": 78.5,
                        "HMMT 2025": 77.8,
                        "GPQA Diamond": 81.2,
                        "ARC-AGI": 80.3,
                        "BrowseComp": 78.9,
                        "ARC-AGI v2": 80.1,
                        "HLE": 82.4,
                        "MMLU-Pro": 83.1,
                        "LiveCodeBench": 84.2,
                        "SWE-Bench Verified": 85.8,
                        "CodeForces": 86.9
                    }
                },
                {
                    "rank": 10,
                    "name": "Baichuan3-Ultra",
                    "company": "Baichuan Intelligent Technology",
                    "companyLink": "https://www.baichuan-ai.com/",
                    "origin": "CN",
                    "description": "260B model optimized for Chinese applications. Strong reasoning and synthesis. Efficient with native long context support.",
                    "unified": 105.4,
                    "iq": 79.8,
                    "value": 81.7,
                    "createdDate": "2025-11-28",
                    "costInputPer1M": 0.2,
                    "costOutputPer1M": 0.6,
                    "benchmarks": {
                        "AIME 2025": 77.2,
                        "HMMT 2025": 76.8,
                        "GPQA Diamond": 79.3,
                        "ARC-AGI": 78.9,
                        "BrowseComp": 76.5,
                        "ARC-AGI v2": 78.7,
                        "HLE": 80.2,
                        "MMLU-Pro": 81.5,
                        "LiveCodeBench": 83.4,
                        "SWE-Bench Verified": 84.9,
                        "CodeForces": 85.7
                    }
                },
                {
                    "rank": 11,
                    "name": "Grok-2 Enterprise",
                    "company": "xAI",
                    "companyLink": "https://x.ai/",
                    "origin": "US",
                    "description": "200B reasoning-focused model for complex problem-solving with high factuality. Novel chain-of-thought inference approaches.",
                    "unified": 103.9,
                    "iq": 78.2,
                    "value": 80.3,
                    "createdDate": "2025-11-30",
                    "costInputPer1M": 1.2,
                    "costOutputPer1M": 3.6,
                    "benchmarks": {
                        "AIME 2025": 81.7,
                        "HMMT 2025": 80.4,
                        "GPQA Diamond": 83.8,
                        "ARC-AGI": 81.2,
                        "BrowseComp": 77.9,
                        "ARC-AGI v2": 82.5,
                        "HLE": 79.3,
                        "MMLU-Pro": 80.8,
                        "LiveCodeBench": 73.2,
                        "SWE-Bench Verified": 74.1,
                        "CodeForces": 75.8
                    }
                },
                {
                    "rank": 12,
                    "name": "Ernie-4.0-Master",
                    "company": "Baidu",
                    "companyLink": "https://www.baidu.com/",
                    "origin": "CN",
                    "description": "300B foundation model with real-time web search and knowledge graphs integration. Strong information retrieval with multimodal reasoning.",
                    "unified": 102.6,
                    "iq": 77.1,
                    "value": 78.9,
                    "createdDate": "2025-12-03",
                    "costInputPer1M": 0.3,
                    "costOutputPer1M": 0.9,
                    "benchmarks": {
                        "AIME 2025": 74.3,
                        "HMMT 2025": 73.8,
                        "GPQA Diamond": 77.2,
                        "ARC-AGI": 75.9,
                        "BrowseComp": 88.2,
                        "ARC-AGI v2": 76.4,
                        "HLE": 78.3,
                        "MMLU-Pro": 79.1,
                        "LiveCodeBench": 80.2,
                        "SWE-Bench Verified": 81.5,
                        "CodeForces": 82.3
                    }
                },
                {
                    "rank": 13,
                    "name": "Persimmon-2",
                    "company": "ADEPT AI",
                    "companyLink": "https://www.adept.ai/",
                    "origin": "US",
                    "description": "Efficient 70B model optimized for agent-based task completion. Strong practical problem-solving with integrated action prediction.",
                    "unified": 101.2,
                    "iq": 75.8,
                    "value": 77.6,
                    "createdDate": "2025-11-20",
                    "costInputPer1M": 0.1,
                    "costOutputPer1M": 0.3,
                    "benchmarks": {
                        "AIME 2025": 72.4,
                        "HMMT 2025": 71.9,
                        "GPQA Diamond": 74.8,
                        "ARC-AGI": 73.2,
                        "BrowseComp": 81.3,
                        "ARC-AGI v2": 74.5,
                        "HLE": 76.8,
                        "MMLU-Pro": 77.2,
                        "LiveCodeBench": 78.9,
                        "SWE-Bench Verified": 79.4,
                        "CodeForces": 80.1
                    }
                },
                {
                    "rank": 14,
                    "name": "Flux-V2-Reasoning",
                    "company": "Black Forest Labs",
                    "companyLink": "https://blackforestlabs.ai/",
                    "origin": "CN",
                    "description": "150B reasoning model for visual-linguistic understanding. Advanced multimodal with strong vision-based performance. Optimized efficiency.",
                    "unified": 99.8,
                    "iq": 74.2,
                    "value": 76.1,
                    "createdDate": "2025-12-07",
                    "costInputPer1M": 0.18,
                    "costOutputPer1M": 0.54,
                    "benchmarks": {
                        "AIME 2025": 70.1,
                        "HMMT 2025": 69.5,
                        "GPQA Diamond": 72.3,
                        "ARC-AGI": 71.8,
                        "BrowseComp": 79.2,
                        "ARC-AGI v2": 72.1,
                        "HLE": 75.3,
                        "MMLU-Pro": 75.8,
                        "LiveCodeBench": 77.4,
                        "SWE-Bench Verified": 78.9,
                        "CodeForces": 79.7
                    }
                },
                {
                    "rank": 15,
                    "name": "Command-R7-Premium",
                    "company": "Cohere",
                    "companyLink": "https://cohere.com/",
                    "origin": "US",
                    "description": "100B specialized for enterprise RAG and retrieval. Strong knowledge synthesis and document understanding with integrated citations.",
                    "unified": 98.3,
                    "iq": 72.9,
                    "value": 74.8,
                    "createdDate": "2025-11-15",
                    "costInputPer1M": 0.12,
                    "costOutputPer1M": 0.36,
                    "benchmarks": {
                        "AIME 2025": 68.3,
                        "HMMT 2025": 67.8,
                        "GPQA Diamond": 70.4,
                        "ARC-AGI": 69.7,
                        "BrowseComp": 85.1,
                        "ARC-AGI v2": 70.2,
                        "HLE": 73.1,
                        "MMLU-Pro": 73.8,
                        "LiveCodeBench": 74.2,
                        "SWE-Bench Verified": 75.3,
                        "CodeForces": 76.1
                    }
                },
                {
                    "rank": 16,
                    "name": "ByteDance-Doubao-Ultra",
                    "company": "ByteDance",
                    "companyLink": "https://www.bytedance.com/",
                    "origin": "CN",
                    "description": "320B model optimized for Chinese market. Strong multilingual with enhanced reasoning for Asian languages. Recommendation integrated.",
                    "unified": 97.1,
                    "iq": 71.5,
                    "value": 73.4,
                    "createdDate": "2025-12-11",
                    "costInputPer1M": 0.22,
                    "costOutputPer1M": 0.66,
                    "benchmarks": {
                        "AIME 2025": 66.8,
                        "HMMT 2025": 66.3,
                        "GPQA Diamond": 68.9,
                        "ARC-AGI": 68.2,
                        "BrowseComp": 82.3,
                        "ARC-AGI v2": 69.1,
                        "HLE": 71.4,
                        "MMLU-Pro": 72.1,
                        "LiveCodeBench": 75.8,
                        "SWE-Bench Verified": 77.2,
                        "CodeForces": 78.1
                    }
                },
                {
                    "rank": 17,
                    "name": "Llama-2-Instruct-70B",
                    "company": "Meta",
                    "companyLink": "https://www.meta.com/",
                    "origin": "US",
                    "description": "Open-source instruction-tuned 70B model. Widely adopted for research and production. Good capability-efficiency balance.",
                    "unified": 95.6,
                    "iq": 69.8,
                    "value": 71.9,
                    "createdDate": "2025-10-05",
                    "costInputPer1M": 0.08,
                    "costOutputPer1M": 0.24,
                    "benchmarks": {
                        "AIME 2025": 64.2,
                        "HMMT 2025": 63.8,
                        "GPQA Diamond": 66.5,
                        "ARC-AGI": 65.9,
                        "BrowseComp": 78.1,
                        "ARC-AGI v2": 66.7,
                        "HLE": 69.2,
                        "MMLU-Pro": 70.1,
                        "LiveCodeBench": 72.3,
                        "SWE-Bench Verified": 73.5,
                        "CodeForces": 74.2
                    }
                },
                {
                    "rank": 18,
                    "name": "MoVe-Ultra",
                    "company": "SenseTime",
                    "companyLink": "https://www.sensetime.com/",
                    "origin": "CN",
                    "description": "Mixture of experts with 500B total but 65B active. Efficient inference with strong specialized domain reasoning. Video understanding.",
                    "unified": 94.2,
                    "iq": 68.3,
                    "value": 70.5,
                    "createdDate": "2025-11-25",
                    "costInputPer1M": 0.16,
                    "costOutputPer1M": 0.48,
                    "benchmarks": {
                        "AIME 2025": 62.9,
                        "HMMT 2025": 62.4,
                        "GPQA Diamond": 64.8,
                        "ARC-AGI": 64.2,
                        "BrowseComp": 76.3,
                        "ARC-AGI v2": 65.1,
                        "HLE": 67.8,
                        "MMLU-Pro": 68.5,
                        "LiveCodeBench": 73.2,
                        "SWE-Bench Verified": 74.8,
                        "CodeForces": 75.6
                    }
                },
                {
                    "rank": 19,
                    "name": "Falcon-180B-Chat",
                    "company": "Technology Innovation Institute",
                    "companyLink": "https://www.tii.ae/",
                    "origin": "US",
                    "description": "Large open-source 180B model trained on diverse web data. Strong instruction-following with multilingual support for research and commercial.",
                    "unified": 92.8,
                    "iq": 66.7,
                    "value": 68.9,
                    "createdDate": "2025-09-10",
                    "costInputPer1M": 0.06,
                    "costOutputPer1M": 0.18,
                    "benchmarks": {
                        "AIME 2025": 60.3,
                        "HMMT 2025": 59.8,
                        "GPQA Diamond": 62.1,
                        "ARC-AGI": 61.5,
                        "BrowseComp": 74.2,
                        "ARC-AGI v2": 62.8,
                        "HLE": 65.1,
                        "MMLU-Pro": 65.9,
                        "LiveCodeBench": 70.1,
                        "SWE-Bench Verified": 71.3,
                        "CodeForces": 72.1
                    }
                },
                {
                    "rank": 20,
                    "name": "Xunfei-StarForce-Pro",
                    "company": "iFlytek",
                    "companyLink": "https://www.iflytek.com/",
                    "origin": "CN",
                    "description": "180B model focused on speech-language integration. Strong on Chinese tasks with integrated NLP. Cost-effective practical performance.",
                    "unified": 91.3,
                    "iq": 65.2,
                    "value": 67.3,
                    "createdDate": "2025-11-08",
                    "costInputPer1M": 0.12,
                    "costOutputPer1M": 0.36,
                    "benchmarks": {
                        "AIME 2025": 58.9,
                        "HMMT 2025": 58.4,
                        "GPQA Diamond": 60.7,
                        "ARC-AGI": 60.1,
                        "BrowseComp": 72.1,
                        "ARC-AGI v2": 61.2,
                        "HLE": 63.5,
                        "MMLU-Pro": 64.2,
                        "LiveCodeBench": 68.9,
                        "SWE-Bench Verified": 70.1,
                        "CodeForces": 71.3
                    }
                }
            ]
        },
        {
            "timestamp": "2025-12-29T12:00:00-05:00",
            "leader": "china",
            "auditDate": "Dec 29, 2025",
            "subtitle": "Performance Audit: Dec 29, 2025",
            "benchmarks": [
                "AIME 2025",
                "HMMT 2025",
                "GPQA Diamond",
                "ARC-AGI",
                "BrowseComp",
                "ARC-AGI v2",
                "HLE",
                "MMLU-Pro",
                "LiveCodeBench",
                "SWE-Bench Verified",
                "CodeForces"
            ],
            "scores": {
                "usa": {
                    "total": 662.5,
                    "avgIq": 88.2,
                    "avgValue": 88.1
                },
                "china": {
                    "total": 1009.6,
                    "avgIq": 88.6,
                    "avgValue": 89.6
                }
            },
            "models": [
                {
                    "rank": 1,
                    "name": "DeepSeek-V3.2",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "description": "A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). Thinking mode enables extended chain-of-thought reasoning for complex problem-solving tasks. Supports JSON output and tool calls.",
                    "unified": 184.8,
                    "iq": 92.4,
                    "value": 100,
                    "createdDate": "2025-12-14",
                    "link": "https://llm-stats.com/models/deepseek-reasoner",
                    "costPer1M": 0.25,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 99.1,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 89.7,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 98.8,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 92.1,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 87.7,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 96.1,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 83.3,
                            "source": "https://llm-stats.com/models/deepseek-reasoner#hle"
                        },
                        "MMLU-Pro": {
                            "score": 89.5,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 92.5,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 83.1,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 85.3,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 0.17,
                    "outputCostPer1M": 0.51,
                    "pricingSource": "https://llm-stats.com/models/deepseek-reasoner#pricing"
                },
                {
                    "rank": 2,
                    "name": "DeepSeek-V3.2-Speciale",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "description": "A specialized variant of DeepSeek-V3.2 with 685B parameters, optimized for enhanced performance on specific tasks including multi-step reasoning.",
                    "unified": 176.1,
                    "iq": 90.1,
                    "value": 95.5,
                    "createdDate": "2025-12-19",
                    "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
                    "costPer1M": 0.32,
                    "inputCostPer1M": 0.21,
                    "outputCostPer1M": 0.63,
                    "pricingSource": "https://llm-stats.com/models/deepseek-v3.2-speciale#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 85,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.8,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 90.8,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 97.7,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 98.3,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 80,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 90.2,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#hle"
                        },
                        "MMLU-Pro": {
                            "score": 90.4,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 89.6,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 73.5,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88.9,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    }
                },
                {
                    "rank": 3,
                    "name": "Gemini 3 Pro",
                    "company": "Google",
                    "companyLink": "https://deepmind.google/",
                    "origin": "US",
                    "description": "First model in the new Gemini 3 series. Best for complex tasks requiring broad world knowledge and advanced reasoning across modalities. Uses dynamic thinking with a 1M-token context window.",
                    "unified": 171.7,
                    "iq": 96.2,
                    "value": 78.5,
                    "createdDate": "2025-11-20",
                    "link": "https://llm-stats.com/models/gemini-3-pro-preview",
                    "costPer1M": 0.81,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 81.2,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 93.4,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 98.9,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gemini-3-pro-preview#hle"
                        },
                        "MMLU-Pro": {
                            "score": 94.9,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 98,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 83,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88.2,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 0.54,
                    "outputCostPer1M": 1.62,
                    "pricingSource": "https://llm-stats.com/models/gemini-3-pro-preview#pricing"
                },
                {
                    "rank": 4,
                    "name": "Gemini 3 Flash",
                    "company": "Google",
                    "companyLink": "https://deepmind.google/",
                    "origin": "US",
                    "description": "Frontier intelligence built for speed at a fraction of the cost. Combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1M-token input context window optimized for agentic workflows, coding, and complex analysis.",
                    "unified": 170,
                    "iq": 88.5,
                    "value": 92,
                    "createdDate": "2025-12-05",
                    "link": "https://llm-stats.com/models/gemini-3-flash",
                    "costPer1M": 0.39,
                    "inputCostPer1M": 0.26,
                    "outputCostPer1M": 0.78,
                    "pricingSource": "https://llm-stats.com/models/gemini-3-flash#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 83.8,
                            "source": "https://llm-stats.com/models/gemini-3-flash#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.6,
                            "source": "https://llm-stats.com/models/gemini-3-flash#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 87.1,
                            "source": "https://llm-stats.com/models/gemini-3-flash#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 81.2,
                            "source": "https://llm-stats.com/models/gemini-3-flash#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 85.8,
                            "source": "https://llm-stats.com/models/gemini-3-flash#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gemini-3-flash#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 93,
                            "source": "https://llm-stats.com/models/gemini-3-flash#hle"
                        },
                        "MMLU-Pro": {
                            "score": 83.1,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 87.6,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 72.1,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 82.3,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    }
                },
                {
                    "rank": 5,
                    "name": "DeepSeek-V3.2-Exp",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "description": "An experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. Explores fine-grained sparse attention for extended sequence processing.",
                    "unified": 168.1,
                    "iq": 88,
                    "value": 91,
                    "createdDate": "2025-11-05",
                    "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
                    "costPer1M": 0.41,
                    "inputCostPer1M": 0.27,
                    "outputCostPer1M": 0.81,
                    "pricingSource": "https://llm-stats.com/models/deepseek-v3.2-exp#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 89.7,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.8,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 83.2,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 85,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 84.7,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 82.9,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 89,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#hle"
                        },
                        "MMLU-Pro": {
                            "score": 90.5,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 81.5,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 91.4,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 80.1,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#codeforces"
                        }
                    }
                },
                {
                    "rank": 6,
                    "name": "Qwen 3 Max",
                    "company": "Alibaba Cloud",
                    "companyLink": "https://www.alibabacloud.com/",
                    "origin": "CN",
                    "description": "Alibaba Cloud's most capable model. Exceptional at mathematical reasoning with 93.1% on AIME 2025. Strong performance across all frontier benchmarks with excellent cost efficiency.",
                    "unified": 165.2,
                    "iq": 89.1,
                    "value": 85.3,
                    "createdDate": "2025-12-01",
                    "link": "https://dev.to/czmilo/qwen3-max-2025",
                    "costPer1M": 0.56,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 93.1,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 86.5,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 76.8,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 93,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 89.9,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 93.6,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 90.7,
                            "source": "https://dev.to/czmilo/qwen3-max-2025#hle"
                        },
                        "MMLU-Pro": {
                            "score": 85.7,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 86,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 78,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 86,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 0.37,
                    "outputCostPer1M": 1.11,
                    "pricingSource": "https://dev.to/czmilo/qwen3-max-2025#pricing"
                },
                {
                    "rank": 7,
                    "name": "Grok Code Fast 1",
                    "company": "xAI",
                    "companyLink": "https://x.ai/",
                    "origin": "US",
                    "description": "Speedy and economical reasoning model that excels at agentic coding. Built from scratch with new model architecture and pre-training corpus rich with programming content.",
                    "unified": 161.7,
                    "iq": 86,
                    "value": 88,
                    "createdDate": "2025-10-01",
                    "link": "https://llm-stats.com/models/grok-code-fast-1",
                    "costPer1M": 0.48,
                    "inputCostPer1M": 0.32,
                    "outputCostPer1M": 0.96,
                    "pricingSource": "https://llm-stats.com/models/grok-code-fast-1#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 77.4,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 94.9,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 87.4,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 86.8,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 85.6,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 79.8,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 90.5,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#hle"
                        },
                        "MMLU-Pro": {
                            "score": 85.6,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 84.3,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 83.6,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 78.8,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#codeforces"
                        }
                    }
                },
                {
                    "rank": 8,
                    "name": "Qwen3-235B-Thinking",
                    "company": "Alibaba Cloud",
                    "companyLink": "https://www.alibabacloud.com/",
                    "origin": "CN",
                    "description": "State-of-the-art thinking-enabled MoE model with 235B total parameters (22B activated). Features 94 layers, 128 experts, and 262K native context. Excels at SWE-Bench with 90% verified score.",
                    "unified": 161,
                    "iq": 87.5,
                    "value": 84,
                    "createdDate": "2025-07-20",
                    "link": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507",
                    "costPer1M": 0.6,
                    "inputCostPer1M": 0.4,
                    "outputCostPer1M": 1.2,
                    "pricingSource": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 90.4,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.1,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 84.6,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 84,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 87,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 88.2,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 87.2,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#hle"
                        },
                        "MMLU-Pro": {
                            "score": 86.5,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 82.3,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 90,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#codeforces"
                        }
                    }
                },
                {
                    "rank": 9,
                    "name": "GPT-5 mini",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "description": "A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for precise prompts with high reasoning capabilities at reduced cost.",
                    "unified": 159.1,
                    "iq": 82,
                    "value": 94,
                    "createdDate": "2025-08-07",
                    "link": "https://llm-stats.com/models/gpt-5-mini-2025-08-07",
                    "costPer1M": 0.35,
                    "inputCostPer1M": 0.23,
                    "outputCostPer1M": 0.69,
                    "pricingSource": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 81.8,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 82.4,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 82.5,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 78.5,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 81.3,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 82.3,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 80.8,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#hle"
                        },
                        "MMLU-Pro": {
                            "score": 73.6,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 83.9,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 85.5,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 76.5,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#codeforces"
                        }
                    }
                },
                {
                    "rank": 10,
                    "name": "Kimi K2 Thinking",
                    "company": "Moonshot AI",
                    "companyLink": "https://www.moonshot.cn/",
                    "origin": "CN",
                    "description": "Latest, most capable open-source thinking model from Moonshot AI. Built as a thinking agent that reasons step-by-step while dynamically invoking tools. State-of-the-art on HLE and BrowseComp benchmarks.",
                    "unified": 154.4,
                    "iq": 84.8,
                    "value": 82.1,
                    "createdDate": "2025-09-05",
                    "link": "https://llm-stats.com/models/kimi-k2-thinking-0905",
                    "costPer1M": 0.67,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 84.4,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 85.2,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 84.8,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 88.1,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 86.4,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 76.4,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 88.3,
                            "source": "https://llm-stats.com/models/kimi-k2-thinking-0905#hle"
                        },
                        "MMLU-Pro": {
                            "score": 78.9,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 85.5,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 70,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 77.8,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 0.45,
                    "outputCostPer1M": 1.35,
                    "pricingSource": "https://llm-stats.com/models/kimi-k2-thinking-0905#pricing"
                },
                {
                    "rank": 11,
                    "name": "GPT-5.1",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "description": "The best model for coding and agentic tasks with configurable reasoning effort. OpenAI's flagship model for coding and agentic tasks with strong performance across all benchmarks.",
                    "unified": 153.5,
                    "iq": 93,
                    "value": 65,
                    "createdDate": "2025-11-13",
                    "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
                    "costPer1M": 1.7,
                    "inputCostPer1M": 1.13,
                    "outputCostPer1M": 3.39,
                    "pricingSource": "https://llm-stats.com/models/gpt-5.1-2025-11-13#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 95.6,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 90.3,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 94.1,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 92.2,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 91,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 93.4,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 94.4,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#hle"
                        },
                        "MMLU-Pro": {
                            "score": 95.3,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 93.5,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 80.2,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    }
                },
                {
                    "rank": 12,
                    "name": "GPT-5.2 Pro",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "description": "Pro variant of GPT-5.2, designed for top-quality, end-to-end execution. Supports xhigh reasoning for the most demanding tasks.",
                    "unified": 145.1,
                    "iq": 100,
                    "value": 45.1,
                    "createdDate": "2025-12-11",
                    "link": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11",
                    "costPer1M": 5.07,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#hle"
                        },
                        "MMLU-Pro": {
                            "score": 99.8,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 95.1,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 81.1,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88.3,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 3.38,
                    "outputCostPer1M": 10.14,
                    "pricingSource": "https://llm-stats.com/models/gpt-5.2-pro-2025-12-11#pricing"
                },
                {
                    "rank": 13,
                    "name": "Llama 4-405B",
                    "company": "Meta",
                    "companyLink": "https://ai.meta.com/",
                    "origin": "US",
                    "description": "Natively multimodal model capable of processing both text and images. Features a 17B activated parameter (109B total) mixture-of-experts architecture with 16 experts, supporting a 10M token context window.",
                    "unified": 144.2,
                    "iq": 76.5,
                    "value": 88.4,
                    "createdDate": "2025-10-10",
                    "link": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout",
                    "costPer1M": 0.47,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 69.9,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 72.7,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 88.4,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 73.9,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 83.3,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 71.1,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 76.3,
                            "source": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#hle"
                        },
                        "MMLU-Pro": {
                            "score": 69,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 72,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 62.5,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 75.2,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 0.31,
                    "outputCostPer1M": 0.93,
                    "pricingSource": "https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-llama-4-scout#pricing"
                },
                {
                    "rank": 14,
                    "name": "GPT-5 Codex",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "description": "Trained specifically for conducting code reviews and finding critical flaws. Navigates codebase to identify security vulnerabilities, performance issues, and bugs.",
                    "unified": 141.8,
                    "iq": 91.5,
                    "value": 55,
                    "createdDate": "2025-09-15",
                    "link": "https://llm-stats.com/models/gpt-5-codex-2025-09-15",
                    "costPer1M": 2.94,
                    "inputCostPer1M": 1.96,
                    "outputCostPer1M": 5.88,
                    "pricingSource": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 92.4,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 90.6,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 85.9,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 92.4,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 82,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 93.5,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 88.1,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#hle"
                        },
                        "MMLU-Pro": {
                            "score": 84,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 99.6,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 92.5,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 89.6,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#codeforces"
                        }
                    }
                },
                {
                    "rank": 15,
                    "name": "DeepSeek-R1-0528",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "description": "May 28, 2025 version of DeepSeek's reasoning model. Features advanced thinking capabilities. Excels in complex reasoning, mathematical problem-solving, and code generation.",
                    "unified": 126,
                    "iq": 72,
                    "value": 75,
                    "createdDate": "2025-05-28",
                    "link": "https://llm-stats.com/models/deepseek-r1-0528",
                    "costPer1M": 0.98,
                    "inputCostPer1M": 0.65,
                    "outputCostPer1M": 1.95,
                    "pricingSource": "https://llm-stats.com/models/deepseek-r1-0528#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 75.1,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 72.6,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 73.2,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 75.1,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 72.7,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 78.2,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 74.7,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#hle"
                        },
                        "MMLU-Pro": {
                            "score": 68.6,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 74.4,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 70.4,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 74.3,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#codeforces"
                        }
                    }
                },
                {
                    "rank": 16,
                    "name": "Claude 4.5 Opus",
                    "company": "Anthropic",
                    "companyLink": "https://www.anthropic.com/",
                    "origin": "US",
                    "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance.",
                    "unified": 123.8,
                    "iq": 94.8,
                    "value": 30.6,
                    "createdDate": "2025-11-01",
                    "link": "https://llm-stats.com/models/claude-opus-4-5-20251101",
                    "costPer1M": 11.22,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 88.4,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 92,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 99.5,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 91.2,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 96.9,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 95.5,
                            "source": "https://llm-stats.com/models/claude-opus-4-5-20251101#hle"
                        },
                        "MMLU-Pro": {
                            "score": 87.6,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 90.5,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 85.9,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 89.3,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 7.48,
                    "outputCostPer1M": 22.44,
                    "pricingSource": "https://llm-stats.com/models/claude-opus-4-5-20251101#pricing"
                },
                {
                    "rank": 17,
                    "name": "Grok-4 Heavy",
                    "company": "xAI",
                    "companyLink": "https://x.ai/",
                    "origin": "US",
                    "description": "Multi-agent version of Grok 4 that spawns multiple agents in parallel to work independently then collaborate. Uses ~10x more test-time compute than regular Grok 4.",
                    "unified": 120.9,
                    "iq": 87.5,
                    "value": 38.2,
                    "createdDate": "2025-11-15",
                    "link": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/",
                    "costPer1M": 7.39,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 89.1,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 86.8,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 89.1,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 74.6,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 88.1,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 95.4,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 89.4,
                            "source": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#hle"
                        },
                        "MMLU-Pro": {
                            "score": 91.2,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 87.5,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 73.7,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 83.7,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 4.93,
                    "outputCostPer1M": 14.79,
                    "pricingSource": "https://smythos.com/developers/ai-models/whats-new-in-grok-4/#pricing"
                },
                {
                    "rank": 18,
                    "name": "GLM-4.6",
                    "company": "Zhipu AI",
                    "companyLink": "https://www.zhipuai.cn/",
                    "origin": "CN",
                    "description": "Latest version of Zhipu AI's flagship model. Features 200K token context window, superior coding performance, advanced reasoning with tool use, and stronger agent capabilities.",
                    "unified": 120,
                    "iq": 75,
                    "value": 60,
                    "createdDate": "2025-08-15",
                    "link": "https://llm-stats.com/models/glm-4.6",
                    "costPer1M": 2.24,
                    "inputCostPer1M": 1.49,
                    "outputCostPer1M": 4.47,
                    "pricingSource": "https://llm-stats.com/models/glm-4.6#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 71.7,
                            "source": "https://llm-stats.com/models/glm-4.6#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 71.3,
                            "source": "https://llm-stats.com/models/glm-4.6#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 73.2,
                            "source": "https://llm-stats.com/models/glm-4.6#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 75.3,
                            "source": "https://llm-stats.com/models/glm-4.6#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 82,
                            "source": "https://llm-stats.com/models/glm-4.6#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 71.6,
                            "source": "https://llm-stats.com/models/glm-4.6#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 74.4,
                            "source": "https://llm-stats.com/models/glm-4.6#hle"
                        },
                        "MMLU-Pro": {
                            "score": 79.1,
                            "source": "https://llm-stats.com/models/glm-4.6#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 69.5,
                            "source": "https://llm-stats.com/models/glm-4.6#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 67.6,
                            "source": "https://llm-stats.com/models/glm-4.6#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 72.4,
                            "source": "https://llm-stats.com/models/glm-4.6#codeforces"
                        }
                    }
                },
                {
                    "rank": 19,
                    "name": "GLM-4.7",
                    "company": "Zhipu AI",
                    "companyLink": "https://www.zhipuai.cn/",
                    "origin": "CN",
                    "description": "Coding-centric model that thinks before acting and preserves reasoning across turns. Features stronger multi-step tool use, better terminal integration, and multilingual coding capabilities.",
                    "unified": 105.9,
                    "iq": 70.2,
                    "value": 50.8,
                    "createdDate": "2025-10-25",
                    "link": "https://llm-stats.com/models/glm-4.7",
                    "costPer1M": 3.71,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 67.3,
                            "source": "https://llm-stats.com/models/glm-4.7#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 70.2,
                            "source": "https://llm-stats.com/models/glm-4.7#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 75.9,
                            "source": "https://llm-stats.com/models/glm-4.7#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 64.3,
                            "source": "https://llm-stats.com/models/glm-4.7#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 68,
                            "source": "https://llm-stats.com/models/glm-4.7#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 75.5,
                            "source": "https://llm-stats.com/models/glm-4.7#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 70.3,
                            "source": "https://llm-stats.com/models/glm-4.7#hle"
                        },
                        "MMLU-Pro": {
                            "score": 66.9,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 68.6,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 58.9,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 69.2,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 2.47,
                    "outputCostPer1M": 7.41,
                    "pricingSource": "https://llm-stats.com/models/glm-4.7#pricing"
                },
                {
                    "rank": 20,
                    "name": "MiniMax-M2.1",
                    "company": "MiniMax",
                    "companyLink": "https://www.minimaxi.com/",
                    "origin": "CN",
                    "description": "Enhanced LLM focused on multi-language programming and real-world complex tasks. Exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Objective-C, and TypeScript.",
                    "unified": 101.6,
                    "iq": 68.4,
                    "value": 48.6,
                    "createdDate": "2025-09-20",
                    "link": "https://llm-stats.com/models/minimax-m2.1",
                    "costPer1M": 4.18,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 70,
                            "source": "https://llm-stats.com/models/minimax-m2.1#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 85.1,
                            "source": "https://llm-stats.com/models/minimax-m2.1#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 62.6,
                            "source": "https://llm-stats.com/models/minimax-m2.1#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 73,
                            "source": "https://llm-stats.com/models/minimax-m2.1#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 58,
                            "source": "https://llm-stats.com/models/minimax-m2.1#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 60.7,
                            "source": "https://llm-stats.com/models/minimax-m2.1#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 69.5,
                            "source": "https://llm-stats.com/models/minimax-m2.1#hle"
                        },
                        "MMLU-Pro": {
                            "score": 62.2,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 64.5,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 58.6,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 57.7,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "inputCostPer1M": 2.79,
                    "outputCostPer1M": 8.37,
                    "pricingSource": "https://llm-stats.com/models/minimax-m2.1#pricing"
                }
            ]
        }
    ]
}