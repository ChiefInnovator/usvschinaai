name: Daily AI Model Scrape

on:
  schedule:
    # Run twice daily at off-peak times (3:17 AM and 3:17 PM UTC)
    - cron: '17 3 * * *'
    - cron: '17 15 * * *'
  # Allow manual triggering from Actions tab
  workflow_dispatch:

permissions:
  contents: write
  
jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: 'scripts/requirements.txt'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/requirements.txt
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
      
      - name: Run scraper
        run: |
          python scripts/scrape_models.py

      - name: Run news scraper
        continue-on-error: true
        env:
          NEWSDATA_API_KEY: ${{ secrets.NEWSDATA_API_KEY }}
        run: |
          python scripts/scrape_news.py

      - name: Generate OG image
        continue-on-error: true
        run: |
          python scripts/generate_og_image.py

      - name: Update sitemap lastmod dates
        run: |
          TODAY=$(date -u +'%Y-%m-%d')
          sed -i "s|<loc>https://usvschina.ai/</loc>\n\s*<lastmod>[^<]*</lastmod>|<loc>https://usvschina.ai/</loc>\n        <lastmod>${TODAY}</lastmod>|" sitemap.xml || true
          python3 -c "
          import re, sys
          today = '${TODAY}'
          with open('sitemap.xml', 'r') as f:
              content = f.read()
          # Update lastmod for dynamic pages
          dynamic = ['https://usvschina.ai/', 'https://usvschina.ai/index.html',
                     'https://usvschina.ai/history.html', 'https://usvschina.ai/models.json']
          for url in dynamic:
              pattern = f'(<loc>{re.escape(url)}</loc>\\s*<lastmod>)[^<]*(</lastmod>)'
              content = re.sub(pattern, f'\\g<1>{today}\\g<2>', content)
          with open('sitemap.xml', 'w') as f:
              f.write(content)
          print(f'Updated sitemap lastmod to {today}')
          "

      - name: Configure git
        run: |
          git config --local user.email "${{ github.repository_owner }}@users.noreply.github.com"
          git config --local user.name "${{ github.repository_owner }}"
        env:
          GITHUB_TOKEN: ${{ secrets.PAT_TOKEN }}
      
      - name: Check for changes
        id: changes
        run: |
          CHANGES=false
          git diff --quiet models.json || CHANGES=true
          git diff --quiet news.json 2>/dev/null || CHANGES=true
          git diff --quiet sitemap.xml 2>/dev/null || CHANGES=true
          git diff --quiet index.html 2>/dev/null || CHANGES=true
          git diff --quiet history.html 2>/dev/null || CHANGES=true
          git diff --quiet about.html 2>/dev/null || CHANGES=true
          git ls-files --others --exclude-standard | grep -q news.json && CHANGES=true
          git ls-files --others --exclude-standard | grep -q 'og-image-' && CHANGES=true
          echo "has_changes=$CHANGES" >> $GITHUB_OUTPUT
      
      - name: Commit and push changes
        if: steps.changes.outputs.has_changes == 'true'
        run: |
          git add models.json
          git add news.json 2>/dev/null || true
          # Remove old OG images from git, then stage the new one
          git ls-files og-image.png | xargs -r git rm -f 2>/dev/null || true
          git ls-files 'og-image-[0-9]*.png' | xargs -r git rm -f 2>/dev/null || true
          git add og-image-[0-9]*.png 2>/dev/null || true
          git add sitemap.xml 2>/dev/null || true
          git add index.html history.html about.html
          TIMESTAMP=$(date -u +'%Y-%m-%dT%H:%M:%S')
          git commit -m "Daily scrape run - $TIMESTAMP UTC"
          git push https://x-access-token:${{ secrets.PAT_TOKEN }}@github.com/${{ github.repository }}.git main
      
      - name: Bust Facebook OG cache
        if: steps.changes.outputs.has_changes == 'true'
        continue-on-error: true
        env:
          FB_APP_ID: ${{ secrets.FB_APP_ID }}
          FB_APP_SECRET: ${{ secrets.FB_APP_SECRET }}
        run: |
          if [ -n "$FB_APP_ID" ] && [ -n "$FB_APP_SECRET" ]; then
            TOKEN="${FB_APP_ID}|${FB_APP_SECRET}"
            for URL in "https://usvschina.ai/" "https://usvschina.ai/index.html" "https://usvschina.ai/history.html" "https://usvschina.ai/about.html"; do
              echo "Scraping Facebook cache for $URL"
              curl -s -X POST "https://graph.facebook.com/?id=${URL}&scrape=true&access_token=${TOKEN}" | head -c 200
              echo ""
            done
            echo "Facebook OG cache refreshed"
          else
            echo "Skipping Facebook cache bust — FB_APP_ID/FB_APP_SECRET not configured"
          fi

      - name: Notify success
        if: steps.changes.outputs.has_changes == 'true'
        run: |
          echo "✅ Daily scrape completed successfully"
          echo "New entry created in models.json"
      
      - name: Notify no changes
        if: steps.changes.outputs.has_changes == 'false'
        run: |
          echo "ℹ️ No changes detected - data unchanged"
