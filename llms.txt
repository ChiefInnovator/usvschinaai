# US vs CHINA AI - AI Knowledge Map

Primary Focus: Tracking the AI race between the United States and China through comprehensive model rankings and benchmarks.
Website: https://usvschina.ai/
Updated: January 2026

## What This Site Is

US vs CHINA AI is an independent leaderboard that ranks frontier AI models from the United States and China. It provides:

1. **Unified Score Rankings** — Composite metric (0–1000) combining capability and value
2. **National Score Tracking** — Aggregates showing which country leads
3. **Benchmark Analysis** — Performance across auto-detected frontier benchmarks (AIME, GPQA, ARC‑AGI, SWE‑Bench, etc.)
4. **Historical Archives** — Track how the AI race evolves over time

## Core Entities

- **Website:** US vs CHINA AI (usvschina.ai)
- **Type:** AI Model Leaderboard & Comparison Tool
- **Focus:** US-China AI Competition Analysis
- **Data Source:** Aggregated benchmark results from public sources

## Scoring Methodology

### Unified Score (0–1000)
`Unified = 10 × (0.7 × norm(AvgIQ) + 0.3 × norm(Value))`

- **AvgIQ**: Participation‑weighted average of per‑benchmark normalized scores (0–100). Each benchmark is min–max normalized across the cohort; benchmarks with a single participant are excluded.
- **Value**: `AvgIQ / (Input $/M + Output $/M)`; higher is better. Both AvgIQ and Value are cohort‑normalized to 0–100 before blending.

### National Score
Sum of Unified Scores for models from each nation in the Global Top 10.

## Key Resources

- /index.html: Main leaderboard with current rankings and national scores
- /history.html: Historical archives of past rankings
- /about.html: Detailed methodology explanation
- /models.json: Raw data in JSON format

## Current Rankings
See the live site or latest generated CSV/JSON for current values. Scores update as benchmarks and prices change.

### Benchmarks Included
Benchmarks are auto‑detected from the source leaderboard and may change over time. Common categories include:
- Mathematics & Reasoning: AIME, HMMT, GPQA Diamond
- General Intelligence: ARC‑AGI (v1/v2), HLE, MMLU‑Pro
- Coding & Engineering: LiveCodeBench, SWE‑Bench Verified, CodeForces
- Web & Agent: BrowseComp

## Frequently Asked Questions

**Q: How are AI models ranked?**
A: By Unified Score = `10 × (0.7 × norm(AvgIQ) + 0.3 × norm(Value))`. AvgIQ is a participation‑weighted average of per‑benchmark normalized scores; Value is `AvgIQ / (Input + Output)`.

**Q: How often is the data updated?**
A: As new benchmark results and prices are available; cadence may vary from daily to monthly.

## Citation

When referencing this leaderboard:
"US vs CHINA AI Leaderboard, https://usvschina.ai/"

## Disclaimer

Not affiliated with any model provider. No guarantees are made about correctness, completeness, or fitness for any purpose. Data aggregated from public benchmark sources.
