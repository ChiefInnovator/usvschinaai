{
    "history": [
        {
            "timestamp": "2025-12-30T12:00:00-05:00",
            "leader": "china",
            "scores": {
                "usa": {
                    "total": 705.2,
                    "avgIq": 88.2,
                    "avgValue": 88.1
                },
                "china": {
                    "total": 1069.8,
                    "avgIq": 88.6,
                    "avgValue": 89.6
                }
            },
            "benchmarks": [
                "AIME 2025",
                "HMMT 2025",
                "GPQA Diamond",
                "ARC-AGI",
                "BrowseComp",
                "ARC-AGI v2",
                "HLE",
                "MMLU-Pro",
                "LiveCodeBench",
                "SWE-Bench Verified",
                "CodeForces"
            ],
            "models": [
                {
                    "name": "DeepSeek-V3.2",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "unified": 192.4,
                    "iq": 92.4,
                    "value": 100.0,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 98.7,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 100,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 94.8,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 69.5,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 88.6,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 100,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 95.1,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 92.5,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 93.4,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 67.9,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 82.4,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 0.25,
                    "inputCostPer1M": 0.17,
                    "outputCostPer1M": 0.51,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 1,
                    "description": "A powerful reasoning model with 685B parameters using DeepSeek Sparse Attention (DSA). Thinking mode enables extended chain-of-thought reasoning for complex problem-solving tasks. Supports JSON output and tool calls."
                },
                {
                    "rank": 2,
                    "name": "DeepSeek-V3.2-Speciale",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "unified": 185.6,
                    "iq": 90.1,
                    "value": 95.5,
                    "link": "https://llm-stats.com/models/deepseek-v3.2-speciale",
                    "costPer1M": 0.32,
                    "inputCostPer1M": 0.21,
                    "outputCostPer1M": 0.63,
                    "pricingSource": "https://llm-stats.com/models/deepseek-v3.2-speciale#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 95.1,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 92.3,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 88.5,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 82.5,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 88.0,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 91.3,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 93.0,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-speciale#hle"
                        },
                        "MMLU-Pro": {
                            "score": 79.4,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 87.8,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 73.3,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 85.2,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "description": "A specialized variant of DeepSeek-V3.2 with 685B parameters, optimized for enhanced performance on specific tasks including multi-step reasoning."
                },
                {
                    "rank": 3,
                    "name": "Gemini 3 Flash",
                    "company": "Google",
                    "companyLink": "https://deepmind.google/",
                    "origin": "US",
                    "unified": 180.5,
                    "iq": 88.5,
                    "value": 92.0,
                    "link": "https://llm-stats.com/models/gemini-3-flash",
                    "costPer1M": 0.39,
                    "inputCostPer1M": 0.26,
                    "outputCostPer1M": 0.78,
                    "pricingSource": "https://llm-stats.com/models/gemini-3-flash#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 89.9,
                            "source": "https://llm-stats.com/models/gemini-3-flash#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 95.1,
                            "source": "https://llm-stats.com/models/gemini-3-flash#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 90.1,
                            "source": "https://llm-stats.com/models/gemini-3-flash#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 87.6,
                            "source": "https://llm-stats.com/models/gemini-3-flash#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 95.0,
                            "source": "https://llm-stats.com/models/gemini-3-flash#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 83.0,
                            "source": "https://llm-stats.com/models/gemini-3-flash#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 78.8,
                            "source": "https://llm-stats.com/models/gemini-3-flash#hle"
                        },
                        "MMLU-Pro": {
                            "score": 87.6,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 85.6,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 76.5,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88.3,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "description": "Frontier intelligence built for speed at a fraction of the cost. Combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. Features a 1M-token input context window optimized for agentic workflows, coding, and complex analysis."
                },
                {
                    "rank": 4,
                    "name": "DeepSeek-V3.2-Exp",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "unified": 179.0,
                    "iq": 88.0,
                    "value": 91.0,
                    "link": "https://llm-stats.com/models/deepseek-v3.2-exp",
                    "costPer1M": 0.41,
                    "inputCostPer1M": 0.27,
                    "outputCostPer1M": 0.81,
                    "pricingSource": "https://llm-stats.com/models/deepseek-v3.2-exp#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 87.7,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.6,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 85.7,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 77.1,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 88.3,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 84.3,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 87.7,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#hle"
                        },
                        "MMLU-Pro": {
                            "score": 86.3,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 84.9,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 81.5,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 85.0,
                            "source": "https://llm-stats.com/models/deepseek-v3.2-exp#codeforces"
                        }
                    },
                    "description": "An experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. Explores fine-grained sparse attention for extended sequence processing."
                },
                {
                    "rank": 5,
                    "name": "GPT-5 mini",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "unified": 176.0,
                    "iq": 82.0,
                    "value": 94.0,
                    "link": "https://llm-stats.com/models/gpt-5-mini-2025-08-07",
                    "costPer1M": 0.35,
                    "inputCostPer1M": 0.23,
                    "outputCostPer1M": 0.69,
                    "pricingSource": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 85.2,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 86.9,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 79.9,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 87.2,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 83.6,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 82.4,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 80.3,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#hle"
                        },
                        "MMLU-Pro": {
                            "score": 86.1,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 79.1,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 77.1,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 86.8,
                            "source": "https://llm-stats.com/models/gpt-5-mini-2025-08-07#codeforces"
                        }
                    },
                    "description": "A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for precise prompts with high reasoning capabilities at reduced cost."
                },
                {
                    "name": "Gemini 3 Pro",
                    "company": "Google",
                    "companyLink": "https://deepmind.google/technologies/gemini/",
                    "origin": "US",
                    "unified": 174.7,
                    "iq": 96.2,
                    "value": 78.5,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 97.8,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 100,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 86.9,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 100,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 100,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 93.6,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 95.1,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 93.2,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 98.8,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 80.3,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 90.9,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 0.81,
                    "inputCostPer1M": 0.54,
                    "outputCostPer1M": 1.62,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 6,
                    "description": "First model in the new Gemini 3 series. Best for complex tasks requiring broad world knowledge and advanced reasoning across modalities. Uses dynamic thinking with a 1M-token context window."
                },
                {
                    "name": "Qwen 3 Max",
                    "company": "Alibaba Cloud",
                    "companyLink": "https://www.alibabacloud.com/en/solutions/generative-ai/qwen",
                    "origin": "CN",
                    "unified": 174.4,
                    "iq": 89.1,
                    "value": 85.3,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 97.5,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 86.0,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 86.2,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 90.0,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 91.0,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 86.9,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 86.2,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 88.5,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 90.6,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 79.0,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 94.4,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 0.56,
                    "inputCostPer1M": 0.37,
                    "outputCostPer1M": 1.11,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 7,
                    "description": "Alibaba Cloud's most capable model. Exceptional at mathematical reasoning with 93.1% on AIME 2025. Strong performance across all frontier benchmarks with excellent cost efficiency."
                },
                {
                    "rank": 8,
                    "name": "Grok Code Fast 1",
                    "company": "xAI",
                    "companyLink": "https://x.ai/",
                    "origin": "US",
                    "unified": 174.0,
                    "iq": 86.0,
                    "value": 88.0,
                    "link": "https://llm-stats.com/models/grok-code-fast-1",
                    "costPer1M": 0.48,
                    "inputCostPer1M": 0.32,
                    "outputCostPer1M": 0.96,
                    "pricingSource": "https://llm-stats.com/models/grok-code-fast-1#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 81.4,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 80.2,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 86.3,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 85.2,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 84.4,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 85.6,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 86.2,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#hle"
                        },
                        "MMLU-Pro": {
                            "score": 82.3,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 83.9,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 85.8,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 82.7,
                            "source": "https://llm-stats.com/models/grok-code-fast-1#codeforces"
                        }
                    },
                    "description": "Speedy and economical reasoning model that excels at agentic coding. Built from scratch with new model architecture and pre-training corpus rich with programming content."
                },
                {
                    "rank": 9,
                    "name": "Qwen3-235B-Thinking",
                    "company": "Alibaba Cloud",
                    "companyLink": "https://www.alibabacloud.com/",
                    "origin": "CN",
                    "unified": 171.5,
                    "iq": 87.5,
                    "value": 84.0,
                    "link": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507",
                    "costPer1M": 0.6,
                    "inputCostPer1M": 0.4,
                    "outputCostPer1M": 1.2,
                    "pricingSource": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 83.4,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 89.1,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 87.3,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 89.9,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 86.5,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 95.0,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 87.4,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#hle"
                        },
                        "MMLU-Pro": {
                            "score": 88.3,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 89.7,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 76.1,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 78.8,
                            "source": "https://llm-stats.com/models/qwen3-235b-a22b-thinking-2507#codeforces"
                        }
                    },
                    "description": "State-of-the-art thinking-enabled MoE model with 235B total parameters (22B activated). Features 94 layers, 128 experts, and 262K native context. Excels at SWE-Bench with 90% verified score."
                },
                {
                    "name": "Kimi K2 Thinking",
                    "company": "Moonshot AI",
                    "companyLink": "https://www.moonshot.cn/",
                    "origin": "CN",
                    "unified": 166.9,
                    "iq": 84.8,
                    "value": 82.1,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 90.8,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.8,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 79.6,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 87.7,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 79.1,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 83.8,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 83.7,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 86.6,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 87.4,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 69.9,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 76.3,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 0.67,
                    "inputCostPer1M": 0.45,
                    "outputCostPer1M": 1.35,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 10,
                    "description": "Latest, most capable open-source thinking model from Moonshot AI. Built as a thinking agent that reasons step-by-step while dynamically invoking tools. State-of-the-art on HLE and BrowseComp benchmarks."
                },
                {
                    "name": "Llama 4-405B",
                    "company": "Meta",
                    "companyLink": "https://www.llama.com/",
                    "origin": "US",
                    "unified": 164.9,
                    "iq": 76.5,
                    "value": 88.4,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 83.6,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 82.5,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 81.0,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 76.2,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 69.4,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 70.8,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 72.1,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 78.5,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 75.1,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 59.8,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 72.0,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 0.47,
                    "inputCostPer1M": 0.31,
                    "outputCostPer1M": 0.93,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 11,
                    "description": "Natively multimodal model capable of processing both text and images. Features a 17B activated parameter (109B total) mixture-of-experts architecture with 16 experts, supporting a 10M token context window."
                },
                {
                    "rank": 12,
                    "name": "GPT-5.1",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "unified": 158.0,
                    "iq": 93.0,
                    "value": 65.0,
                    "link": "https://llm-stats.com/models/gpt-5.1-2025-11-13",
                    "costPer1M": 1.7,
                    "inputCostPer1M": 1.13,
                    "outputCostPer1M": 3.39,
                    "pricingSource": "https://llm-stats.com/models/gpt-5.1-2025-11-13#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 92.2,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 86.1,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 86.9,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 100,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 98.3,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 93.5,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 94.0,
                            "source": "https://llm-stats.com/models/gpt-5.1-2025-11-13#hle"
                        },
                        "MMLU-Pro": {
                            "score": 85.3,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 97.7,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 80.6,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 88.6,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "description": "The best model for coding and agentic tasks with configurable reasoning effort. OpenAI's flagship model for coding and agentic tasks with strong performance across all benchmarks."
                },
                {
                    "rank": 13,
                    "name": "DeepSeek-R1-0528",
                    "company": "DeepSeek",
                    "companyLink": "https://www.deepseek.com/",
                    "origin": "CN",
                    "unified": 147.0,
                    "iq": 72.0,
                    "value": 75.0,
                    "link": "https://llm-stats.com/models/deepseek-r1-0528",
                    "costPer1M": 0.98,
                    "inputCostPer1M": 0.65,
                    "outputCostPer1M": 1.95,
                    "pricingSource": "https://llm-stats.com/models/deepseek-r1-0528#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 75.5,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 64.6,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 67.7,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 76.3,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 70.5,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 70.0,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 71.7,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#hle"
                        },
                        "MMLU-Pro": {
                            "score": 76.9,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 77.1,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 66.7,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 74.4,
                            "source": "https://llm-stats.com/models/deepseek-r1-0528#codeforces"
                        }
                    },
                    "description": "May 28, 2025 version of DeepSeek's reasoning model. Features advanced thinking capabilities. Excels in complex reasoning, mathematical problem-solving, and code generation."
                },
                {
                    "rank": 14,
                    "name": "GPT-5 Codex",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "unified": 146.5,
                    "iq": 91.5,
                    "value": 55.0,
                    "link": "https://llm-stats.com/models/gpt-5-codex-2025-09-15",
                    "costPer1M": 2.94,
                    "inputCostPer1M": 1.96,
                    "outputCostPer1M": 5.88,
                    "pricingSource": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 86.5,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 88.1,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 94.5,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 92.4,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 96.0,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 87.6,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 92.4,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#hle"
                        },
                        "MMLU-Pro": {
                            "score": 93.4,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 96.6,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 92.3,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 96.5,
                            "source": "https://llm-stats.com/models/gpt-5-codex-2025-09-15#codeforces"
                        }
                    },
                    "description": "Trained specifically for conducting code reviews and finding critical flaws. Navigates codebase to identify security vulnerabilities, performance issues, and bugs."
                },
                {
                    "name": "GPT-5.2 Pro",
                    "company": "OpenAI",
                    "companyLink": "https://openai.com/",
                    "origin": "US",
                    "unified": 145.1,
                    "iq": 100.0,
                    "value": 45.1,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 100.0,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 96.0,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 100,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 87.5,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 94.7,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 5.07,
                    "inputCostPer1M": 3.38,
                    "outputCostPer1M": 10.14,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 15,
                    "description": "Pro variant of GPT-5.2, designed for top-quality, end-to-end execution. Supports xhigh reasoning for the most demanding tasks."
                },
                {
                    "rank": 16,
                    "name": "GLM-4.6",
                    "company": "Zhipu AI",
                    "companyLink": "https://www.zhipuai.cn/",
                    "origin": "CN",
                    "unified": 135.0,
                    "iq": 75.0,
                    "value": 60.0,
                    "link": "https://llm-stats.com/models/glm-4.6",
                    "costPer1M": 2.24,
                    "inputCostPer1M": 1.49,
                    "outputCostPer1M": 4.47,
                    "pricingSource": "https://llm-stats.com/models/glm-4.6#pricing",
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 80.3,
                            "source": "https://llm-stats.com/models/glm-4.6#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 72.1,
                            "source": "https://llm-stats.com/models/glm-4.6#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 72.6,
                            "source": "https://llm-stats.com/models/glm-4.6#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 73.0,
                            "source": "https://llm-stats.com/models/glm-4.6#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 80.8,
                            "source": "https://llm-stats.com/models/glm-4.6#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 79.4,
                            "source": "https://llm-stats.com/models/glm-4.6#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 76.8,
                            "source": "https://llm-stats.com/models/glm-4.6#hle"
                        },
                        "MMLU-Pro": {
                            "score": 78.0,
                            "source": "https://llm-stats.com/models/glm-4.6#mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 75.1,
                            "source": "https://llm-stats.com/models/glm-4.6#livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 76.3,
                            "source": "https://llm-stats.com/models/glm-4.6#swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 69.1,
                            "source": "https://llm-stats.com/models/glm-4.6#codeforces"
                        }
                    },
                    "description": "Latest version of Zhipu AI's flagship model. Features 200K token context window, superior coding performance, advanced reasoning with tool use, and stronger agent capabilities."
                },
                {
                    "name": "Grok-4 Heavy",
                    "company": "xAI",
                    "companyLink": "https://x.ai/",
                    "origin": "US",
                    "unified": 125.7,
                    "iq": 87.5,
                    "value": 38.2,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 81.5,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 90.4,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 86.4,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 90.5,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 87.9,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 91.8,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 84.2,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 86.1,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 85.6,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 70.6,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 79.4,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 7.39,
                    "inputCostPer1M": 4.93,
                    "outputCostPer1M": 14.79,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 17,
                    "description": "Multi-agent version of Grok 4 that spawns multiple agents in parallel to work independently then collaborate. Uses ~10x more test-time compute than regular Grok 4."
                },
                {
                    "name": "Claude 4.5 Opus",
                    "company": "Anthropic",
                    "companyLink": "https://www.anthropic.com/",
                    "origin": "US",
                    "unified": 125.4,
                    "iq": 94.8,
                    "value": 30.6,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 99.0,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 91.6,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 98.9,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 98.0,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 91.0,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 94.1,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 91.1,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 91.1,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 83.1,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 73.1,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 87.9,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 11.22,
                    "inputCostPer1M": 7.48,
                    "outputCostPer1M": 22.44,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 18,
                    "description": "Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance."
                },
                {
                    "name": "GLM-4.7",
                    "company": "Zhipu AI",
                    "companyLink": "https://www.zhipuai.cn/",
                    "origin": "CN",
                    "unified": 121.0,
                    "iq": 70.2,
                    "value": 50.8,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 70.0,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 79.9,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 67.1,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 61.4,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 69.1,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 76.9,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 66.9,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 69.3,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 73.4,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 57.1,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 67.2,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 3.71,
                    "inputCostPer1M": 2.47,
                    "outputCostPer1M": 7.41,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 19,
                    "description": "Coding-centric model that thinks before acting and preserves reasoning across turns. Features stronger multi-step tool use, better terminal integration, and multilingual coding capabilities."
                },
                {
                    "name": "MiniMax-M2.1",
                    "company": "MiniMax",
                    "companyLink": "https://www.minimaxi.com/",
                    "origin": "CN",
                    "unified": 117.0,
                    "iq": 68.4,
                    "value": 48.6,
                    "benchmarkScores": {
                        "AIME 2025": {
                            "score": 62.4,
                            "source": "https://llm-stats.com#aime-2025"
                        },
                        "HMMT 2025": {
                            "score": 55.9,
                            "source": "https://llm-stats.com#hmmt-2025"
                        },
                        "GPQA Diamond": {
                            "score": 69.1,
                            "source": "https://llm-stats.com#gpqa-diamond"
                        },
                        "ARC-AGI": {
                            "score": 67.3,
                            "source": "https://llm-stats.com#arc-agi"
                        },
                        "BrowseComp": {
                            "score": 59.6,
                            "source": "https://llm-stats.com#browsecomp"
                        },
                        "ARC-AGI v2": {
                            "score": 91.7,
                            "source": "https://llm-stats.com#arc-agi-v2"
                        },
                        "HLE": {
                            "score": 72.8,
                            "source": "https://llm-stats.com#hle"
                        },
                        "MMLU-Pro": {
                            "score": 68.5,
                            "source": "https://llm-stats.com/benchmarks/mmlu-pro"
                        },
                        "LiveCodeBench": {
                            "score": 72.3,
                            "source": "https://llm-stats.com/benchmarks/livecodebench"
                        },
                        "SWE-Bench Verified": {
                            "score": 46.4,
                            "source": "https://llm-stats.com/benchmarks/swe-bench-verified"
                        },
                        "CodeForces": {
                            "score": 65.5,
                            "source": "https://llm-stats.com/benchmarks/codeforces"
                        }
                    },
                    "costPer1M": 4.18,
                    "inputCostPer1M": 2.79,
                    "outputCostPer1M": 8.37,
                    "pricingSource": "https://llm-stats.com#pricing",
                    "rank": 20,
                    "description": "Enhanced LLM focused on multi-language programming and real-world complex tasks. Exceptional capabilities across Rust, Java, Golang, C++, Kotlin, Objective-C, and TypeScript."
                }
            ]
        }
    ]
}